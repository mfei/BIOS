 \section{ANOVA}

\subsection{Expectation of Quadratic Form}
Theorem: Let $X$ be an $n \times 1$ random vector with mean $\mu$ and covariance $\Sigma$ and let A be a symmetric matrix $n \times n$. Then, the expectation of the quadratic form 

\begin{align*}
  E[X^T A X] &= \mu^T A \mu + tr(A \Sigma) 
\end{align*}

Proof (the proof is not easy as I thought, we need to use twice the trace property):
\begin{align*}
  E[X^T A X] &= E \Big[ tr(X^T A X) \Big] = E \Big[ tr(A X X^T ) \Big] \\
  &= tr \Big( A E \Big[ X X^T  \Big] \Big) =  tr \Big[ A \Big( Var(X) + E(X) E(X)^T  \Big) \Big] \\
  &= tr \Big[ A ( \Sigma+ \mu \mu^T) \Big] \\
  &= tr(A\Sigma) + tr(\mu^T A \mu) \\
  &= tr(A\Sigma) + \mu^T A \mu
\end{align*}


\subsection{Two Way Interaction ANOVA}

 Consider the two way ANOVA table with interaction, given by
   \[ 
   Y_{ijk}  = \mu + \alpha_i + \eta_j + \gamma_{ij} + \epsilon_{ijk},
    \] 
    
 where $i=1,2,...a, j=1,2.., b$ and $k=1,..N$. Further suppose that the $\epsilon_{ijk}$ are i.i.d and $\epsilon_{ijk} \sim N(0, \sigma^2)$, where $\sigma^2$ is unknown. Let $M_{\alpha}, M_{\eta}$ and $M_{\gamma}$ denote the orthogonal operations for the $\alpha, \eta$ spaces, and interaction space, respectively.  
 
 By the previous o.p.o method we learnt, we can calculate the M using the column space of each effector. In the design matrix, the column space of A (all the columns add up to intercept) will not include the intercept component, which is exactly the Kroneck product. 
 
 Then based on that, we can further get the quadratic form of each effect, and F-test for hypothesis test.
 
 We can summarize the subspaces and the orthogonal projection operators for the two-way ANOVA model as follows.
 
 \begin{center}
\begin{tabular}{ c c c }
 Effect & Subspace & Orthogonal Projection Operator \\ 
 \hline
 $\mu$ & $C\Big( J_a \otimes J_b \otimes J_N \Big)$ & $P_a  \otimes P_b \otimes P_N $ \\  
 $\alpha$ & $C\Big( Q_a \otimes J_b \otimes J_N \Big)$ & $Q_a  \otimes P_b \otimes P_N $ \\  
 $\eta$ & $C\Big( J_a \otimes Q_b \otimes J_N \Big)$ & $P_a  \otimes Q_b \otimes P_N $ \\  
 $\gamma$ & $C\Big( Q_a \otimes Q_b \otimes J_N \Big)$ & $Q_a  \otimes Q_b \otimes P_N $ \\  
Error & $C\Big( I-M \Big)$ & $I_a  \otimes I_b \otimes Q_N $ \\  
 \hline
Total &  & $I_a  \otimes I_b \otimes I_N $ \\  
\end{tabular}
\end{center}

 
\begin{itemize}

    \item [(a)] Show that $\sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}\gamma_{ij}$ is estimable, and find the UMVUE and the variance of the UMVUE, where the $c_{ij}$'s are real numbers that satisfy $\sum_{i=1}^{a} c_{ij}= \sum_{j=1}^{b} c_{ij}= 0$.\\
    Let 
\begin{align*}
   \beta &= (\mu, \alpha_1, \alpha_2,.. \alpha_a, \eta_1, \eta_2,.. \eta_b, \gamma_{11}, \gamma_{12},... \gamma_{ab}),\\
   X &= (J_a \otimes J_b \otimes J_N, I_a \otimes J_b \otimes J_N, J_a \otimes I_b \otimes J_N, I_a \otimes I_b\otimes J_N),
\end{align*}

    The two way ANOVA model could be written as 
\begin{align*}
   Y = X\beta + \epsilon, \epsilon \sim N(0, \sigma^2)
\end{align*}

    The contrast is only the weighted two way interactions
    
\begin{align*}
    \lambda^T \beta &= \sum_{i=1}^a \sum_{j=1}^b c_{ij}\gamma_{ij},\\
    \lambda^T &= (0, ... 0_{a+b+1}, c_{11}, ...c_{ab}),
\end{align*}

    We need to find $\rho$ such that $\lambda^T = \rho^T X$ to show estimability.
    Let 
\begin{align*}
    \rho^T &= \frac{1}{N} (c_{11}J_N^T,c_{12}J_N^T, ... c_{ab}J_N^T),\\
    \rho^T X &= \frac{1}{N} (c_{11},c_{12}, ... c_{ab})\otimes J_N^T) (J_a \otimes J_b \otimes J_N, I_a \otimes J_b \otimes J_N, J_a \otimes I_b \otimes J_N, I_a \otimes I_b\otimes J_N),\\
    \rho^T X &= \frac{1}{N} \Bigg \{ \Big [(c_{11}, ... c_{ab})(J_a \otimes J_b) \Big ]\otimes J_N^TJ_N, 
    & \Big [(c_{11}, ... c_{ab})(I_a \otimes J_b) \Big ]\otimes J_N^TJ_N,  \Big [(c_{11}, ... c_{ab})(J_a \otimes I_b) \Big ]\otimes J_N^TJ_N,  \Big [(c_{11}, ... c_{ab})(I_a \otimes I_b) \Big ]\otimes J_N^TJ_N \Bigg \}
    \end{align*}
    
Since $\sum_{i=1}^{a} c_{ij} = \sum_{i=1}^{b} c_{ij} = 0$ Then
    \begin{align*}
    [(c_{11}, ... c_{ab})(J_a \otimes J_b)] &= 0, \\
      \left(c_{11}, ... c_{ab} \right) \left(J_a \otimes I_b \right) &= 0\\
     \left(c_{11}, ... c_{ab} \right) \left(I_a \otimes J_b \right) &= 0\\
    [(c_{11}, ... c_{ab})(I_a \otimes I_b)] &= (c_{11}, c_{12},... c_{ab})\\
    \rho^T X &= 0, 0_a, 0_b, (c_{11}, ... c_{ab}) = \lambda^T
 \end{align*}
 
    Thus, $\lambda^T \beta = \sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}\gamma_{ij}$ is estimable.
    
    The UMVUE of $\lambda^T \beta$ is $\rho^T MY$, where $M= I_a \otimes I_b \otimes P_N$, where $P_N = \frac{1}{N} J_{N}^{N} $,  
    
    Therefore, we have
 \begin{align*}
    \rho^T MY &= \frac{1}{N} [(c_{11}, ... c_{ab})]\otimes J_N^T] [(I_{ab} \otimes P_N)] Y\\
    & = \sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}\bar{Y}_{ij.}
 \end{align*}
    The variance of UMVUE is
 \begin{align*}
    Var(\rho^T MY) &= Var(\sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}\bar{Y}_{ij.}) \\
    &=\sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}^2Var(\bar{Y}_{ij.}) = \sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}^2 \frac{\sigma^2}{N}
 \end{align*}
    
    \item[(b)] Using Kronecker product and notation, derive the orthogonal projection operator for the interaction space, denoted by $M_{\gamma}$.
    
    Let $s$ be an arbitrary index. Define $J_s$ as the $s \times 1$ vector of ones, $P_s = \frac{1}{N} J_s J_s^{'}$ and $\mathbf{Q}_s = I_s -P_s$, where $I_s$ is the $s \times s$ identity matrix. 
    
    Thus $P_s$ is the orthogonal projection operator onto $C(J_s)$ and $\mathbf{Q}_s$ is the orthogonal projection operator onto $C(J_s)^{\perp}$. PAY ATTENTION THAT, WE ALWAYS WANT TO TAKE OUT THE FIRST COLUMN TO GET ORTHOGONAL MATRIX.
      
    Computing $M_\gamma$. The interaction space is given by $C(Q_a \otimes Q_b \otimes P_N)$. This yields
    
 \begin{align*}
    M_\gamma = Q_a \otimes Q_b \otimes P_N
 \end{align*}
 
    Compute $M_\mu$, THE FIRST COLUMN STAY AS IT IS. 
    
 \begin{align*}
    M_\mu &= (J_a \otimes J_b \otimes J_N) [(J_a \otimes J_b \otimes J_N)^{T}(J_a \otimes J_b \otimes J_N)]^{-1} (J_a \otimes J_b \otimes J_N)^{T}\\
     &= (J_a \otimes J_b \otimes J_N) [(J_a ^{'}\otimes J_b^{'} \otimes J_N^{'})(J_a \otimes J_b \otimes J_N)]^{-1} (J_a^{'} \otimes J_b^{'} \otimes J_N^{'})\\
     &= (J_a \otimes J_b \otimes J_N) [(J_a ^{'}J_a\otimes J_b^{'}J_b \otimes J_N^{'}J_N)]^{-1}(J_a^{'} \otimes J_b^{'} \otimes J_N^{'})\\
     &= (J_a \otimes J_b \otimes J_N) (abN)^{-1} (J_a^{'} \otimes J_b^{'} \otimes J_N^{'})\\
     &= \frac{1}{a}J_aJ_a^{'} \otimes \frac{1}{b}J_bJ_b^{'} \otimes \frac{1}{N}J_NJ_N^{'} \\
     &= P_a \otimes P_b \otimes P_N
 \end{align*}
    
    Compute $M_\gamma$, the $\gamma$ space is $(Q_a \otimes Q_b \otimes J_N)$, thus
 \begin{align*}
    M_\gamma &= (Q_a \otimes Q_b \otimes J_N) [(Q_a \otimes Q_b \otimes J_N)^{T}(Q_a \otimes Q_b \otimes J_N)]^{-1} (Q_a \otimes Q_b \otimes J_N)^{T}\\
     &= (Q_a \otimes Q_b \otimes J_N) [(Q_a ^{'}Q_a\otimes Q_b^{'}Q_b \otimes J_N^{'}J_N)]^{-1}(Q_a^{'} \otimes Q_b^{'} \otimes J_N^{'})\\
     &= (Q_a \otimes Q_b \otimes J_N) [(Q_a^{-}\otimes Q_b^{-1} \otimes N^{-1}](Q_a^{'} \otimes Q_b^{'} \otimes J_N^{'})\\
     &= (Q_a \otimes Q_b \otimes P_N) 
 \end{align*}
    Now $M = M_{\mu} + M_{\alpha} + M_{\eta} + M_{\gamma}$, we have
 \begin{align*}
     M &=(P_a \otimes P_b \otimes P_N) + (Q_a \otimes P_b \otimes P_N) + (P_a \otimes Q_b \otimes P_N)  + (Q_a \otimes Q_b \otimes P_N) \\
     M &=(P_a + Q_a) \otimes P_b \otimes P_N + (P_a + Q_a) \otimes Q_b \otimes P_N  = I_a \otimes I_b \otimes P_N
 \end{align*}
    The error space is $I-M$
 \begin{align*}
     I-M = I_a \otimes I_b \otimes I_N - I_a \otimes I_b \otimes P_N = I_a \otimes I_b \otimes Q_N
 \end{align*}
 
    \item[(c)] Derive the simply possible scalar expression for $E[Y'(M_\alpha + M_\eta)Y]$.\\
 \begin{align*}
     E[Y'(M_\alpha + M_\eta)Y] &= tr((M_\alpha + M_\eta)\Sigma) + \mu'(M_\alpha + M_\eta)\mu \\
     \mu &= E[Y] = \mu \otimes J_a \otimes J_b \otimes J_N + \alpha \otimes J_b \otimes J_N + J_a \otimes \eta \otimes J_N + \gamma \otimes J_N \\
    \alpha &= (\alpha_1, \alpha_2,... \alpha_a)^T, \qquad \eta = (\eta_1, \eta_2,... \eta_b)^T, \qquad \gamma= (\gamma_{11},.. \gamma_{ab})^T\\
     \Sigma &= \sigma^2 I_{ab} \\
     E[Y'(M_\alpha + M_\eta)Y] &= tr((M_\alpha + M_\eta)\Sigma) + \mu'(M_\alpha + M_\eta)\mu \\
     &= tr(M_\alpha \Sigma) + tr(M_\eta\Sigma) + \mu'M_\alpha \mu + \mu' M_\eta \mu 
 \end{align*}
  
  We have $M_{\alpha}$ and $M_{\eta}$ orthogonal to $M_{\mu}$
 \begin{align*}
 M_\alpha &= (Q_a \otimes P_b \otimes P_N) \\
     \mu'M_\alpha \mu &= (\mu J_a \otimes J_b \otimes J_N + \alpha \otimes J_b \otimes J_N + J_a \otimes \eta \otimes J_N + \gamma \otimes J_N)^T (Q_a \otimes P_b \otimes P_N)\\
     &(\mu J_a \otimes J_b \otimes J_N + \alpha \otimes J_b \otimes J_N + J_a \otimes \eta \otimes J_N + \gamma \otimes J_N)  
\end{align*}
    because $Q_a J_a = 0$
 \begin{align*}
     \mu'M_\alpha \mu &= (\alpha \otimes J_b \otimes J_N + \gamma \otimes J_N)^T (Q_a \otimes P_b \otimes P_N)     (\alpha \otimes J_b \otimes J_N + \gamma \otimes J_N)  \\ 
     &= (\alpha \otimes J_b \otimes J_N)^T (Q_a \otimes P_b \otimes P_N) (\alpha \otimes J_b \otimes J_N) + 2(\gamma \otimes J_N)^T (Q_a \otimes P_b \otimes P_N) (\alpha \otimes J_b \otimes J_N) \\ +  (\gamma \otimes J_N)^T(Q_a \otimes P_b \otimes P_N)(\gamma \otimes J_N)\\
     &= (\alpha^T Q_a \alpha) \otimes (J_b^T P_b J_b) \otimes (J_N^T P_N J_N) + 2(\gamma \otimes J_N)^T (\alpha Q_a) \otimes (P_bJ_b) \otimes (P_NJ_N) + (\gamma^T (Q_a \otimes P_b) \gamma) \otimes (J_N^T P_N J_N)\\
     &= (\alpha^T Q_a \alpha) \otimes (J_b^T P_b J_b) \otimes (J_N^T P_N J_N) + 2[\gamma^T (\alpha Q_a \otimes P_bJ_b)] \otimes (J_N^T P_NJ_N) + (\gamma^T (Q_a \otimes P_b) \gamma) \otimes (J_N^T P_N J_N) \end{align*}

    break down into each term
 \begin{align*}
     \alpha'Q_a\alpha &= \alpha^T [I- \frac{1}{a}J_a^a] \alpha  = [(I-\frac{1}{a}J_a^a)\alpha]^T[(I-\frac{1}{a}J_a^a)\alpha] \\
     &= [(\alpha -\bar \alpha J_a]^T[(\alpha -\bar \alpha J_a] = \sum_{i=1}^{n} (\alpha_i - \bar \alpha)^2\\
     J_b^T P_b J_b &= J_b^T \frac{1}{b} J_b^b J_b = b\\
     J_N^T P_N J_N &= N\\
     (\alpha^T Q_a \alpha) \otimes (J_b^T P_b J_b) \otimes (J_N^T P_N J_N) &=bN \sum_{i=1}^{n} (\alpha_i - \bar \alpha)^2
\end{align*}
    
    \item[(d)] Derive the F-test for the hypothesis: $H_0: \sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}\gamma_{ij} =4$, and state its distribution under the null and alternative hypothesis.\\
    The orthogonal operator projection for $M\rho$ is\\
    \[ 
    M_{MP} = (M\rho) [(M\rho)^T (M\rho)]^{-} (\rho'M) = (M\rho) [\rho^T M\rho]^{-} (\rho'M)
    \]
     The F-test is given by:\\
    \[ 
    F = \frac{(\rho'MY - 4)' (\rho'M\rho)^{-} (\rho'MY -4)/ r(M_{MP})}{MSE}
    \] 
    Also because $M\rho = \rho$,
 \begin{align*}
    \rho'MY &= \rho'Y = \sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}\bar{Y}_{ij.}\\
    \rho'M\rho &= \rho'\rho =  \sum_{i=1}^{a}\sum_{j=1}^{b} \frac{c_{ij}^2}{N}\\
    MSE &= \frac{1}{abN - ab} \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^N (Y_{ijk} - \bar{Y}_{ij.})^2
\end{align*}
    Thus,
    \[ 
    F =\frac{\frac{N}{\sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}^2} (\sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}\bar{Y}_{ij.} - 4)^2} {\frac{1}{abN - ab} \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^N (Y_{ijk} - \bar{Y}_{ij.})^2} \sim F[1, ab(N-1), \gamma]
    \] 
    Under $H_0, \gamma = 0$, and under $H_1, \gamma = \frac{(\sum_{i=1}^{a}\sum_{j=1}^{b}c_{ij}^2)N}{2\sigma^2 \sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}^2 }$ 
    
    \item[(e)] Using only Kronecker product development and notation, obtain the simplest possible expression for $M_\alpha + M_\eta$.
    
Compute $M_\alpha$, the $\alpha$ space is $(Q_a \otimes J_b \otimes J_N)$, thus
 \begin{align*}
    M_\alpha &= (Q_a \otimes J_b \otimes J_N) [(Q_a \otimes J_b \otimes J_N)^{T}(Q_a \otimes J_b \otimes J_N)]^{-1} (Q_a \otimes J_b \otimes J_N)^{T}\\
     &= (Q_a \otimes J_b \otimes J_N) [(Q_a ^{'}Q_a\otimes J_b^{'}J_b \otimes J_N^{'}J_N)]^{-1}(Q_a^{'} \otimes J_b^{'} \otimes J_N^{'})\\ 
     &= (Q_a \otimes J_b \otimes J_N) [(Q_a^{-}\otimes b^{-1} \otimes N^{-1}](Q_a^{'} \otimes J_b^{'} \otimes J_N^{'})\\
     & = (Q_a \otimes P_b \otimes P_N) 
\end{align*}
    Here $Q_a$ is symmetric, semi-definite.\\
    Compute $M_\eta$, the $\eta$ space is $(J_a \otimes Q_b \otimes J_N)$, thus
 \begin{align*}
    M_\eta &= (J_a \otimes Q_b \otimes J_N) [(J_a \otimes Q_b \otimes J_N)^{T}(J_a \otimes Q_b \otimes J_N)]^{-1} (J_a \otimes Q_b \otimes J_N)^{T}\\
     &= (J_a \otimes Q_b \otimes J_N) [(J_a ^{'}J_a\otimes Q_b^{'}Q_b \otimes J_N^{'}J_N)]^{-1}(J_a^{'} \otimes Q_b^{'} \otimes J_N^{'})\\
     & = (J_a \otimes Q_b \otimes J_N) [(a^{-}\otimes Q_b^{-1} \otimes N^{-1}](J_a^{'} \otimes Q_b^{'} \otimes J_N^{'})\\
     & = (P_a \otimes Q_b \otimes P_N) \\
     M_\alpha + M_\eta &=(Q_a \otimes P_b \otimes P_N) + (P_a \otimes Q_b \otimes P_N) 
\end{align*}  
\end{itemize}


\subsection{ANOVA Table}

Breaking a sum of squares into independent components

We consider a two way ANOVA table without interaction. The model is given by
 \begin{align*}
    Y_{ijk} &= \mu + \alpha_i + \eta_j + \epsilon_{ijk}, \qquad i=1,..a, j=1,.. b, k=1,..N, n=abN   
\end{align*}

\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
 Source & DF & SS & MS \\ 
  \hline
 Meam & 1 & $Y' \Big(\frac{J_n^n}{n} \Big) Y$  &$Y' \Big(\frac{J_n^n}{n} \Big) Y$ \\ 
Treatment $(\alpha)$ & a-1 & $Y' M_{M_{\alpha}} Y$ & $ \frac{Y' M_{M_{\alpha}} Y}{a-1}$ \\ 
Treatment $(\eta)$ & b-1 & $Y' M_{M_{\eta}} Y$ & $ \frac{Y' M_{M_{\alpha}} Y}{b-1}$ \\ 
Error $(\epsilon)$ & n-a-b + 1 & $Y' (I-M) Y$ & $ \frac{Y' (I-M) Y}{n-a-b + 1}$ \\ 
 \hline
\end{tabular}
\end{center}

How to understand the degrees of freedom? 
WE CAN USE THE RANK OF THE COLUMN SPACE OF X. FOR INTERCEPT, THERE IS ONLY ONE COLUMN. FOR $\alpha$, THE RANK IS $a-1$.
when we get the M rank = a, while $r(M_{\mu}) = 1$ and $r(M_{\alpha}) = r(M) - r(M_{\mu}) = a-1$ because $M_{\alpha}$ and $M_{\mu}$ are orthogonal.


$M_{\alpha}$ is the orthogonal projection operator onto the column space of $X_{\alpha}$, where $X_{\alpha}$ is the design matrix corresponding to the model
 \begin{align*}
    Y_{ijk} &= \mu + \alpha_i +  \epsilon_{ijk},  
\end{align*}

I need to be aware that, we need to use Gram-Schmidt method to create two orthogonal column space $M_{\mu}$ and $M_{\alpha}$. Well, in the notes, we just took off the first column to get the $M = M_{\mu} + M_{\alpha}$. The two M are different.

And we can break up the $\alpha$ treatment sums of square into $a-1$ separate components, each having 1 degree of freedom. That is, the quadratic form $Y'M_{M_{\alpha}} Y$ must be decomposed into
 \begin{align*}
    Y'M_{M_{\alpha}} Y &= \sum_{i=1}^{a-1} Y'M_i Y  
\end{align*}

where each $M_i$ has rank 1 and $M_iM_j = 0, i \neq j$. Thus, in terms of subspaces, we decompose $C(M_{\alpha})$ into a sum of $a-1$ orthogonal subspaces each of dimension 1. Thus
 \begin{align*}
    C(M_{\alpha}) &= C(M_{1}) + C(M_2) +… + C(M_{a-1}) 
\end{align*}

So $Y'M_TY$ correspond to the sums of squares for a set of orthogonal contrasts. 


\subsubsection{Find the OPO}

There are two ways to find $M_{\alpha}$. One is to find the column space, that we use the orthogonal projection operator to get it. The second is used widely in hypothesis testing, we found the $M_{H_0}$ and $M_{H_1}$, then use $M_{\alpha} = M_{H_1} - M_{H_0}$. 

And $C(M_{\mu})$ and $C(M_{\alpha})$ are orthogonal. 

You can see that $C(1, X) = C(X)$, as the column of X add up to $J_n^n$. So the OPO from $C(X)$ is actually M, however $M_{\alpha} = M - M_{\mu}$.



\subsection{Exercise}

\begin{itemize}

\item[(i)] Consider balanced two way ANOVA model with interaction $y_{ijk} = \mu + \alpha_i + \eta_j + \gamma_{ij} + \epsilon_{ijk}, i=1,..a, j=1…, b, k=1,.N$, with $\epsilon_{ijk} \sim N(0, \sigma^2)$ i.i.d. 
Find $E \Big[ Y' \Big(\frac{1}{n} J_n^n + M_{\alpha} \Big) Y \Big]$ in terms of $\mu, \alpha_i, \eta_j, \gamma_{ij}$.

1. We will need to break down the quadratic form into simpler components (trace, expectations $\Vert M Y \Vert$ ) for computation efficiency, after that we can calculate using matrix linear algebra.
2. The link of expectation: $E[Y] = E[\mu + \alpha_i + \eta_j + \gamma_{ij} ] $, here we can write the $E(Y) = M_{\mu} Y = \bar{y}_{..} J_{abN}^1$, or  $E(Y) = M Y = \bar{y}_{i.} J_{bN}^1$. 
3. The o.p.o would be the key. $M_{\mu} = \frac{1}{n} J_n^n$, and $M_{\alpha} = M - M_{\mu}$, so $M_{\mu}$ and $M_{\alpha}$ are orthogonal. 

 \begin{align*}
    E \Big[ Y' \Big(\frac{1}{n} J_n^n + M_{\alpha} \Big) Y \Big] &= E \Big[ Y' M Y \Big] \\
    C(M) &= C \begin{pmatrix} 
    J_{abN} &Blk \Big( J_{bN} \Big)
    \end{pmatrix}  = C \Big[ Blk \Big( J_{bN} \Big) \Big]\\
    M &= X (X'X)^{-1} X' = \begin{pmatrix} 
    Blk \{ \frac{1}{bN} J_{bN} \} & .. & 0 & 0 \\
    0 & Blk \{ \frac{1}{bN} J_{bN} \} &..& 0 \\
    &..&..&.. \\
    0 & 0 & .. & Blk \{ \frac{1}{bN} J_ \}  \\
    \end{pmatrix} \\
    M_{\alpha} &= M - M_{\mu} \\
    M_{\mu} Y_{ijk} &= Y_{..}  =\mu + \alpha_{..} + \eta_{..} + \gamma_{..} \\\
    M_{\alpha} Y_{ijk} &= Y_{i.} - Y_{..} = \alpha_{i.} - \alpha_{..} + \gamma_{i.} - \gamma_{..}\\
    E \Big[ Y' M_{\mu} + M_{\alpha} Y \Big] &= tr \Big( \sigma^2 I_{a \times a} \Big) + E(Y)' M_{\mu} E(Y) +  E(Y)' M_{\alpha} E(Y)\\
    &= a \sigma^2 + bN \sum_{i=1}^a \Big(\alpha_{i.} - \alpha_{..} + \gamma_{i.} - \gamma_{..} \Big)^2 + abN \Big( \mu + \alpha_{..} + \eta_{..} + \gamma_{..}  \Big)^2 
\end{align*}

\end{itemize}

 