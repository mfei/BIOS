% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[thinc]{esdiff}
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{BIOS 767 HW1}
\author{Mingwei Fei}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{Problem 1 - Computing moments}
Consider the following model for subject i in group h at time j:
 
\begin{align*}
	Y_{hij} &= \mu + \alpha_h + \beta_j + \gamma_{hj} + b_{hi} + \epsilon_{hij}
\end{align*}

where $b_{hi} \sim N(0, \sigma^2_b)$ and $\epsilon_{hij} \sim N(0, \sigma^2_e)$, for $h=1,..H, i=1,..n_h,$ and $j=1,.. J$. Assume that random variables that do not share a common value of h or i are independent. 

\begin{itemize}
\item[(a)] Compute $E[Y_{hij}]$.
	
	\begin{align*}
		Y_{hij} &= \mu + \alpha_h + \beta_j + \gamma_{hj} + b_{hi} + \epsilon_{hij} \\
		E[Y_{hij}] &= \mu + \alpha_h + \beta_j + \gamma_{hj} + E[b_{hi}] + E[\epsilon_{hij}] \\
		 E[b_{hi}] &= 0, \qquad E[\epsilon_{hij}] = 0, \qquad \text{as} \quad b_{hi} \sim N(0, \sigma^2_b), \epsilon_{hij} \sim N(0, \sigma^2_e) \\
		E[Y_{hij}] &= \mu + \alpha_h + \beta_j + \gamma_{hj} 
	\end{align*}

\item[(b)] Don't assume $b_{hi}$ and $\epsilon_{hij}$ are independent. Derive an expression for $Var[Y_{hij}]$ that it as simplified as possible.

If $b_{hi}$ and $\epsilon_{hij}$ are not independent,
\begin{align*}
	Var[b_{hi} + \epsilon_{hij}] &= Var[b_{hi}] + Var[\epsilon_{hij}] +2 Cov[b_{hi}, \epsilon_{hij}] \\
	Y_{hij} &= \mu + \alpha_h + \beta_j + \gamma_{hj} + b_{hi} + \epsilon_{hij} \\
	Var[Y_{hij}] &= Var[b_{hi} + \epsilon_{hij}] \\
	Var[b_{hi}] &= \sigma^2_b, \qquad Var[\epsilon_{hij}] = \sigma^2_e, \qquad \text{as} \quad b_{hi} \sim N(0, \sigma^2_b), \epsilon_{hij} \sim N(0, \sigma^2_e) \\
	Var[Y_{hij}] &= \sigma^2_b + \sigma^2_e + 2 Cov[b_{hi}, \epsilon_{hij}]
\end{align*}

\item[(c)] assume $b_{hi}$ and $\epsilon_{hij}$ are independent. Derive an expression for $Var[Y_{hij}]$. 

If $b_{hi}$ and $\epsilon_{hij}$ are independent,
\begin{align*}
	Cov[b_{hi}, \epsilon_{hij}] &= 0\\
	Var[Y_{hij}] &= \sigma^2_b + \sigma^2_e
\end{align*}

\end{itemize}

\section{Problem 2 - Correlation in Data}
Consider the general linear regression model:

\begin{align*}
	Y_{n \times 1} &= X_{n \times p} \beta + \epsilon_{n \times 1}
\end{align*}

where $\epsilon$ is normal, and $\theta$ is unknown.

\begin{itemize}
	\item [(a)] Derive Jeffreys's prior for $\theta$.
	
	The Jeffreys's prior is non-informative prior distribution for a parameter space. 
	
\begin{align*}
	p(\theta) & \propto \sqrt{det I(\theta)}
\end{align*}	
	which is the determination of the Fisher information matrix. As r is known, so the only unknown parameter is $\theta$.
	
	The Fisher Information of $\theta$ is 
\begin{align*}
	ln p(x|\theta, r) &= ln {r+x-1 \choose x} + r ln \theta + x ln (1-\theta) \\
	ln(\theta) &= n r ln \theta + ln (1-\theta) \sum x \\
	I(\theta) &= \diffp{ln(\theta)^2}{\theta \theta}
\end{align*}	
	
	
	\item[(b)] Suppose the prior for $\theta$ is $\theta \sim beta(\alpha_0, \lambda_0)$, where $\alpha_0, \lambda_0$ are specified hyperparameters. Derive $E(X)$ and $Var(X)$.
	
	
\end{itemize}

\subsubsection{Prove Beta distribution}
The $Z_{n+1} = 1$, so we don't need to calculate this distribution. 
We can use the method in above problem to get the distribution of $Z_i$, and we need to get the joint distribution of $(Z_1, ... Z_n)$.

Let 
\begin{align*}
	U &=Y_1 + ...+ Y_i, \qquad V= Y_{i+1}+ .. + Y_{n+1} 
\end{align*}
Then $U \sim Gamma(i, \theta), V \sim Gamma(n+1-i, \theta)$, Let
\begin{align*}
 	Z_i &= U/(U+V), \qquad W = U+V 
\end{align*}

Consider the transformation $(U,V)^T \rightarrow (Z_i, W)^T$, note that the transform is one-to-one with the Jacobian
\begin{align*}
	\Bigg |\diffp{(U,V)}{{(Z_i, W)}} \Bigg | &= |W|
\end{align*}
 
For joint distribution of  $(U,V)^T$, 
\begin{align*}
	\frac{1}{\Gamma{(i)}} \theta exp(-\theta u) (\theta u)^{i-1} I(u > 0) \times \frac{1}{\Gamma{(n+1-i)}} \theta exp(-\theta v) (\theta v)^{n-i} I(v > 0)
\end{align*}
We obtain the joint density of $(Z_i, W)$ as 
\begin{align*}
&	\frac{1}{\Gamma{(i)}} \theta exp(-\theta z_i w) (\theta z_i w)^{i-1} \\
	& \times \frac{1}{\Gamma{(n+1-i)}} \theta exp(-\theta (1-z_i) w) (\theta (1-z_i) w)^{n-i} w \times I(0< z_i< 1) I(w > 0)
\end{align*}

Thus, the marginal density of $Z_i = X/(X+Y)$ is equal to
\begin{align*}
 & (1-z_i)^{n-i} z_i^{i-1} I(0< z_i <1)	\frac{\Gamma(n+1)}{\Gamma(i) \Gamma(n+1-i)} \int_{w} \theta exp(-\theta w) (\theta w)^n I(w>0) 	dw \\
 &= \frac{\Gamma(n+1)}{\Gamma(i) \Gamma(n+1-i)} (1-z_i)^{n-i} z_i^{i-1} I(0< z_i <1)
\end{align*}
That is $Z_i \sim Beta(i, n+1-i)$

\subsubsection{Prove Uniform distribution}



We need to note that, we always construct a joint distribution as we are using the $Y_1, ..., Y_{n+1}$ in joint distribution. So the transformed variables are also joint distribution.

Then we need to find the relationship between $Y_1, ..., Y_{n+1}$ and $(Z_1, ... Z_n)$. We can set the total sum as $S_{n+1} = Y_1+ .. + Y_{n+1}$.

\begin{align*}
	Y_1 &= Z_1 \times S_{n+1} \\
	Y_2 &= Z_2 \times S_{n+1} - Y_1 = (Z_2  - Z_1) \times S_{n+1}\\
	Y_3 &= Z_3 \times S_{n+1} - Y_1 - Y_2 = (Z_3  - Z_2) \times S_{n+1}\\
	.. & ..\\
	Y_n &= Z_n \times S_{n+1} - Y_1 - ...- Y_{n-1} = (Z_n  - Z_{n-1}) \times S_{n+1}\\
	Y_{n+1} &= S_{n+1} - Y_1 - ...- Y_{n} = (1- Z_n)  \times S_{n+1}
\end{align*}
Then we have the Jacobian transform distribution
\begin{align*}
	J &= \begin{pmatrix}
		\diffp{Y_1}{{Z_1}} & \diffp{Y_1}{{Z_2}} ... & \diffp{Y_1}{{Z_n}} & \diffp{Y_1}{{S_{n+1}}}\\
		..& ..& ..& ..\\
		\diffp{Y_n}{{Z_1}} & \diffp{Y_n}{{Z_2}}  ... & \diffp{Y_n}{{Z_n}} & \diffp{Y_n}{{S_{n+1}}}\\
		\diffp{Y_{n+1}}{{Z_1}} & \diffp{Y_{n+1}}{{Z_2}} & ...& \diffp{Y_{n+1}}{{S_{n+1}}}
	\end{pmatrix} =  \begin{pmatrix}
		S_{n+1} & 0 & 0 &.. & ..& Z_1  \\
		-S_{n+1} & S_{n+1} & 0 &.. &..& Z_2- Z_1  \\
		.. & .. & .. \\
		0 & 0&.. -S_{n+1} & S_{n+1}  &0 & Z_{n}- Z_{n-1}  \\
		0 & 0 &..& 0 & S_{n+1}& 1- Z_n\\
	\end{pmatrix}\\
	|J| &= S_{n+1}^{n}
\end{align*}

The joint distribution of $Y_1, ..., Y_{n+1}$
\begin{align*}
	f(Y_1, Y_2,.. Y_{n+1}) &= \prod_{i=1}^{n+1} \frac{1}{\theta} exp(-\frac{y_i}{\theta}) I(0< y_i < \infty) \\
	&= \frac{1}{\theta^{n+1}} exp(-\frac{\sum_{i=1}^{n+1} y_i}{\theta}) I(0< y_i < \infty)\\
	&= \frac{1}{\theta^{n+1}} exp(-\frac{S_{n+1}}{\theta}) I(0< y_i < \infty)
\end{align*}

Then the transformed variables
\begin{align*}
	f(Z_1, Z_2,.. Z_{n}, S_{n+1}) &= \frac{1}{\theta^{n+1}} exp(-\frac{S_{n+1}}{\theta}) S_{n+1}^{n} I(0< z_1 < z_2 <.. <z_n < 1) I(S_{n+1} > 0)
\end{align*}

We obtain the joint distribution of $(Z_1, ... Z_n)$

\begin{align*}
	f(Z_1, Z_2,.. Z_{n}) &= \int_{s} f(Z_1, Z_2,.. Z_{n}, S_{n+1}) dS \\
	&= \int_{s} \frac{1}{\theta^{n+1}} exp(-\frac{S}{\theta}) S^{n} I(0< z_1 < z_2 <.. <z_n < 1) I(S > 0) dS\\
	&= n! I(0< z_1 < z_2 <.. <z_n < 1), \qquad \text{Gamma integral}
\end{align*}
which is the joint density of order statistics of n uniform (0,1) random variables.




\section{Construct Distribution}

\subsection{Order Statistics}
The joint distribution of minimum and maximum:

Let's go for the joint cdf of the minimum and maximum 

\begin{align*}
	F_{X_{1}, X_{n}} (x, y) &= P(X_{(1)} \leq x, X_{(n)} \leq y)  
\end{align*}

Why do we start from cdf? It is much easier to get cdf than pdf, as the pdf need to take derivative, while cdf only needs to get the integral.

And if the observations are independent, the pdf and cdf also follows the same rule. 

We will need to write this in terms of the individual $X_i$ as the minimum and maximum are statistics of the individuals. Consider instead the relationship

\begin{align*}
	P{(X_{n}) \leq y} &= P(X_{(1)} \leq x, X_{(n)} \leq y) + P(X_{(1)} > x, X_{(n)} \leq y)
\end{align*}

This is the integral of x, which is a common in getting the marginal distribution from joint distribution.

We know how to write out the term on the left-hand side. The first term on the right-hand side is what we want to compute. As for the final term, $P(X_{(1)} > x, X_{(n)} \leq y)$,

note that this is 0 if $x > y$. So, we consider $x < y$

\begin{align*}
	P(X_{(1)} > x, X_{(n)} \leq y) &= P(x < X_{1} \leq y, x < X_{2} \leq y, .., x < X_{n} \leq y) \\
	&= [P(x < X_{1} \leq y)]^n, \qquad \text{i.i.d} \\
	&= [F(y)- F(x)]^n
\end{align*}

So, we have 

\begin{align*}
	F_{(X_{(1)}, X_{(n)})} (x,y) &= P(X_{(1)} \leq x, X_{(n)} \leq y) \\
	&= P(X_{n} \leq y) - P(X_{1} > x, X_{n} \leq y) \\
	&= [F(y)]^n - [F(y)- F(x)]^n
\end{align*}

Now the joint pdf is 

\begin{align*}
	f_{(X_{(1)}, X_{(n)})} (x,y) &= \frac{d}{dx} \frac{d}{dy} \{ [F(y)]^n - [F(y)- F(x)]^n\}\\
	&= \frac{d}{dx} n F(y)^{n-1} f(y) - n(F(y)- F(x))^{n-1} f(y) \\
	&= n (n-1) (F(y)- F(x))^{n-2} f(x) f(y)
\end{align*}

This hold for $x < y$ and for x and y both in the support of the original distribution.

\section{Moment Generating Function}

\subsection{Chi-square MGF}
We can get MGF for chi-square from $E[x^2 t]$ and $E[(\mu + Z)^2 t]$, where $Z \sim N(0,1)$.
Let's prove it in two methods:
\begin{itemize}
	\item [(i)] Method 1:
	
	\begin{align*}
		M_i(t) &= E[x^2 t] = \frac{1}{\sqrt{2\pi}} \int exp(x^2 t) exp \left( - \frac{(x-\mu)^2}{2} \right) dx\\
		&=  \frac{1}{\sqrt{2\pi}}  \int exp \left( (t- \frac{1}{2}) x^2 + \mu x -\frac{\mu^2}{2} \right) dx \\
		&=  \frac{1}{\sqrt{2\pi}}  \int exp \left(  -\frac{1}{2}(1-2t) \{ x^2 - \frac{2 \mu x}{(1-2t)} + \frac{\mu^2}{(1-2t)^2} \} + \frac{\mu^2}{2(1-2t)} -\frac{\mu^2}{2}  \right) dx \\
		&= \frac{1}{\sqrt{(1-2t)}} \int \frac{(1-2t)}{\sqrt{2\pi}} exp\left( -\frac{(x-\frac{\mu}{1-2t})^2}{2 (1-2t)^{-1}} \right) dx \left[exp \left( \frac{\mu^2 t}{1-2t} \right) \right]\\
		&=\frac{1}{\sqrt{(1-2t)}} exp \left( \frac{\mu^2 t}{1-2t} \right), \qquad \lambda = \mu^2\\
		&=\frac{1}{\sqrt{(1-2t)}} exp \left( \frac{\lambda t}{1-2t} \right)
	\end{align*}
	
	Then the MGF for $Q_i \sim \chi^2 _{k_i}(\lambda_i)$
	\begin{align*}
		M(t) &= E[ \sum_{i=1}^k x_i^2 t] = \prod_{i=1}^k M_i(t) \\
		&=\left(\frac{1}{\sqrt{(1-2t)}} \right)^k exp \left( \frac{ \sum_{i=1}^k \lambda_i t }{1-2t} \right)\\
		&=\left(\frac{1}{\sqrt{(1-2t)}} \right)^k exp \left( \frac{ \lambda t }{1-2t} \right)\\
		&= (1-2t)^{-k/2} exp \left( \frac{ \lambda t }{1-2t} \right), \qquad \text{i.i.d}
	\end{align*}
	
	\item[(ii)] Method 2:
	
	\begin{align*}
	M(t) &= E[(\mu + Z)^2 t] = \frac{1}{\sqrt{2\pi}} \int exp \left((\mu + Z)^2 t \right) exp \left( - \frac{Z^2}{2} \right) dz\\
	&=  \frac{1}{\sqrt{2\pi}}  \int exp \left( (t- \frac{1}{2}) z^2 + 2\mu t z + \mu^2 t \right) dz \\
	&=  \frac{1}{\sqrt{2\pi}}  \int exp \left(  -\frac{(1-2t)}{2} \{ z^2 - \frac{4 \mu t z}{(1-2t)} + \frac{2 \mu^2 t^2}{(1-2t)^2} \} + \frac{2 \mu^2 t^2}{(1-2t)} + \mu^2 t   \right) dz \\
	&= \frac{1}{\sqrt{(1-2t)}} \left[exp \left( \frac{\mu^2 t}{1-2t} \right) \right] \int \frac{1}{\sqrt{2\pi/(1-2t)}} exp\left( -\frac{(z-\frac{2 \mu t}{1-2t})^2}{2 (1-2t)^{-1}} \right) dz \\
	&=\frac{1}{\sqrt{(1-2t)}} exp \left( \frac{\mu^2 t}{1-2t} \right), \qquad t<1/2
\end{align*}


\end{itemize}

The general case of a linear combination of independent $\chi^2_{k_i}(\lambda_i)$

\begin{align*}
	Q &= \sum_{i=1}^k a_i Q_i
\end{align*}
We also can prove using MGF.

\subsection{Linear Combination of Chi-Square Distribution}
The linear combination of chi-square distribution $Y_j$. Let us denote by $X \sim \Gamma(r, \lambda)$ the fact that the r.v. X has a Gamma distribution with shape parameter r and rate parameter $\lambda$ 

\begin{align*}
	f_{X}(x) &= \frac{\lambda^x}{\Gamma (r)} exp(- \lambda x) x^{r-1}, \qquad (r, \lambda >0, x >0)
\end{align*}

Then we have, for $j=1,..p$,

\begin{align*}
	Y_j & \sim  \Gamma(\frac{k_j}{2}, \frac{1}{2}) \rightarrow Z_j = w_j Y_j \sim \Gamma(\frac{k_j}{2}, \frac{1}{2w_j})
\end{align*}

The MGF for linear combinations $Z_j = w_j Y_j$
\begin{align*}
	M(t) &= E[exp(Y_j t)] = (1-2t)^{-k/2} exp \left( \frac{ \lambda t }{1-2t} \right)\\
	M_{Z_j} (t) & = E[exp(w_j Y_j t)] = E[exp( Y_j (w_jt))] \\
	&= (1-2 w_jt)^{-1/2} exp \left( \frac{ \lambda w_j t }{1-2 w_j t} \right)
\end{align*}

\begin{align*}
	M_Y(t) &= E[exp(Y t)] = E[exp(t [w_1 Y_1 + w_2 Y_2 + w_3 Y_3 +.. w_n Y_n])]\\
	&= E[exp(w_1 t Y_1)]E[exp(w_2 t Y_2)]... E[exp(w_n t Y_n)]\\
	&= M_{X_1}(w_1 t) M_{X_2}(w_2 t) M_{X_3}(w_3 t).. M_{X_n}(w_n t)\\
	&= \prod_{i=1}^n M_{X_i}(w_i t)
\end{align*}

The third equation comes from the properties of exponents, as wells as from the expectation of the product of functions of independent random variables. 

I need to pay attention that, only under independent and identical situation, we can write
 \begin{align*}
 	M_Y(t) &= M_{X}(t)^n
 \end{align*}

Other than that, we can not further simplify that. So back to the non-central chi-square distribution, we have the MGF of Y
\begin{align*}
	M_Y(t) &= \prod_{i=1}^n M_{X_i}(w_i t)\\
	&=\prod_{i=1}^n  (1-2 w_jt)^{-1/2} exp \left( \frac{ \lambda w_j t }{1-2 w_j t} \right)
\end{align*}

Then we can see that the shape parameter is $\frac{1}{2 w_i}$. If we want to have a non-central chi-square distribution for $Y$, then all $w_j$ need to be the same. 


\subsection{Exponential Distribution Family}

KGF could be used to directly get the expectation and variance, more common than MGF. To get KGF, we will need to write distribution in exponential distribution.

Suppose the exponential distribution family is 

\begin{align*}
	f(Y, \theta) &= exp\left( \phi (y \theta - b(\theta) -c(y)) - 0.5s(y, \phi) \right)
\end{align*}

The MGF of exponential family

\begin{align*}
	M_Y(t) &= E[ exp(yt)] = \int exp(yt) exp\left( \phi (y \theta - b(\theta) -c(y)) - 0.5s(y, \phi) \right) \\
	&= \int  exp \left( \phi (y (\theta + t/\phi) - b(\theta) -c(y)) - 0.5s(y, \phi) \right) dy\\
	&= exp(\phi [b(\theta + t/\phi) - b(\theta)]) \int  exp\left( \phi (y (\theta + t/\phi) - b(\theta + t/\phi) -c(y)) - 0.5s(y, \phi) \right) dy\\
	&= exp(\phi [b(\theta + t/\phi) - b(\theta)]) 
\end{align*}

The KGF is $\phi [b(\theta + t/\phi) - b(\theta)]$, then we can get the expectation and variance
\begin{align*}
	E(y) &= \diffp{K(t)}{t} \Bigg |_{t=0} = \dot{b}(\theta)\\
	Var(y) &= \diffp{K(t)}{t t} \Bigg |_{t=0} = \phi^{-1} \ddot{b}(\theta)
\end{align*}

The MGF/KGF has shown that we can use the derivative function to get expectation or variance other than using the integral. The efficiency of computation also could be shown in the getting the covariance matrix using Fisher Information.


\section{Fisher Information}

The Fisher Information always comes with the asymptotic normal distribution of the estimator, and hypothesis testing.

\subsection{Multinomial Distribution}
Multinomial distribution is a very typical distribution to demonstrate the relationship between the covariance matrix and Fisher Information.

If the observations from multinomial distribution are independent, so we can construct the variance and covariance between two observations, and then get the covariance matrix. This step we can use the MGF or by definition.

But the Fisher Information don't use the inverse of Covariance, use the definition of Fisher Information, which is the variance of score function.

The log-likelihood function of Multinomial distribution 
\begin{align*}
	p(x, \theta) &= {n \choose x_1, x_2.. x_k} \theta_1^{x_1} \theta_2^{x_2} \theta_3^{x_3}.. \theta_k^{x_k} \\
	l_n(\theta) &= const + \sum_{i=1}^k x_i log(\theta_i)
\end{align*}

Then the score function

\begin{align*}
	S(x, \theta) &= \left(\frac{x_1}{\theta_1}, \frac{x_2}{\theta_2},.. \frac{x_k}{\theta_k} \right) = diag(\frac{1}{\theta}) x
\end{align*}

Consequently, the Fisher Information

\begin{align*}
	I_n(\theta) &= Var(S(\theta)) = diag(\frac{1}{\theta}) Var(x) diag(\frac{1}{\theta})\\
	&= n diag(1/\theta) [diag(\theta) - \theta \theta^T] diag(1/\theta)\\
	&= n \left[ diag(1/\theta) - (diag(1/\theta) \theta) (diag(1/\theta) \theta)^T  \right]\\
	&= n \left[ diag(1/\theta) - 11^T  \right]\\
	&= n \begin{pmatrix}
		\frac{1-p_1}{p_1} & -1 & -1 &.. & -1 \\
		-1 & \frac{1-p_2}{p_2} & -1 &.. &-1 \\
		..&..&..&..& \\
		-1 & -1 & -1... & & \frac{1-p_k}{p_k}
	\end{pmatrix}
\end{align*}



\section{Weighted Statistics}
Let $X_1,.. X_n$ be i.i.d $N(0, \sigma^2)$. $w_1, ..., w_n$ is a constant vector such that $w_1,.., w_n > 0$ and $w_1 + .. + w_n = 1$. Define $\bar{X}_{nw} = \sqrt{w_1} X_1 + .. + \sqrt{w_n} X_n$. Show that $Y_n = \bar{X}_{nw}/\sigma \sim N(0,1)$. 

\subsection{Question}

Note that $\bar{X}_{nw}$ is a linear combination of $X_1,.. X_n$, we need to use the vector/matrix to show the distribution, while not single one variable. 

If $X_i \sim N(\mu_i, \sigma_i^2)$, which we can have a MVN distribution, which each $X_i$ has its own normal distribution. Then the transformation matrix, orthogonal matrix, etc could be applied. Here all the $X_i$ follows the same distribution, and we also can use the similar concept by applying orthogonal matrix. 

Also we have the slutsky's theorem, delta method for the asymptotic distribution, however that is under the $n \rightarrow \infty$. In this problem, we can't use that.

So this is the exact distribution using the transformation (just the transform is by orthogonal matrix). The MGF or characteristic distribution is always the method when doing transformation.

\subsection{MGF}

\begin{align*}
	M(t) &= exp(\mu t + \sigma^2 t^2/2), \qquad \text{MGF for } N(\mu, \sigma^2)\\
	M_{\sqrt{w_i}X_i}(t) &= E[exp(\sqrt{w_i} t X_i)] = exp(\mu \sqrt{w_i} t + \sigma^2 [\sqrt{w_i} t]^2/2), \qquad \mu=0 \\
	&= exp(\sigma^2 w_i t^2/2)
\end{align*}
Then the linear combination $y_n$
\begin{align*}
	M_{Y_n}(t) &= E[exp \left( (\sqrt{w_1} X_1 + \sqrt{w_2} X_2 + .. + \sqrt{w_n} X_n) t \right)] \\
	&= E[exp(\sqrt{w_1} X_1 t)] E[exp(\sqrt{w_2} X_2 t)] E[exp(\sqrt{w_3} X_3 t)].. E[exp(\sqrt{w_n} X_n t)] \\
	&= exp(\sigma^2 w_1 t^2/2) exp(\sigma^2 w_2 t^2/2) exp(\sigma^2 w_3 t^2/2).. exp(\sigma^2 w_n t^2/2) \\
	&= exp(\sigma^2 [w_1+ w_2 + .. w_n] t^2/2) = exp(\sigma^2 t^2/2)
\end{align*}
So $Y_n \sim N(0, \sigma^2)$. 

\subsection{Orthogonal Matrix}

Consider an orthogonal matrix $\Sigma$ such that the first row is $(\sqrt{w_1}, \sqrt{w_2},.. \sqrt{w_n})$. Let

\begin{align*}
	(Z_1, Z_2,.. Z_n)^T &= \Sigma  (X_1, X_2,.. X_n)^T
\end{align*}
We have $Z^T Z = (\Sigma X)^T (\Sigma X) = X^T \Sigma^T \Sigma X = X^T X$. The characteristic function of Z is 

\begin{align*}
	\phi_Z(t) &= E[exp(i t'Z)] = E[exp(i (\Sigma' t)' X)] = exp(-\sigma^2 t't/2)
\end{align*}

Need to get familiar with the vector form in MGF/characteristic function

Therefore, we have $Z_1, .. Z_n \sim N(0, \sigma^2)$ 

\begin{align*}
	Y_n &= \bar{X}_{nw}/\sigma = (\sqrt{w_1} X_1 + \sqrt{w_2} X_2 + .. + \sqrt{w_n} X_n)/\sigma\\
	&= Z_1/\sigma \sim N(0,1)
\end{align*}

Also, 
\begin{align*}
	(n-1) S_n^2/\sigma^2 &= \sum_{i=1}^n (X_i^2 - \bar{X}_{nw}^2)/\sigma^2 \\
	&= (X^T X - Z_1^2)/\sigma^2 = (Z_2^2 + ... + Z_n^2)/\sigma^2 \sim \chi^2_{n-1}
\end{align*}

Since $Y_n$ and $S_n^2$ are functions of $Z_1$ and $(Z_2,.. Z_n)$ respectively, and from the independence of $Z_i, (i=1,..n)$, we have $Y_n$ and $S_n^2$ are independent. It follows that, by the definition of t-distribution, $T_n \sim t_{n-1}/\sigma$. When $w_1=w_2=..=w_n=1/n, Y_n = \sum_{i=1}^n X_i/(\sigma \sqrt{n})$, which is the standardized sample mean. Also,

\begin{align*}
	S_n^2/\sigma^2 &= \frac{\sum_{i=1}^n X_i^2 - \bar{X}_{nw}^2 }{n-1} \\
	&= \frac{\sum_{i=1}^n X_i^2 - {\sum_{i=1}^n X_i}^2/n }{n-1}\\
	&= \frac{\sum_{i=1}^n X_i^2 -  n\bar{X}_i^2 }{n-1}
\end{align*} 	   
which is the sample variance.


If there are quadratic forms, we can consider the orthogonal matrix that transform to standard normal distribution.

\section{Sufficient and Complete Statistics}


\subsection{Minimum Sufficient Statistics}


\subsection{Complete Statistics}


\subsection{Ancillary Statistics}




\section{Bivariate Normal Distribution / Partition Matrix}
The Bivariate Normal Distribution is always connected with partitioned covariance matrix. Assume vector (X, Y) is Gaussian. 

\textbf{Definition}: Two random variables X and Y are said to be bivariate normal, or jointly normal, if $aX+bY$ has a normal distribution for all $a,b \in R$.

If $X \sim N( \mu_X, \sigma^2_X)$ and $Y \sim N( \mu_Y, \sigma^2_Y)$ are jointly normal, then $ X + Y  \sim N( \mu_X + \mu_Y, \sigma^2_X + \sigma^2_Y + 2 \rho(X,Y) \sigma_X \sigma_Y) $. 

We consider $X + Y$ is a also normal distribution, then the covariance 

\begin{align*}
	Cov(X+Y) &= Cov(X) + Cov(Y) + 2Cov(XY) = \sigma^2_X + \sigma^2_Y + 2 \rho(X,Y) \sigma_X \sigma_Y
\end{align*}

How to provide a simple way to generate jointly normal random variables? The basic idea is that we can start from several independent random variables and by considering their linear combinations, we can obtain bivariate normal random variables. 

Let $Z_1$ and $Z_2$ be two independent N(0,1) random variables. Define

\begin{align*}
	X &= Z_1, \qquad Y= \rho Z_1 + \sqrt{1-\rho^2} Z_2
\end{align*}

where $\rho$ is a real number in (-1, 1). Show that X and Y are bivariate normal.

First, note that since $Z_1$ and $Z_2$ are normal and independent, they are jointly normal, with the joint PDF

\begin{align*}
	f_{Z_1, Z_2} (z_1, z_2) &= f_{Z_1}(z_1) f_{Z_2}(z_2) \\
	&= \frac{1}{2 \pi} exp \left(-\frac{1}{2} [z_1^2 + z_2^2] \right)
\end{align*}

We need to show $aX+bY$ is normal for all $a,b \in R$. We have

\begin{align*}
	aX + bY &= a Z_1 + b(\rho Z_1 + \sqrt{1- \rho^2} Z_2) \\
	&= (a + b\rho) Z_1 + b \sqrt{1- \rho^2} Z_2
\end{align*}

which is a linear combination of $Z_1$ and $Z_2$, and thus it is normal.


We can use the method of transformation to find the joint PDF of X and Y. The inverse transformation is given by

\begin{align*}
	Z_1 &= X = h_1(X, Y) \\
	Z_2 &= -\frac{\rho}{\sqrt{1-\rho^2}} X + \frac{1}{\sqrt{1-\rho^2}} Y = h_2(X, Y)
\end{align*}

We have

\begin{align*}
	f_{XY}(z_1, z_2) &= f_{Z_1, Z_2} (h_1(X, Y), h_2(X, Y)) |J|\\
	&= f_{Z_1, Z_2} (x, -\frac{\rho}{\sqrt{1-\rho^2}} x + \frac{1}{\sqrt{1-\rho^2}} y) |J|
\end{align*}

where 

\begin{align*}
	J &= det \begin{bmatrix}
		\diffp{h_1}{x} & \diffp{h_1}{y} \\
		\diffp{h_2}{x} & \diffp{h_2}{y}
	\end{bmatrix} = det \begin{bmatrix}
	1 & 0 \\
	-\frac{\rho}{\sqrt{1-\rho^2}} & \frac{1}{\sqrt{1-\rho^2}}
\end{bmatrix} = \frac{1}{\sqrt{1-\rho^2}}
\end{align*}

Thus, we conclude that,

\begin{align*}
	f_{XY}(z_1, z_2) &= \frac{1}{2 \pi \sqrt{1-\rho^2}} exp \left( -\frac{1}{2 (1- \rho^2)} [x^2 -2 \rho xy + y^2] \right)
\end{align*}


To find the $\rho$

\begin{align*}
	Var(X) &= Var(Z_1) =1 \\
	Var(Y) &= \rho^2 Var(Z_1) + (1- \rho^2) Var(Z_2) = 1 \\
	\rho(X,Y) &= Cov(X, Y) = Cov(Z_1, \rho Z_1 + \sqrt{1-\rho^2} Z_2) \\
	&= \rho Cov(Z_1, Z_2) + \sqrt{1-\rho^2} Cov(Z_1, Z_2) \\
	&= \rho
\end{align*}

Now, if you want two jointly normal random variables X and Y such that $X \sim N(\mu_X, \sigma^2_X), Y \sim N(\mu_Y, \sigma^2_Y)$, and $\rho(X,Y)= \rho$, you can start with two independent N(0,1) random variables, $Z_1$ and $Z_2$, and define

\begin{align*}
	X &= \sigma_X Z_1 + \mu_X \\
	Y &= \sigma_Y \left(\rho Z_1 + \sqrt{1-\rho^2} Z_2 \right)  + \mu_Y
\end{align*}

construction using Z1 and Z2 can be used to solve problems regarding bivariate normal distributions. Third, this method gives us a way to generate samples from the bivariate normal distribution using a computer program. 

\subsection{Conditional Distribution}

Suppose X and Y are jointly normal random variables with parameters $\mu_X, \sigma^2_X, \mu_Y, \sigma^2_Y$, and $ \rho $. Then, given $X=x, Y$ is normally distributed with

\begin{align*}
	E[Y|X=x] &= \mu_Y + \rho \sigma_Y \frac{x-\mu_X}{\sigma_X} \\
	Var(Y|X=x) &= (1- \rho^2) \sigma^2_Y
\end{align*}

One way to solve this problem is by using the joint PDF formula. In particular, since $X \sim N(\mu_X, \sigma^2_X), Y \sim N(\mu_Y, \sigma^2_Y)$, we can use

\begin{align*}
	f_{Y|X=x} (y|x) &= \frac{f_{XY} (x, y)}{f_X(x)}
\end{align*}
or we use

\begin{align*}
	X &= \sigma_X Z_1 + \mu_X \\
	Y &= \sigma_Y \left(\rho Z_1 + \sqrt{1-\rho^2} Z_2 \right)  + \mu_Y
\end{align*}

Thus, given $X=x$, 
\begin{align*}
	Z_1 &= \frac{x - \mu_X}{\sigma_X} \\
	Y &= \sigma_Y \rho \frac{x- \mu_X}{\sigma_X} + \sigma_Y \sqrt{1- \rho^2} Z_2 + \mu_Y 
\end{align*}

Since $Z_1$ and $Z_2$ are independent, knowing $Z_1$ does not provide any information on $Z_2$. We have shown that given $X=x, Y$ is a linear function of $Z_2$, thus it is normal. In particular

\begin{align*}
	E[Y|X=x] &= \sigma_Y \rho \frac{x- \mu_X}{\sigma_X} + \sigma_Y \sqrt{1- \rho^2} E[Z_2] + \mu_Y  \\
	 &= \mu_Y + \rho \sigma_Y \frac{x-\mu_X}{\sigma_X} \\
	Var[Y|X=x] &= \sigma_Y^2 (1- \rho^2) Var(Z_2) = (1- \rho^2) \sigma_Y^2
\end{align*}

\subsubsection{Marginal and conditional distributions of a multivariate normal vector}

A $K \times 1$ random vector X is multivariate normal if its joint probability density function is 
\begin{align*}
	f_X(x) &= (2\pi)^{-K/2} |det(V)|^{-1/2} exp(-\frac{1}{2} (x-\mu)^T V^{-1}(x-\mu)) 
\end{align*}

where $\mu$ is a $K \times 1$ mean vector, $V$ is a $K \times K$ covariance matrix.

Partition of the vector:

 We partition X into two sub-vectors $X_a$ and $X_b$ such that
\begin{align*}
	X &= \begin{pmatrix}
		X_a \\
		X_b
	\end{pmatrix}
\end{align*}
The sub-vectors $X_a$ and $X_b$ have dimensions $K_a \times 1$ and $K_b \times 1$ respectively. Moreover, $K_a + K_b = K$.

Partition of the parameters

We partition the mean vector and covariance matrix as follows:

\begin{align*}
	\mu &= \begin{pmatrix}
		\mu_a \\
		\mu_b
	\end{pmatrix}
\end{align*}
and 

\begin{align*}
	V &= \begin{pmatrix}
		V_a & V_{ab}^T \\
		V_{ab} & V_b
	\end{pmatrix}
\end{align*}

Normality of the sub-vectors

The marginal distributions of the two sub-vectors are also multivariate normal.

\subsubsection{Proof}
The random vector $X_a$ can be written as a linear transformation of X:

\begin{align*}
	X_a &= A X
\end{align*}
Where $A$ is a $K_a \times K$ matrix whose entries are either zero or one. Thus, $X_a $ has a multivariate normal distribution because it is a linear transformation of the multivariate normal random vector X and multivariate normality is preserved by linear transformations. Same as $X_b = B X$ where $B$ is a $K_b \times K$ matrix whose entries are either zero or one. 



Independence of the sub-vectors

$X_a$ and $X_b$ are independent if and only if $V_{ab} = 0$.

$X_a$ and $X_b$ are independent if and only if their joint moment generating function is equal to the product of their individual moment generating functions. Since $X_a$ is multivariate normal, its joint moment generating function is 

\begin{align*}
	M_{X_a}(t_a) &= exp(t^T_a \mu_a + \frac{1}{2} t_a^T V_a t_a) \\
	M_{X_b}(t_b) &= exp(t^T_b \mu_b + \frac{1}{2} t_b^T V_b t_b) 
\end{align*}

The joint moment generating function of $X_a$ and $X_b$, which is just the joint moment generating function of X, is

\begin{align*}
	M_{X_a, X_b}(t_a, t_b) &= M_{X}(t) \\
	&= exp(t^T \mu + \frac{1}{2} t^T V t) \\
	&= exp \left( [t_a^T t_b^T] \begin{bmatrix}
		\mu_a \\
		\mu_b
	\end{bmatrix} + \frac{}{} [t_a^T t_b^T] \begin{bmatrix}
	V_a & V_{ab}^T \\
	V_{ab} & V_b
\end{bmatrix} [t_a t_b] \right) \\
&= exp \left( t_a^T \mu_a + t_b^T \mu_b + \frac{1}{2} t_a^T V_a t_a + \frac{1}{2} t_b^T V_b t_b + \frac{1}{2} t_b^T V_{ab} t_a + \frac{1}{2} t_a^T V_{ab}^T t_b \right) \\
&= exp \left( t_a^T \mu_a + t_b^T \mu_b + \frac{1}{2} t_a^T V_a t_a + \frac{1}{2} t_b^T V_b t_b +  t_b^T V_{ab} t_a  \right) \\
&= exp \left( t_a^T \mu_a + \frac{1}{2} t_a^T V_a t_a \right) exp \left( t_b^T \mu_b + \frac{1}{2} t_b^T V_b t_b \right) exp( t_b^T V_{ab} t_a)
\end{align*}

from which it is obvious that $M_{X_a, X_b}(t_a, t_b) = M_{X_a}(t_a) M_{X_b}(t_b)$ if and only if $V_{ab} = 0$.

\subsection{Schur Complement}
In order to derive the conditional distributions, we are going to reply on Schur complements. 

In linear algebra and the theory of matrices, the Schur complement of a block matrix is defined as follows.

Suppose p, q are nonnegative integers, and suppose A, B, C, D are respectively $p \times p, p \times q, q \times p$, and $q \times q$ matrices of complex numbers. Let
\begin{align*}
M &= \begin{bmatrix}
	A & B \\
	C & D
\end{bmatrix}
\end{align*}

So that M is a $(p+q) \times (q + p)$ matrix. If D is invertible, then the \textbf{ Schuler complement
} of the block D of the matrix M is the $p \times p$ matrix defined by 

\begin{align*}
	M/D &:= A - B D^{-1} C
\end{align*}

If A is invertible, the Schur complement of the block A of the matrix M is the q × q matrix defined by

\begin{align*}
	M/A &:= D - C A^{-1} B
\end{align*}

In the case that A or D is singular, substituting a generalized inverse for the inverses on M/A and M/D yields the generalized Schur complement.

\subsubsection{Background}

The Schur complement arises when performing a block Gaussian elimination on the matrix M. In order to eliminate the elements below the block diagonal, one multiplies the matrix M by a block lower triangular matrix on the right as follows:

\begin{align*}
	M &= \begin{bmatrix}
		A & B \\
		C & D
	\end{bmatrix} \rightarrow \begin{bmatrix}
	A & B \\
	C & D
\end{bmatrix} \begin{bmatrix}
I_{p} & 0 \\
-D^{-1}C & I_{q}
\end{bmatrix} = \begin{bmatrix}
A - BD^{-1}C & B \\
0 & D
\end{bmatrix}
\end{align*}

where Ip denotes a $p \times p$ identity matrix. As a result, the Schur complement $ M/D=A-BD^{-1}C$ appears in the upper-left $p \times p$ block.

Continuing the elimination process beyond this point (i.e., performing a block Gauss–Jordan elimination),

\begin{align*}
\begin{bmatrix}
		A - BD^{-1}C & B \\
		0 & D
	\end{bmatrix} \rightarrow \begin{bmatrix}
	I_{p} & -BD^{-1} \\
	0 & I_{q}
\end{bmatrix} \begin{bmatrix}
A - BD^{-1}C & B \\
0 & D
\end{bmatrix} = \begin{bmatrix}
A - BD^{-1}C & 0 \\
0 & D
\end{bmatrix} 
\end{align*}

leads to an LDU decomposition of M, which reads

\begin{align*}
	M &= \begin{bmatrix}
	I_{p} & -BD^{-1} \\
	0 & I_{q}
\end{bmatrix}  \begin{bmatrix}
		A - BD^{-1}C & 0 \\
		0 & D
	\end{bmatrix}  \begin{bmatrix}
	I_{p} & 0 \\
	-D^{-1}C & I_{q}
\end{bmatrix}
\end{align*}

Thus, the inverse of M may be expressed involving $D^{-1}$ and the inverse of Schur's complement, assuming it exists, as

\begin{align*}
	M^{-1} &= \begin{bmatrix}
		A & B \\
		C & D
	\end{bmatrix}^{-1} = \left(\begin{bmatrix}
	I_{p} & BD^{-1} \\
	0 & I_{q}
\end{bmatrix}  \begin{bmatrix}
A - BD^{-1}C & 0 \\
0 & D
\end{bmatrix}  \begin{bmatrix}
I_{p} & 0 \\
D^{-1}C & I_{q}
\end{bmatrix} \right)^{-1} 
\end{align*}
Here I need to separate the inverse of $2 \times 2$ matrix and this partitioned matrix. 

\begin{align*}
\begin{bmatrix}
		I_{p} & -BD^{-1} \\
		0 & I_{q}
	\end{bmatrix} \begin{bmatrix}
	I_{p} & BD^{-1} \\
0 & I_{q}
\end{bmatrix} &= \begin{bmatrix} 
I_{p} & 0 \\
0 & I_{q}
\end{bmatrix}\\
\begin{bmatrix}
	I_{p} & -BD^{-1} \\
	0 & I_{q}
\end{bmatrix}^{-1} = \begin{bmatrix}
I_{p} & BD^{-1} \\
0 & I_{q}
\end{bmatrix} \\
\begin{bmatrix}
	I_{p} & 0 \\
	-D^{-1}C & I_{q}
\end{bmatrix} ^{-1} = \begin{bmatrix}
I_{p} & 0 \\
D^{-1}C & I_{q}
\end{bmatrix} 
\end{align*}

So, we have
\begin{align*}
	M^{-1} &=   \begin{bmatrix}
		I_{p} & 0 \\
		-D^{-1}C & I_{q}
	\end{bmatrix}   \begin{bmatrix}
		[A - BD^{-1}C]^{-1} & 0 \\
		0 & D^{-1}
	\end{bmatrix}  \begin{bmatrix}
	I_{p} & -BD^{-1} \\
	0 & I_{q}
\end{bmatrix}  
\end{align*}

If p and q are both 1 (i.e., A, B, C and D are all scalars), we get the familiar formula for the inverse of a 2-by-2 matrix:

\begin{align*}
	M^{-1} &= \frac{1}{AD-BC}  \begin{bmatrix}
		D & -B \\
		-C & A
	\end{bmatrix} 
\end{align*}

\subsubsection{Applications to probability theory and statistics}
Suppose the random column vectors X, Y live in $R_n$ and $R_m$ respectively, and the vector (X, Y) in $R_{n + m}$ has a multivariate normal distribution whose covariance is the symmetric positive-definite matrix

\begin{align*}
	\Sigma &= \begin{bmatrix}
		A & B \\
		B^T & C
	\end{bmatrix} 
\end{align*}

where $\mathbf{R} ^{n\times n}$ is the covariance matrix of X, ${ C\in \mathbf {R} ^{m\times m}}$ is the covariance matrix of Y and ${ B\in \mathbf {R} ^{n\times m}}$ is the covariance matrix between X and Y.

Then the conditional covariance of X given Y is the Schur complement of C in ${\textstyle \Sigma }{\textstyle \Sigma }$

\begin{align*}
	Cov(X|Y) &= A - B C^{-1}B^T \\
	E(X|Y) &= E(X) + B C^{-1} (Y- E(Y))
\end{align*}



Let $V_a$ be invertible. Let $V/V_a$ be the Schur complement of $V_a$ in V, defined as
\begin{align*}
	V/V_a &= V_b - V_{ab} V_{a}^{-1} V_{ab}^T
\end{align*}

If $V/V_a$ is invertible, then V is invertible 




\subsection{b}Consider the following
\begin{itemize}
	\item[(a)] For an arbitrary model, consider the conditional score statistic
	\begin{align*}
		U_{\psi}(\xi) &= \frac{\partial l_c(\xi, \psi_0)}{\partial \psi} |_{\psi_0=\psi}
	\end{align*} 
	Show that the conditional score statistic for any model can be written as
	\begin{align*}
		U_{\psi}(\xi) &= \partial_{\psi} log p(Y|\xi)- E[\partial_{\psi} log p(Y|\xi)|s_{\lambda}(\psi_0)]|_{\psi_0=\psi}
	\end{align*} 
	The conditional score statistic is the derivative of the conditional distribution
	\begin{align*}
		U_{\psi}(\xi) &= \frac{\partial l_c(\xi, \psi_0)}{\partial \psi} |_{\psi_0=\psi}\\
		p(\textbf{Y}| \xi) &= p(\textbf{Y}|s_{\lambda}(\psi_0), \xi) p(s_{\lambda}(\psi_0) | \xi), \qquad p(\textbf{Y}|s_{\lambda}(\psi_0), \xi) = \frac{p(\textbf{Y}| \xi)}{p(s_{\lambda}(\psi_0) | \xi)} \\
		l_c(\xi, \psi_0) &= log p(\textbf{Y}|s_{\lambda}(\psi_0), \xi)= log p(\textbf{Y}| \xi) - log p(s_{\lambda}(\psi_0) | \xi)
	\end{align*}
	Then we need to prove 
	\begin{align*}
		U_{\psi}(\xi) &= \frac{\partial l_c(\xi, \psi_0)}{\partial \psi} |_{\psi_0=\psi} = \partial_{\psi} log p(\textbf{Y}| \xi) - \partial_{\psi} log p(s_{\lambda}(\psi_0) | \xi)\\
		\partial_{\psi} log p(s_{\lambda}(\psi_0) | \xi) &= E[\partial_{\psi} log p(Y|\xi)|s_{\lambda}(\psi_0)]|_{\psi_0=\psi}
	\end{align*}
	We can write
	\begin{align*}
		log p(\textbf{Y}| \xi) &= log  p(\textbf{Y}|s_{\lambda}(\psi_0), \xi) + log p(s_{\lambda}(\psi_0) | \xi)\\
		E \left( \partial_{\psi}[log p(\textbf{Y}| \xi)| s_{\lambda}]\right) &= E \left(\partial_{\psi}[log  p(\textbf{Y}|s_{\lambda}(\psi_0), \xi)|s_{\lambda}]\right) + E \left(\partial_{\psi}[log p(s_{\lambda}(\psi_0), \xi)|s_{\lambda}]\right)
	\end{align*}    
	in which, the integral and expectation can switch, then we have
	\begin{align*}
		E \left(\partial_{\psi}[log  p(\textbf{Y}|s_{\lambda}(\psi_0), \xi)|s_{\lambda}]\right) & = \partial_{\psi} E \left([log  p(\textbf{Y}|s_{\lambda}(\psi_0), \xi)|s_{\lambda}]\right) = \partial_{\psi} E \left([log  p(\textbf{Y}| \xi)]\right)= 0
	\end{align*}      
	So,
	\begin{align*}
		E \left( \partial_{\psi}[log p(\textbf{Y}| \xi)| s_{\lambda}]\right) &= \partial_{\psi}log p(s_{\lambda}(\psi_0),\xi)
	\end{align*}
	Then we show
	\begin{align*}
		U_{\psi}(\xi) &= \partial_{\psi} log p(Y|\xi)- E[\partial_{\psi} log p(Y|\xi)|s_{\lambda}(\psi_0)]|_{\psi_0=\psi}
	\end{align*} 
	\item[(b)] Suppose that $y_1;.. y_n$ are independent and $y_i$ follows a Poisson distribution with mean $exp(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2})$, where $(x_{i1}; x_{i2})$ are covariates, $\lambda = (\lambda_0; \lambda_1)$ is the
	nuisance parameter vector and $\psi$  is the parameter of interest. Derive the conditional
	likelihood of $\psi$   and show that this conditional likelihood is free of $\lambda$.\\
	The joint distribution of $(y_1, · · · , y_n)$ is given by 
	\begin{align*}
		P(Y|\lambda, \psi)&=  exp \left( \sum_{i=1}^n y_i(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - log y_i! \right)
	\end{align*}
	Thus, $S_0 = \sum_{i=1}^n y_i$ is the sufficient and complete statistics for $\lambda_0$, and $S_1 = \sum_{i=1}^n y_i x_{i1}$ is the sufficient and complete statistics for $\lambda_1$.\\
	The conditional distribution of $\psi$ given $S_0, S_1$ is given by
	\begin{align*}
		p(\textbf{Y}, \psi|S=(S_0, S_1)) &= \frac{exp \left( \sum_{i=1}^n y_i(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - log y_i! \right)}{\sum_{y' \in S} exp \left( \sum_{i=1}^n y'_i(\lambda_0 + \lambda_1 x_{i1} +  \psi x_{i2}) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1 x_{i1} +  \psi x_{i2}) - log y'_i! \right)}\\
		&= \frac{exp \left( S_1 \lambda_0 + S_2 \lambda_1 +  S_3 \psi) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - log y_i! \right)}{\sum_{y' \in S} exp \left( S'_1\lambda_0 + S'_2 \lambda_1 + S'_3 \psi) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1 x_{i1} +  \psi x_{i2}) - log y'_i!\right)} \\
		&= \frac{exp \left( S_3 \psi  - log y_i!\right)}{\sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right)}, \qquad S_3 = \sum_{i=1}^n y_i x_{i2}, S'_3 = \sum_{i=1}^n y'_i x_{i2}
	\end{align*}
	which is independent of $\lambda$. \\
	\item[(c)] Derive the conditional score statistic for part (b) and write out a Newton-Raphson algorithm for obtaining the conditional maximum likelihood estimate of $\psi$  based on $U_{\psi}(\xi)$.\\
	The log likelihood of the conditional distribution is
	\begin{align*}
		l_c(\psi) &= S_3 \psi  - log y_i! -log \left[ \sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right) \right], \qquad S_3 = \sum_{i=1}^n y_i x_{i2}, S'_3 = \sum_{i=1}^n y'_i x_{i2}
	\end{align*} 
	The score function and observed fisher information is
	\begin{align*}
		U_{\psi}(\xi) &= \frac{\partial l_c(\xi, \psi_0)}{\partial \psi} |_{\psi_0=\psi}\\
		&= \psi - \frac{\sum_{y' \in S} S'_3 exp \left( S'_3 \psi - log y'_i! \right)}{\sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right)}\\
		\frac{\partial^2 l_c(\xi, \psi_0)}{\partial \psi^2} &= \left[ \frac{\sum_{y' \in S} S'_3 exp \left( S'_3 \psi - log y'_i! \right)}{\sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right)}\right]^2 - \frac{\sum_{y' \in S} S'^2_3 exp \left( S'_3 \psi - log y'_i! \right)}{\sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right)}
	\end{align*}
	The newton-Raphson algorithm
	\begin{align*}
		\psi^{k+1} &= \psi^{k} - \left[\frac{\partial^2 l_c(\psi^{k})}{\partial \psi^2} \right]^{-1} U_{\psi}(\psi^{k})
	\end{align*}
	where $\frac{\partial^2 l_c(\psi^{k})}{\partial \psi^2}, U_{\psi}(\psi^{k})$ are from above equations.
	
	\item[(d)] Now suppose that we only have two random variables $y_1 \sim Poisson(\mu_1)$ and $y_2 \sim
	Poisson(\mu_2)$, where $y_1$ and $y_2$ are independent. We are interested in making inferences on the ratio $\psi = \mu_1/\mu_2$. Let $\xi = (\psi , \lambda)$, where $\lambda$ represents the nuisance parameter.
	\begin{itemize}
		\item [(i)] Show that the log-likelihood function of $\xi$ can be written as
		\begin{align*}
			l(\xi) &= (y_1 + y_2)\lambda + y_1 log (\psi) - exp(\lambda) (1+\psi)
		\end{align*}
		where $\lambda$ is a function of $\mu_2$. Explicitly state what $\lambda$ is.\\
		Write the joint distribution of $y_1, y_2$
		\begin{align*}
			P(y_1, y_2) &= \frac{\mu_1^{y_1} e^{-\mu_1}}{y_1!} \frac{\mu_2^{y_2} e^{-\mu_2}}{y_2!} \\
			log P(y_1, y_2) &= y_1 log \mu_1 - \mu_1 + y_2 \log \mu_2 - \mu_2 - log y_1! - log y_2!\\
			&= y_1 log \frac{\mu_1}{\mu_2} + y_1 log \mu_2 + y_2 log \mu_2 -\mu_1 - \mu_2 -log y_1! - log y_2!\\
			&= y_1 log \frac{\mu_1}{\mu_2} + (y_1+y_2) log \mu_2 - \mu_2(\mu_1/\mu_2 + 1) -log y_1! - log y_2!
		\end{align*}
		where 
		\begin{align*}
			\psi &=log \frac{\mu_1}{\mu_2} \\
			\lambda &= log \mu_2
		\end{align*}
		\item[(ii)] Derive the conditional likelihood of $\psi$  and write out a Newton-Raphson algorithm for obtaining the conditional maximum likelihood estimate of $\psi$ .\\
		From part (a), we see $y_1 + y_2$ is the sufficient statistics for $\lambda$, while $y_1 + y_2 \sim Poission (\mu_1+\mu_2)$ then we have conditional distribution of $\psi$ condition on $S = y_1 + y_2$.
		\begin{align*}
			Y(\psi|S= y_1+y_2,\lambda) &= \frac{exp \left[ y_1 \psi + (y_1+y_2) \lambda - exp(\lambda)(\psi + 1) -log y_1! - log y_2! \right] }{exp \left[ (y_1+y_2) log (\mu_1+\mu_2) - (\mu_1+\mu_2) -log (y_1+y_2)!  \right]}\\
			&= \frac{exp \left[ y_1 \psi + S \lambda - exp(\lambda)(\psi + 1) -log y_1! - log y_2! \right] }{exp \left[ S (\lambda + log(\psi + 1)) -  exp(\lambda)(\psi + 1) -log S!  \right]}\\
			&= \frac{exp \left[ y_1 \psi -log y_1! - log y_2! \right] }{exp \left[ (y_1+ S-y_1) log(\psi + 1)) -log S!  \right]}\\
			&= {S \choose y_1} \left( \frac{\psi}{1+\psi}\right)^{y_1} \left(\frac{1}{1+\psi} \right)^{S-y_1}
		\end{align*}
		The conditional distribution is a binomial, $B(S, \psi/(1+\psi))$.\\
		The score function and observed fisher information 
		\begin{align*}
			log Y(\psi|S,\lambda) &= y_1 log \psi -S log(1+\psi) + log {S \choose y_1} \\
			\partial_{\psi} log Y(\psi|S,\lambda) &= \frac{y_1}{\psi} - \frac{S}{1+\psi} = 0, \qquad \hat{\psi} = y_1/(S-y_1)\\
			\partial^2_{\psi} log Y(\psi|S,\lambda) &= -\frac{y_1}{\psi^2} + \frac{S}{(1+\psi)^2}
		\end{align*}
		The $CMLE = \hat{\psi} = y_1/(S-y_1)$. And the newton-Raphson equation 
		\begin{align*}
			\psi^{k+1} &= \psi^{k} - \left[\frac{\partial^2 l_c(\psi^{k})}{\partial \psi^2} \right]^{-1} U_{\psi}(\psi^{k})\\
			&= \psi^{k} - \left[ -\frac{y_1}{\psi^2} + \frac{S}{(1+\psi)^2}\right]^{-1} \left[\frac{y_1}{\psi} - \frac{S}{1+\psi} \right]|_{\psi = \psi^{k}}\\
			&=  \psi^{k} + \frac{y_1/\psi^{k} - S/(1+\psi^{k})}{y_1/{\psi^{k}}^2 - S/(1+\psi^{k})^2}
		\end{align*}
	\end{itemize}
\end{itemize}



\end{document}
