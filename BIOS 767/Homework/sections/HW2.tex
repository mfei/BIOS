
\section{Problem 1- Derive fundamental properties of the multivariate normal distribution}

Consider the $n \times 1$ random vector Y satisfying $Y \sim MVN (\mu, \Sigma)$, $\Sigma$ is positive definite matrix.

\begin{itemize}
	\item [(a)] Let $Z = aY + c$, where $a$ is an $r \times n$ matrix of constants having rank $r (r \leq n)$ and $c$ is a $r \times 1$ vector of constants. Show using the definition of moment generating function that
	$Z \sim Normal (a \mu + c, a \Sigma a')$.  
	
As $Y \sim MVN(\mu, \Sigma)$, $\Sigma$ is positive definite matrix. 

Then we can decompose $\Sigma = B B^T$ for some nonsingular matrix B since $\Sigma$ is positive definite. 

 \begin{align*}
    X &= B^{-1} (Y- \mu), \qquad Y = \mu + B X 
\end{align*}

So we have $X_1, … X_n$ are independent standard normal, $X = (X_1,.. X_n)^T \sim MVN(0, I_n)$. 

 \begin{align*}
    M_Y(t) &=  E\Big[ e^{t^T Y}\Big ]=  E\Big[ e^{t^T (\mu + B X)}\Big ] = e^{t^T \mu} E \Big[ e^{l^T X} \Big], \qquad l^T = t^T B \\
    &= e^{t^T \mu} E \Big[ e^{\sum_{i=1}^n l_i Y_i} \Big], \qquad l= (l_1,.. l_n)\\
    &= e^{t^T \mu} \prod_{i=1}^n E \Big[ e^{ l_i Y_i} \Big] \\
    &= e^{t^T \mu} \prod_{i=1}^n e^{l_i^2/2}\\
    &= \exp \Big( \mu^T t + \frac{1}{2} l^T l \Big) \\
    &= \exp \Big( \mu^T t + \frac{1}{2} t^T \Sigma t \Big)
\end{align*}

While $Z = aY + c$, then we have the MGF for Z

 \begin{align*}
    M_Z(t) &=  E\Big[ e^{t^T Z}\Big ]=  E\Big[ e^{t^T (aY + c)}\Big ] = e^{t^T c} E \Big[ e^{t^T aY} \Big] \\
    &= e^{t^T c} E \Big[ e^{ (t^T a)Y} \Big] \\
    &= e^{t^T c} \exp \Big( \mu^T (a' t)^T + \frac{1}{2} (a' t)^T \Sigma (a' t) \Big) \\
    &= \exp \Big( t^T (a \mu + c) + \frac{1}{2} t^T a \Sigma a' t \Big) 
\end{align*}	

So $E(Z) = (a \mu + c), Var(Z) = a \Sigma a'$, and $Z \sim Normal (a \mu + c, a \Sigma a')$.


\end{itemize}

\section{Problem 2 - Develop a theoretical understanding of REML estimation}

$Y_i \sim MVN(X_i \beta, \Sigma)$

\begin{itemize}
\item[(a)] Read "Bayesian inference for variance components using only error contrasts" by Harville(1974). 

In the paper, an alternative method for deriving the likelihood function for error contrasts are provided using Bayesian method. I would like to walk through the derivation of the likelihood here in the homework.
The error contrasts are the product of $n-p$ independent normal distributions, 
 \begin{align*}
    M &= X(X'X)^{-1}X', \qquad \Sigma = \Sigma(\theta) \\
    I - M &= A A', \qquad A'A = I, \qquad \text{Cholesky decomposition} \\
    A' y & \sim MVN( A' X \hat{\beta}, A' \Sigma A) , \qquad Var(A'y) = A' Var(y) A = A' \Sigma A = \Sigma\\
    f_w(A' y| \theta, \hat{\beta}) &= ( 2\pi )^{- \frac{n-p}{2}} \Big( |\Sigma | \Big)^{-1/2} \exp \Big( -\frac{n-p}{2} (A'y - A' X \hat{\beta})^T (\Sigma)^{-1} (A'y - A' X \hat{\beta}) \Big)
\end{align*}

where $f_w(A' y| \theta, \hat{\beta})$ is the likelihood function of $\theta, \hat{\beta}$, and we could use Bayesian method to find the distribution of $\hat{\beta}$ and further integrate out $\hat{\beta}$ to get the distribution of $f_w(A' y| \theta)$ 

Let $\Sigma = Q Q'$, then we have the orthogonal projection matrix onto $C(Q^{-1}X)$
 \begin{align*}
P &= Q^{-1}y \Big( (Q^{-1} y)^T (Q^{-1} y) \Big)^{-1} (Q^{-1} y)'  = Q^{-1} y (y^T Q'^{-1} Q^{-1} y)^{-1} (Q^{-1}y)^T \\
&= \Sigma^{-1} y (y^T \Sigma^{-1} y)^{-1} y^T
\end{align*}

The estimate of $\hat{\beta}$ could be denoted as 

 \begin{align*}
G &= \Sigma^{-1} y (y^T \Sigma^{-1} y)^{-1} , \qquad G'Y = \hat{\beta}\\
 G' y & \sim MVN( \hat{\beta}, G' \Sigma G) , \qquad Var(G'y) = G' Var(y) G = G' \Sigma G = (Y' \Sigma^{-1} Y)^{-1} 
\end{align*}

We also have the relationship as below, it shows that the $f_w(\beta, \theta)$ are normal distribution, and the quadratic form could be decomposed into $f_w(\theta, \hat{\beta})$ and $f_{\hat{\beta}}(\beta)$. 

 \begin{align*}
    p(\beta | Y, \Sigma) & \propto \exp \Big( -\frac{1}{2} (Y- X {\beta})' \Sigma^{-1} (Y- X {\beta})  \Big) \\
    (Y- X \beta)' \Sigma^{-1} (Y- X \beta)  &= (Y- X \hat{\beta})' \Sigma^{-1} (Y- X \hat{\beta}) + (\beta - \hat{\beta})' (X \Sigma^{-1}X) (\beta - \hat{\beta})\\
    p(\beta | Y, \Sigma) & \propto \exp \Big((Y- X \hat{\beta})' \Sigma^{-1} (Y- X \hat{\beta}) + (\beta - \hat{\beta})' (X \Sigma^{-1}X) (\beta - \hat{\beta}) \Big) \\
    & \propto \exp \Big( (\beta - \hat{\beta})' (X \Sigma^{-1}X) (\beta - \hat{\beta}) \Big) 
\end{align*}

Also the paper provided the derivation using distribution of $A'y $, which is $f_w(\theta, \hat{\beta})$ and distribution for $G'y$, which is $f_{\hat{\beta}}(\beta)$. 

 \begin{align*}
    f_w(A' y| \theta) &= \int f_w(A' y| \theta, \hat{\beta}) f_{\hat{\beta}} (G'y | \theta, \beta) d\beta \\ 
    &= \Big(det(Y'Y) \Big)^{1/2} \int f_y(y| \theta, \beta) d\beta \\
    &= (2\pi)^{-\frac{1}{2} (n-p)}  \Big(det(Y'Y) \Big)^{1/2}  \Big(det(\Sigma) \Big)^{-1/2}  \Big(det(Y' \Sigma Y) \Big)^{-1/2} \\
    & \exp \Big(-\frac{1}{2} (y- X \hat{\beta})^T \Sigma^{-1} (y- X \hat{\beta}) \Big)
\end{align*}

Overall, this paper provided a Bayesian method to get the marginal distribution of $\Sigma(\theta)$, so that we can use the maximum likelihood method to estimate.

\item[(b)] Read the paper "A Conditional Derivation of Residual Maximum Likelihood" by Verbyla (1990).

This paper reviewed the Bayesian method in deriving likelihood function in Harville's paper and provided an alternative method in doing transformation without $\Sigma$. The approach is to construct the conditional distribution in matrix form. The estimate of $\hat{\beta}$ is based on the conditional distribution $Y_1|Y_2$ which based on the orthogonal projection matrix onto $C(Q^{-1}X)$.
 \begin{align*}
    \hat{\beta} &= (X' \Sigma^{-1} X)^{-1} X' \Sigma^{-1} Y
\end{align*}

And the REML log-likelihood for $\sigma^2$ and $\gamma$ is the marginal log-likelihood based on $Y_2$. The approach used the standard decomposition of the joint distribution of $L' Y$ into conditional and marginal distributions. The idea of residual maximum likelihood as a marginal likelihood is actually the idea of the use of a conditional likelihood.

\item[(c)] Derive the REML log-likelihood for $\Sigma$ 
$Y \sim X \beta + \epsilon$, where $\epsilon \sim N(0, \sigma^2 \Omega(\gamma))$
Let $L_1(n \times p)$ and $L_2(n \times (n-p))$ satisfying $L'_1 X = I_p$ and $L'_2 X = 0$. Also
 \begin{align*}
    L' Y&= [L'_1, L'_2]^T Y = [Y_1, Y_2]^T
\end{align*}

Thus, 
 \begin{align*}
    Y &= [Y_1, Y_2]^T \sim N \{ \mu, \Sigma \} \\
    \mu &= \begin{pmatrix}
     \beta \\
     0
    \end{pmatrix}, \qquad \Sigma = \sigma^2 \begin{pmatrix}
     L'_1 \Omega L_1 &  L'_1 \Omega L_2 \\
     L'_2 \Omega L_1 & L'_2 \Omega L_2
    \end{pmatrix},
\end{align*}

From the paper, $Y_2 \sim N(0, \sigma^2 L'_2 \Omega L_2) $, and the REML log-likelihood for $\sigma^2$ and $\gamma$ is the marginal log-likelihood based on $Y_2$ and is 

 \begin{align*}
    Log L(\sigma^2, \gamma, Y_2) &= -\frac{1}{2} \Big( (n-p) log \sigma^2 - log \det (L'_2 \Omega L_2) - \frac{Y'_2 (L'_2 \Omega L_2)^{-1} Y_2}{\sigma^2} \Big) \\
        &=  -\frac{1}{2} \Big( (n-p) log \sigma^2 - log \det ( \Omega ) - log \det (X' \Omega^{-1} X) - \frac{Y' P Y}{\sigma^2} \Big) \\
    P &= L_2 (L'_2 \Omega L_2)^{-1} L'_2 = \Omega^{-1} - \Omega^{-1} X (X' \Omega X)^{-1} X' \Omega^{-1} 
\end{align*}
where we kept only the terms associated with $\sigma^2$ and $\gamma$. We will use the following property in deriving the MLE

 \begin{align*}
    tr[x'Ax] &= tr[xx'A],\\
    \partial_{A} log |A| &= A^{-1}, \\
    \partial_{A} tr(BA) &= B, \text{if A is symmetric} \\
    Log L(\sigma^2, \gamma, Y_2) &=  -\frac{1}{2} \Big( (n-p) log \sigma^2 + \text{log} | \Omega^{-1}|- \text{log} | \text{tr}(XX' \Omega^{-1})| \\
        & - \frac{Y' \Big[\Omega^{-1} - \Omega^{-1} X (X' \Omega X)^{-1} X' \Omega^{-1}  \Big] Y}{\sigma^2} \Big) \\
    \partial_{\Omega^{-1}} Log f&= - \frac{1}{2} \Omega + \frac{1}{2} \Omega + \frac{1}{2 \sigma^2} YY' - \frac{1}{2 \sigma^2} YY' \Omega^{-1}X(X' \Omega X)^{-1} =0\\
    \partial_{\sigma^2} Log f&= -\frac{(n-p)}{2 \sigma^2}  - \frac{Y' P Y}{2 \sigma^4} =0
\end{align*}

We can solve $\sigma^2$ and $\gamma$ by above equations.

\begin{itemize}
\item[(i)] Verify Verbyla's representation is equivalent to equation (3) from Harville's paper. 
By looking at the equation(3) from Harville's paper
 \begin{align*}
    f_w(A' y| \theta) &= (2\pi)^{-\frac{(n-p)}{2} }  \Big(det(X'X) \Big)^{1/2}  \Big(\det(\Sigma) \Big)^{-1/2}  \Big(\det(Y' \Sigma Y) \Big)^{-1/2} \\
    & \exp \Big(-\frac{1}{2} (y- X \hat{\beta})^T \Sigma^{-1} (y- X \hat{\beta}) \Big)
\end{align*}

Let $\Sigma = \sigma^2 \Omega$, then 

\begin{align*}
\det(\Sigma) &= \det(\sigma^2 \Omega) = ( \sigma^2 )^{n-p} \det(\Omega) , \qquad \Sigma_{n-p \times n-p}\\
\det(\Sigma)^{-1/2} &= \det(\sigma^2 \Omega)^{-1/2} = ( \sigma^2 )^{-(n-p)/2} \det(\Omega)^{-1/2}
\end{align*}

So, also need to show that $\exp \Big(-\frac{1}{2} \frac{Y' P Y}{\sigma^2} \Big)$ is the same as $\exp \Big(-\frac{1}{2} (y- X \hat{\beta})^T \Sigma^{-1} (y- X \hat{\beta}) \Big)$
 \begin{align*}
\exp \Big(-\frac{1}{2} \frac{Y' P Y}{\sigma^2} \Big) &= \exp \Big(-\frac{1}{2} Y' \Big[ \Sigma^{-1} -  \Sigma^{-1} X (X'  \Sigma X)^{-1} X'  \Sigma^{-1} \Big]Y \Big) \\
&= \exp \Big(-\frac{1}{2} (y- X \hat{\beta})^T \Sigma^{-1} (y- X \hat{\beta}) \Big) , \qquad (X'  \Sigma X)^{-1} X'  \Sigma^{-1} Y = \hat{\beta} 
\end{align*}


From above equation, we can see that the two equations are the same from the two papers.

\item[(ii)] Explain how the REML estimator of $\beta$ in this setting can be viewed as a type of "plug-in" generalized least squares estimator where the weight matrix is equal to the REML estimate for $\Omega$.

The REML log-likelihood does not contain $\beta$, we could use the REML log-likelihood to estimate the $\Omega$, denoted by $\hat{\Omega}$. Furthermore, we can estimate the $\beta$ by the following

 \begin{align*}
\hat{\beta} &= (X' \hat{\Omega}^{-1}X)^{-1} X' \hat{\Omega}^{-1} Y 
\end{align*}

In the weighted least square estimator, 
 \begin{align*}
\tilde{\beta} &= (X' W X)^{-1} X' W Y , \qquad W= diag\{ w_1, …w_n\}
\end{align*}

Thus, the REML estimate of $\beta$ could be considered as WLS estimate with "plug-in" $W = \hat{\Omega}^{-1}$.
\end{itemize}

\item[(d)] Now, consider the non-regression case where each random vector $Y_i$ has mean $\mu$. Note that we may formulate this case as a regression model with matrix $X_{N n \times n}$ defined by

 \begin{align*}
X &= \begin{pmatrix}
X_1 \\
X_2 \\
.. \\
X_N
\end{pmatrix} = \begin{pmatrix}
I_n \\
I_n \\
.. \\
I_n
\end{pmatrix}
\end{align*}
and $\beta = \mu$. Show that the REML estimate for $\Sigma$ maximizes the log-likelihood given by

\begin{align*}
log l_{REML} (\Sigma) & \propto \frac{N-1}{2} log(\vert \Sigma^{-1} \vert) - \frac{1}{2} tr \Big[ (N-1) S \Sigma^{-1} \Big]
\end{align*}

Proof: From above, we have the REML log-likelihood as following:

\begin{align*}
ln_{REML}(\beta) & \propto {\frac{1}{2}}  (\log \vert \Omega^{-1}  \vert ) -\frac{1}{2} (\log \vert X' \Omega^{-1} X \vert )  -\frac{1}{2} (Y- X \hat{\beta})^T \Omega^{-1} (Y- \hat{\beta})  \\
\vert \Omega \vert &= \vert \Sigma \vert ^{N}, \qquad X' \Omega^{-1} X = \sum_{i=1}^N X' \Sigma^{-1} X \\
ln_{REML}(\beta)  & \propto  \frac{N}{2} (log \vert \Sigma^{-1} \vert ) -\frac{N}{2}  (\log \vert X' \Sigma^{-1} X \vert ) -\frac{1}{2} (\beta- \hat{\beta})^T X^T \Omega^{-1} X (\beta- \hat{\beta}) , \qquad X= I \\
& \propto  \frac{N}{2} (\log \vert \Sigma^{-1} \vert ) -\frac{1}{2}  (\log \vert N \Sigma^{-1}  \vert ) -\frac{1}{2} tr \Big[ \sum_{i=1}^N (Y_i - X_i \hat{\beta})^T \Sigma^{-1} (Y_i - X_i \hat{\beta})^T    \Big ], \\
& \propto  \frac{N}{2} (\log \vert \Sigma^{-1} \vert )- \frac{n}{2} log N -\frac{1}{2}  (\log \vert \Sigma^{-1}  \vert ) - \frac{1}{2} tr \Big[ \sum_{i=1}^N (Y_i - \bar{Y}_i) (Y_i - \bar{Y}_i))^T \Sigma^{-1}     \Big ]   \\
& \propto \frac{N-1}{2} log(\vert \Sigma^{-1} \vert) - \frac{1}{2} tr \Big[ (N-1) S \Sigma^{-1} \Big]
\end{align*}

We prove the REML log-likelihood function

\item[(e)] Derive the REML estimate for $\Sigma$ is the same as $S$.

We will take the first derivative of the REML log-likelihood function as in part(d) 

\begin{align*}
\partial_{\Sigma^{-1}}ln_{REML}(\beta) & = \frac{N-1}{2} \Sigma- \frac{N-1}{2} S = 0 \\
\Sigma &= S
\end{align*}


\end{itemize}

\section{Problem 3}

Objective: Develop understanding of applying the EM Algorithm by deriving the REML estimator for using an EM approach. 

Let $Y_i, i=1,..N$, be i.i.d. random sample of $n \times 1$ random vectors with $Y_i \sim MVN(\mu, \Sigma)$ for positive definite $\Sigma$. We will apply the EM algorithm to estimate $\Sigma$ and observe the estimator is equal to that obtained in problem 2. 

\begin{itemize}
\item[(a)] Write the complete data log-likelihood $l(\Sigma | \mu, Y)$. Note that we are writing the expression with $\mu$ on the RHS simply to convey that we are treating it effectively as missing data for this exercise.

The complete data log-likelihood, treating $\mu$ as missing,

\begin{align*}
p(\Sigma | \mu, Y_i) &= (2\pi)^{-\frac{n}{2}} \vert \Sigma \vert^{-\frac{1}{2}} \exp \Big( -\frac{1}{2} (Y_i - \mu)^T \Sigma^{-1} (Y_i - \mu) \Big) \\
ln(\Sigma | \mu, Y_i) & =-\frac{n}{2} log (2 \pi) - \frac{1}{2} \log  \vert \Sigma \vert - \frac{1}{2} (Y_i - \mu)^T \Sigma^{-1} (Y_i - \mu) \\
ln(\Sigma | \mu, Y) & =-\frac{Nn}{2} log (2 \pi) - \frac{N}{2} \log  \vert \Sigma \vert - \sum_{i=1}^N \frac{1}{2} (Y_i - \mu)^T \Sigma^{-1} (Y_i - \mu) 
\end{align*}

\item[(b)] Verify the conditional distribution for the missing data $\mu$ given the observed data $Y_i, i=1,..N$ is given by

\begin{align*}
\mu | Y, \Sigma^{(t)} & \sim Normal \Big( \bar{Y}, \frac{\Sigma^{(t)}}{N} \Big) 
\end{align*}

Prove: We know that the conditional distribution of $\mu$

\begin{align*}
f(\mu | \Sigma, Y) &= \frac{f(\mu, Y | \Sigma)}{f(Y| \Sigma)} \propto f(\mu, Y | \Sigma)
\end{align*}

When $\Sigma$ is known, we will have the distribution of $\mu$ 

\begin{align*}
f(\mu, Y | \Sigma) & \propto \prod_{i=1}^N \exp \Big( -\frac{1}{2} \sum_{i=1}^N (Y_i - \mu)^T \Sigma^{-1} (Y_i - \mu) \Big) 
\end{align*}

We will write the distribution in terms of known Y statistics and $\Sigma^{(t)}$ 

\begin{align*}
f(\mu, Y | \Sigma) & \propto \prod_{i=1}^N \exp \Big( -\frac{1}{2} \sum_{i=1}^N (Y_i^T  \Sigma^{-1} Y_i - 2 \mu^T \Sigma^{-1} Y_i + \mu^T \Sigma^{-1} \mu \Big) \\
& \propto \prod_{i=1}^N \exp \Big( -\frac{1}{2} \sum_{i=1}^N (\mu^T \Sigma^{-1} \mu - 2 \mu^T \Sigma^{-1} Y_i \Big) \\
& \propto \prod_{i=1}^N \exp \Big( -\frac{1}{2}(N \mu^T \Sigma^{-1} \mu - 2 N \mu^T \Sigma^{-1} \bar{Y_i} \Big) \\
& \propto \prod_{i=1}^N \exp \Big( -\frac{1}{2}( (\mu -  \bar{Y_i} )^T (N \Sigma^{-1}) (\mu -  \bar{Y_i} ) \Big) \\
& \sim N \Big(\bar{Y_i},  \frac{\Sigma}{N} \Big)
\end{align*}

We could replace $\Sigma$ by $\Sigma^{(t)}$, thus we have the proof.

\item[(c)] 	Compute $Q(\Sigma | \Sigma^{(t)})$ and derive the $\Sigma^{(t+1)}$ as a function of $\Sigma^{(t)}$. 

$Q(\Sigma | \Sigma^{(t)})$ is the expectation of $ln(\Sigma | \mu^{(t)}, Y) $

\begin{align*}
Q(\Sigma | \Sigma^{(t)}) &= E\Big[ ln(\Sigma | \mu, Y) \Big] \\
& = E \Big[ -\frac{Nn}{2} log (2 \pi) - \frac{N}{2} \log  \vert \Sigma \vert - \sum_{i=1}^N \frac{1}{2} (Y_i - \mu)^T \Sigma^{-1} (Y_i - \mu) \Big] \\
& \propto - \frac{N}{2}E \Big[ \log  \vert \Sigma \vert \Big] - \frac{1}{2} E \Big[ \sum_{i=1}^N  (Y_i - \mu)^T \Sigma^{-1} (Y_i - \mu)  \Big] \\
&  \propto - \frac{N}{2}E \Big[ \log  \vert \Sigma \vert \Big] - \frac{1}{2} E \Big[ \sum_{i=1}^N  Y_i^T \Sigma^{-1} Y_i - 2 Y_i^T  \Sigma^{-1}  \mu + \mu^T \Sigma^{-1} \mu \Big] 
\end{align*}

We have 
\begin{align*}
E[\mu] &= \bar{Y_i} \\
E[\mu^T \Sigma^{-1} \mu] &= E \Big[ tr \Big(  \mu \mu^T \Sigma^{-1} \Big) \Big] \\
&= tr \Big( E \Big[   \mu \mu^T \Sigma^{-1}  \Big] \Big) = \Sigma^{-1}  tr \Big( Var (\mu)  + E[\mu]^2 \Big) \\
&= tr \Big[ \Big( \frac{\Sigma^{(t)}}{N}  + \bar{Y}^{\otimes 2} \Big)\Sigma^{-1}  \Big] \\
\end{align*}

Then we have
\begin{align*}
Q(\Sigma | \Sigma^{(t)}) &  \propto - \frac{N}{2}E \Big[ \log  \vert \Sigma^{(t)} \vert \Big] - \frac{1}{2} E \Big[ \sum_{i=1}^N  Y_i^T \Sigma^{-1} Y_i - 2 Y_i^T  \Sigma^{-1}  \mu + \mu^T \Sigma^{-1} \mu \Big] \\
& \propto - \frac{N}{2} \log  \vert \Sigma^{(t)} \vert - \frac{1}{2} \sum_{i=1}^N  Y_i^T \Sigma^{-1} Y_i +  \sum_{i=1}^N Y_i^T  \Sigma^{(t)-1} \bar{Y_i} + \frac{N}{2} tr \Big( \frac{\Sigma^{(t)}}{N} \Sigma^{-1}  + \bar{Y}^{\otimes 2} \Sigma^{-1} \Big) \\
& \propto - \frac{N}{2}\log  \vert \Sigma^{(t)} \vert - \frac{1}{2} \sum_{i=1}^N  Y_i^T \Sigma^{-1} Y_i +  \sum_{i=1}^N Y_i^T  \Sigma^{(t) -1} \bar{Y_i} + \frac{N}{2} tr \Big( \frac{\Sigma^{(t)}}{N} \Sigma^{-1}  + \bar{Y}^{T} \Sigma^{-1} \bar{Y} \Big) \\
& \propto - \frac{N}{2} \log  \vert \Sigma^{(t)} \vert  - \frac{1}{2} tr \Big[ \sum_{i=1}^N (Y_i- \bar{Y})^T \Sigma^{-1} (Y_i - \bar{Y}) + \Sigma^{(t)} \Sigma^{-1}  \Big]\\
& \propto - \frac{N}{2} \log  \vert \Sigma^{(t)} \vert - \frac{1}{2} tr \Big[ \{ \sum_{i=1}^N (Y_i - \bar{Y})(Y_i- \bar{Y})^T + \Sigma^{(t)} \} \Sigma^{-1}  \Big]\\
& \propto - \frac{N}{2} \log  \vert \Sigma^{(t)} \vert - \frac{1}{2} tr \Big[ \{ (N-1)S + \Sigma^{(t)} \} \Sigma^{-1}  \Big]
\end{align*}

The $\Sigma^{(t+1)}$ could be derived by MLE of $Q(\Sigma | \Sigma^{(t)}) $
\begin{align*}
\partial_{\Sigma}Q(\Sigma | \Sigma^{(t)}) &= - \frac{N}{2}   \Sigma^{(t)}  - \frac{1}{2}  \{ (N-1)S + \Sigma^{(t)} \} = 0 \\
\Sigma &= \frac{N-1}{N} S + \frac{1}{N} \Sigma^{(t)} \\
\Sigma^{(t+1)} &= \frac{N-1}{N} S + \frac{1}{N} \Sigma^{(t)}
\end{align*}

The limiting of  $\Sigma^{(t+1)}$ 
\begin{align*}
\Sigma^{(t+1)} &= \frac{N-1}{N} S + \frac{1}{N} \Sigma^{(t)} \\
\Sigma^{(t+k)} &= \Big( \frac{N-1}{N} + \frac{N-1}{N} \frac{1}{N} + .. +  \frac{N-1}{N}  \frac{1}{N}^{k-1}  \Big) S + \frac{1}{N}^k \Sigma^{(t)} \\
\Sigma^{(t+k)} \xrightarrow{k \rightarrow \infty} S \frac{\frac{N-1}{N}}{1 - \frac{1}{N}} = S
\end{align*}

So $\hat{\Sigma_{MLE}} = S$
\end{itemize}

 

