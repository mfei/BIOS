\documentclass{homework}
\usepackage{amsmath}
\title{BIOS762 - Practice}
\author{Mingwei Fei}

\begin{document}

\maketitle

\exercise
\textbf{Chain Rule} Suppose that $y_1, ... y_n$ are independent bi-variate observations, where $y=(y_{i1}, y_{i2})'$ is a bivariate binary random vector such that $y_{ij}$ takes values 0 and 1 for $j=1,2$. Suppose that $y_i \sim QE(\theta, \lambda)$, where $QE(\theta, \lambda)$ is a bivariate distribution of quadratic exponential form 
\[
        p(y_i|\theta, \lambda) = (\Delta(\theta, \lambda))^{-1} exp\{y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\}
    \]
where $\Delta(\theta, \lambda)$ is the normalizing constant and $C(y_{i1},y_{i2})$ is a function that does not depend on $\theta=(\theta_1, \theta_2)$.
\begin{itemize}
    \item [(a)] Find an explicit expression for $\Delta(\theta, \lambda)$.
     \[
        \sum{p(y_i|\theta, \lambda)} = 1, \text{so we have }
        (\Delta(\theta, \lambda))^{-1} \sum{exp\{y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\}} = 1.  
        \]
       \[  
        exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\}, 
         \]
        Then we have
       \[  
        (\Delta(\theta, \lambda)) = exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\}
    \]
     \item [(b)] Derive the marginal distribution of $y_{i1}$ and the conditional distribution of $y_{i2}$ given $y_{i1}$. Specify a sufficient and necessary condition for $y_{i1}$ and $y_{i2}$ to be independent. \\
     The marginal distribution of $y_{i1}$:
\begin{align*}
    p(y_1|\theta, \lambda) &= (\Delta(\theta, \lambda))^{-1} \left(exp\{y_{i1}\theta_1 + \theta_2 + y_{i1}\lambda - C(y_{i1},1)\} + exp\{y_{i1}\theta_1 - C(y_{i1},0)\} \right)
\end{align*}
         
     The conditional distribution of $y_{i2}$ given $y_{i1}$:
     \[
        p(y_2|y_1) = \frac{p(y_1,y_2)}{p(y_1)} = \frac{(\Delta(\theta, \lambda))^{-1} exp\{y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\}}{(\Delta(\theta, \lambda))^{-1} (exp\{y_{i1}\theta_1 + \theta_2 + y_{i1}\lambda - C(y_{i1},1)\} + exp\{y_{i1}\theta_1 - C(y_{i1},0)\})}.  
        \]
        Then we have
        \[
        p(y_2|y_1) =  \frac{ exp\{y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\}}{exp\{y_{i1}\theta_1 + \theta_2 + y_{i1}\lambda - C(y_{i1},1)\} + exp\{y_{i1}\theta_1 - C(y_{i1},0)\}}.  
        \]
        For $y_{i1}$ and $y_{i2}$ to be independent, we need to have\\
        \[
        p(y_2|y_1) = p(y_2) \hspace{1cm} or  \hspace{1cm} 
        \]
For example, $C(y_{i1},y_{i2}) = H(y_1)H(y_2)$ so that we can cancel the $y_1$ item from the nominator and denominator. Also we need $\lambda = 0$ so that the conditional probability does not depend on $y_1$.\\
        
        
      \item[(c)] Let $\mu = E(y_i), \eta_{12} = E(y_{i1}y_{i2}), \sigma_{12}=Cov(y_{i1},y_{i2})$. Derive explicit expression for $\mu, \eta_{12}, \sigma_{12}$ based on the distribution.\\
      Derive the expectation of exponential family distribution by Bartlett identities. It is the transformation from working with $log(pi)$ to $y_i$.
      \begin{align*}
        E_{\xi}[\partial_j ln] &= 0\\
        E_{\xi}[\partial^2_{j,k} ln] + E_{\xi}[\partial_{j} ln \partial_{k} ln ] &= 0, \qquad E_{\xi}[\partial_{j} ln \partial_{k} ln ] = - E_{\xi}[\partial^2_{j,k} ln]\\
        I_n(\xi) &= E_{\xi}[\partial_{\xi} ln ^{\otimes^2}] = -E_{\xi}[\partial^2_{\xi} ln] = n \phi b_{\theta}^2(\theta)\\
        p(\theta, \phi) &= exp(\phi[y\theta - b(\theta) - c(y)] - \frac{1}{2}s(y_i, \phi))\\
        \int p(\theta, \phi) &= \int exp \left(\phi[y\theta - b(\theta) - c(y)] - \frac{1}{2}s(y_i, \phi) \right) dy\\
        1 &= exp[- \phi b(\theta)] \int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy\\
        exp[ \phi b(\theta)] &= \int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy\\
        \phi b(\theta) &= log \int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy
\end{align*}
Then when we take second derivative in terms of $b(\theta)$
      \begin{align*}
        \frac{\partial b(\theta)}{\partial \theta} &= \frac{1}{\phi} \partial_{\theta} \{ log \int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy \}\\
        &= \frac{1}{\phi} \frac{\int \phi y exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy}{\int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy}\\
        &= \int y exp \left(\phi[y\theta - b(\theta) - c(y)] - \frac{1}{2}s(y_i, \phi) \right) dy= E[y]\\
        \frac{\partial^2 b(\theta)}{\partial \theta^2} &= \int \phi (y^2 - y \frac{\partial b(\theta)}{\partial \theta}) exp \left(\phi[y\theta - b(\theta) - c(y)] - \frac{1}{2}s(y_i, \phi) \right) dy\\
        \phi^{-1} \frac{\partial^2 b(\theta)}{\partial \theta^2} &= E[YY^T] - E[Y]E[Y]^T= Var(Y)
\end{align*}
Further, we can derive expectation and variance from cumulant function, or from MGF.
\begin{align*}
        log(p(y_i|\theta, \lambda)) &= -log(\Delta(\theta, \lambda)) + y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\\
         b(\theta) &= log(\Delta(\theta, \lambda)) = log(exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\})\\
         \mu_1 = E(y_{i1}) &=\frac{\partial b(\theta)}{\partial \theta_1} = \Delta(\theta, \lambda)^{-1} (exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\})\\
         \mu_2 &= E(y_{i2}) =\frac{\partial b(\theta)}{\partial \theta_2} = \Delta(\theta, \lambda)^{-1} (exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_2 - C(0,1)\})
\end{align*}
    Because $y_{i1}y_{i2}$ is the sufficient statistics for $\lambda$\\
    \[
       \lambda = \frac{\partial b(\theta)}{\partial \lambda} = \Delta(\theta, \lambda)^{-1} exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\}
    \]
    \[
       \sigma_{12} =E[-\frac{\partial b^2(\theta)}{\partial \lambda^2}] =-\partial_{\lambda} [\Delta(\theta, \lambda)^{-1} exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\}]
    \]
    We can calculate covariance matrix from Fisher information by taking inverse, but that's very computing intensive. Instead we use covariance matrix definition:
    Also we can use expectation definition to get:\\
    \[
       \mu_1 = E(y_{i1}) = \Delta(\theta, \lambda)^{-1} exp(\theta_1)(exp\{\theta_2 + \lambda - C(1,1)\} + exp\{- C(1,0)\})
    \]
    \[
       \mu_2 = E(y_{i2}) = \Delta(\theta, \lambda)^{-1} exp(\theta_2)(exp\{\theta_1 + \lambda - C(1,1)\} + exp\{- C(0,1)\})
    \]
    \[
       \eta_{12} = E(y_{i1}y_{i2}) = P(y_{i1}= y_{i2} = 1) = \Delta(\theta, \lambda)^{-1} (exp\{\theta_1 + \theta_2+ \lambda - C(1,1)\})
    \]
    \begin{align*}
        \sigma_{12} = E(y_{i1}y_{i2}) - E(y_{i1})E(y_{i2}) 
    \end{align*}
       
    
    \begin{equation}
    \begin{split}
=   \Delta(\theta, \lambda)^{-1} (exp\{\theta_1 + \theta_2+ \lambda - C(1,1)\}) 
       - \Delta(\theta, \lambda)^{-1} exp(\theta_1)(exp\{\theta_2 + \lambda - C(1,1)\} + exp\{- C(1,0)\})\\
       \Delta(\theta, \lambda)^{-1} exp(\theta_2)(exp\{\theta_1 + \lambda - C(1,1)\} + exp\{- C(0,1)\})\\
       =\Delta(\theta, \lambda)^{-1}exp(\theta_1+\theta_2)\{ exp(\lambda - C(1,1))
       - \Delta(\theta, \lambda)^{-1} [exp(\theta_2+\lambda - C(1,1)+ exp(-C(1,0)]\\
       [exp(\theta_1+\lambda - C(1,1))+ exp(-C(0,1)] \}
\end{split}
\end{equation}
\item[(d)] Suppose consider a reparameterization from $(\theta,\lambda)$ to $(\mu,\eta_{12})$. Calculate the Jacobian transformation denoted as $V = \frac{\partial(\theta, \lambda)}{\partial(\mu, \eta_{12})}$. Use $V^{-1}$ to characterize the covariance matrix of $(y_1, y_2, y_1y_2)^{'}$ and specify sufficient and complete condition that the transform is one to one. \\
We need to find the link between $(\theta,\lambda)$ to $(\mu,\eta_{12})$, and the link is $\Delta(\theta, \lambda)$:\\
\begin{align*}
\frac{\partial \mu}{\partial \theta} &= \frac{\partial^2 b(\theta)}{\partial \theta} = \frac{\partial^2 log \Delta(\theta)}{\partial \theta}, \text{which is variance-covariance of } \theta\\
     \Delta(\theta, \lambda) &= exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\}   + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\}
\end{align*}

    Let  $A_{11} = exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\},  A_{10} = exp\{\theta_1 - C(1,0)\}, A_{01} =exp\{\theta_2 - C(0,1)\}, A_{00} =exp\{- C(0,0)\}$\\
    Then
     \[
      \frac{\partial (A_{00},A_{01},A_{1,0},A_{11})}{\partial (\theta,\lambda) } = 
      \begin{pmatrix}
  0 & 0 & 0 \\ 
  0 & A_{01} & 0 \\
  A_{10} & 0 & 0 \\
  A_{11} & A_{11} & A_{11}\\
\end{pmatrix}
    \] 
 \[
       V^{-1} = \frac{\partial (\mu,\eta_{12})}{\partial (\theta,\lambda) } = \frac{\partial (\mu,\eta_{12})}{\partial (A_{00},A_{01},A_{10},A_{11})) } \frac{\partial (A_{00},A_{01},A_{10},A_{11}))}{\partial (\theta,\lambda) }
    \]
    \[
       \mu_1 = \frac{A_{11} + A_{10}}{A_{00}+ A_{11} + A_{10}+ A_{01}}, \mu_2 = \frac{A_{11} + A_{01}}{A_{00}+ A_{11} + A_{10}+ A_{01}}, \eta_{12} = \frac{A_{11}}{A_{00}+ A_{11} + A_{10}+ A_{01}} 
    \]
    
 \[
       \frac{\partial (\mu,\eta_{12})}{\partial (A_{00},A_{01},A_{1,0},A_{11})) } = 
       \begin{pmatrix}
  \frac{-(A_{11} + A_{10})}{(A_{00}+ A_{11} + A_{10}+ A_{01})^2} & \frac{-(A_{11} + A_{10})}{\Delta^2} & \frac{(A_{01} + A_{00})}{\Delta^2} & \frac{(A_{01} + A_{00})}{\Delta^2} \\ 
  \frac{-(A_{11} + A_{01})}{\Delta^2} & \frac{(A_{10} + A_{00})}{\Delta^2} &  \frac{-(A_{11} + A_{01})}{\Delta^2} & \frac{(A_{10} + A_{00})}{\Delta^2} \\
  \frac{-(A_{11})}{\Delta^2} & \frac{-(A_{11})}{\Delta^2}&  \frac{-(A_{11})}{\Delta^2} & \frac{(A_{01} +A_{10} + A_{00})}{\Delta^2} \\
\end{pmatrix}
    \]
    So we have
    \[
       V^{-1} =  \begin{pmatrix}
  \frac{-(A_{11} + A_{10})}{(A_{00}+ A_{11} + A_{10}+ A_{01})^2} & \frac{-(A_{11} + A_{10})}{\Delta^2} & \frac{(A_{01} + A_{00})}{\Delta^2} & \frac{(A_{01} + A_{00})}{\Delta^2} \\ 
  \frac{-(A_{11} + A_{01})}{\Delta^2} & \frac{(A_{10} + A_{00})}{\Delta^2} &  \frac{-(A_{11} + A_{01})}{\Delta^2} & \frac{(A_{10} + A_{00})}{\Delta^2} \\
  \frac{-(A_{11})}{\Delta^2} & \frac{-(A_{11})}{\Delta^2}&  \frac{-(A_{11})}{\Delta^2} & \frac{(A_{01} +A_{10} + A_{00})}{\Delta^2} \\
\end{pmatrix} \begin{pmatrix}
  0 & 0 & 0 \\ 
  0 & A_{01} & 0 \\
  A_{10} & 0 & 0 \\
  A_{11} & A_{11} & A_{11}\\
\end{pmatrix}
    \]
    \[
       V^{-1} = \begin{pmatrix}
  \frac{(A_{01}+A_{00})(A_{10}+A_{11})}{\Delta^2} & \frac{-A_{10}A_{01} + A_{00}A_{11}}{\Delta^2} & \frac{(A_{01}+A_{00})A_{11}}{\Delta^2} \\ 
  \frac{-A_{10}A_{01} + A_{00}A_{11}}{\Delta^2} & \frac{(A_{10}+A_{00})(A_{01}+A_{11})}{\Delta^2} & \frac{(A_{10}+A_{00})(A_{11})}{\Delta^2} \\
  \frac{ (A_{00}+ A_{01})A_{11}}{\Delta^2} & \frac{A_{10}A_{11} + A_{00}A_{11}}{\Delta^2} & \frac{(A_{10}+A_{00}+A_{01})(A_{11})}{\Delta^2}  \\
\end{pmatrix} 
    \]
    \[
      = \begin{pmatrix}
  \frac{(1-\mu_1)\mu_1}{\Delta^2} & \frac{\eta_{12}-\mu_1\mu_2}{\Delta^2} & \frac{(1-\mu_1)\eta_{12}}{\Delta^2} \\ 
  \frac{\eta_{12}-\mu_1\mu_2}{\Delta^2} & \frac{(1-\mu_2)(\mu_2)}{\Delta^2} & \frac{(1-\mu_2)(\eta_{12})}{\Delta^2} \\
  \frac{(1-\mu_1)\eta_{12}}{\Delta^2} & \frac{(1-\mu_2)\eta_{12}}{\Delta^2} & \frac{\eta_{12}(1-\eta_{12})}{\Delta^2}  \\
\end{pmatrix} 
    \]
    Since $(y_1, y_2, y_1y_2)'$ are sufficient statistics for $(\theta_1, \theta_2, \lambda)$, so 
   \[ 
    Cov(y_1, y_2, y_1y_2) = \frac{\partial^2 log(\Delta(\theta, \lambda))}{\partial(\theta,\lambda)\partial(\theta, \lambda)^T} = V^{-1}
    \]
    So $V^{-1}$ is semi-positive.
    \item[(e)] Suppose that we also observe a $p \times 1$ column vector $x_i$ for each $i$ and that conditionally on $x_i$, $y_i \sim QE(\theta_i, \lambda_i)$, where $\theta_i = (\theta_{i1}, \theta_{i2})'$ and $\lambda_i$ may depend on $x_i$, for $i=1,2,.. n$. Consider the model\\
   \[ 
    E[y_i|x_i] = \mu_i = (\mu_{i1}, \mu_{i2} )' = \mu(x_i, \beta)
    \]    
   \[ 
    E[(y_{i1}-\mu_{i1})(y_{i2}-\mu_{i2})|x_i] = \sigma_{i12} = \sigma(x_i, \beta, \alpha)
    \]     
    $\alpha$ is unknown scalar, and $\beta$ is unknown $p\times 1$ vector. Derive likelihood score equation and fisher information for $\alpha, \beta$. Please clarify whether the score equations explicitly involve $C(y_{i1}, y_{i2})$. \\
    $\theta = \{(\theta_{i1}, \theta_{i2}), i=1,2,.. n \}, \lambda = (\lambda_1, \lambda_2, ...\lambda_n)$\\
   \[ 
    p[y_{i}|\theta_i, \lambda_i] = \Delta(\theta_i, \lambda_i){-1} exp(y_{i1}\theta_{i1} + y_{i2}\theta_{i2} + y_{i1}y_{i2}\lambda_{i} - C(y_{i1},y_{i2}))
    \] 
   \[ 
    ln(\theta, \lambda) = \sum_{i=1}^{n} \{-log (\Delta(\theta_i, \lambda_i)) (y_{i1}\theta_{i1} + y_{i2}\theta_{i2} + y_{i1}y_{i2}\lambda_{i} - C(y_{i1},y_{i2})) \}
    \] 
   \[ 
   ln(\theta, \lambda) = \sum_{i=1}^{n} \{-log (\Delta(\theta_i, \lambda_i)) + y_i' \begin{bmatrix}
           \theta \\
           \lambda \\
         \end{bmatrix}\}
    \]  
    The Likelihood score equation:\\
   \[ 
    \frac{\partial ln(\beta, \alpha)}{\partial \beta} =  \sum_{i=1}^{n} \frac{-1}{\Delta(\theta, \lambda)} \frac{\partial \Delta}{\partial (\theta, \lambda)} \frac{\partial (\theta, \lambda)}{\partial (\mu, \eta)} \frac{\partial (\mu, \eta)}{\partial (\beta, \alpha)} +  \frac{\partial (\theta, \lambda)}{\partial (\mu, \eta)} \frac{\partial (\mu, \eta)}{\partial (\beta, \alpha)} y_i'
    \]
   \[ 
     = \sum_{i=1}^{n} \frac{-1}{\Delta(\theta, \lambda)}[\frac{\partial \Delta}{\partial (\theta, \lambda)} V_i D]^T + [V_i D]^T y_i
    \]
   \[ 
     = \sum_{i=1}^{n} [\frac{\partial \Delta}{\partial (\theta, \lambda)}\frac{-1}{\Delta(\theta, \lambda)} + y_i]^T + [V_i D]^T
    \]
    
    \[
      \Delta(\theta, \lambda) = exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\}
    \] 
   \[ 
   \frac{\partial \Delta}{\partial (\theta, \lambda)}^T  = \begin{bmatrix}
           \frac{\partial \Delta}{\theta_1} \\
           \frac{\partial \Delta}{\theta_2} \\
           \frac{\partial \Delta}{\lambda} \\
         \end{bmatrix} \frac{1}{\Delta}
    \]   
   \[ 
    = \begin{bmatrix}
           exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} \\
           exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_2 - C(0,1)\}\\
           exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} \\
         \end{bmatrix} \frac{1}{\Delta} = \begin{bmatrix}
           \mu_1 \\
           \mu_2\\
           \eta_{12} \\
         \end{bmatrix}
    \] 
   \[ 
  D^T= \frac{\partial (\mu, \eta)}{\partial (\beta, \alpha)}  = \begin{bmatrix}
           \frac{\partial \mu_{i1}}{\partial \beta} & 0 \\
           \frac{\partial \mu_{i2}}{\partial \beta} & 0\\
           \frac{\partial \eta_{i12}}{\partial \beta} & \frac{\partial \eta_{i12}}{\partial \alpha}\\
         \end{bmatrix} 
    \]     
    The fisher information
   \[ 
   I (\alpha, \beta)= -E[SCn(\beta, \alpha)']  = \sum D_i^T V_i^T D_i
    \]    
    
    \item[(f)] If $y_{i1}, y_{i2}$ are continuous variables, will the results change?\\
    No, it won't affect. 
\end{itemize}





Let D denote having a certain disease and E denote having exposure to a certain risk factor. The attributable risk (AR) is the proportion of disease cases attributable to that exposure.\\
\begin{itemize}
    \item [(a)] Let $P(\bar{E}) = 1-P(E)$. Explain why 
\begin{align*}
     AR = [P(D) - P(D|\bar{E})]/P(D)
\end{align*}
Attributable risk = Incidence risk among an exposed group - Incidence risk among a non-exposed group\\
\begin{align*}
     P(D) &= P(D, E) + P(D, \bar{E})\\
     P(E)P(D|\bar{E}) &= [1-P(\bar{E})]P(D|\bar{E}) = P(D|\bar{E}) - P(D, \bar{E})
\end{align*}
The above formula changed the product of probability to subtraction of two probabilities.\\

\item [(b)] Show that AR relates to the relative risk RR by  
    \begin{align*}
     AR = [P(E)(RR-1)]/[1+ P(E)(RR-1)]\\
\end{align*}
\end{itemize}

\exercise
\textbf{Expectation, Variance Calculation for conditional/joint/marginal distribution}Consider a pair of random variables $(X, Y$ and let $\mu_x = E(X), \sigma_x^2= Var(X), \mu_y = E(Y), \sigma_y^2 = Var(Y), \sigma_{xy} = Cov(X,Y)$.
\begin{itemize}
    \item [(a)] Define $(\alpha_0, \beta_0)$ as the minimizers of $E[Y-(\alpha + \beta X)]^2$, where $\alpha$ and $\beta$ are real numbers and the expectation is with respect to the joint distribution of (X,Y). Derive explicit expression for $(\alpha_0, \beta_0)$ .
    \begin{align*}
    Cov(X,Y) &= E(XY) - E(X)E(Y), \qquad E(XY) = \sigma_{xy} + \mu_x\mu_y\\
    Var(Y) &= E(Y^2) - E(Y)^2, \qquad E(Y^2) = \sigma_y^2 + \mu_y^2\\
    Var(X) &= E(X^2) - E(X)^2, \qquad E(X^2) = \sigma_x^2 + \mu_x^2\\
     E[Y-(\alpha + \beta X)]^2 &= E[Y^2 + (\alpha + \beta X)^2 -2(\alpha + \beta X)Y ]\\
     &= E(Y^2) + (\alpha^2 + \beta^2 E(X^2) + 2\alpha\beta E(X)) - 2(\alpha E(Y) + \beta E(XY))\\
     &= \sigma_y^2+\mu_y^2 + \alpha^2 + \beta^2 (\sigma_x^2 + \mu_x^2) + 2\alpha\beta \mu_x -2\alpha \mu_y -2 \beta (\sigma_{xy}+ \mu_x\mu_y)\\
     \frac{\partial g(\alpha,\beta)}{\partial \alpha} &= 2\alpha + 2\beta \mu_x - 2\mu_y = 0\\
     \frac{\partial g(\alpha,\beta)}{\partial \beta} &=2\beta (\sigma_x^2 + \mu_x^2) + 2\alpha \mu_x -2(\sigma_{xy}+\mu_x\mu_y) = 0\\
     \alpha &= \mu_y - \beta \mu_x\\
     \beta &= \frac{\sigma_{xy}}{\sigma_x^2}
    \end{align*}    
    \item[(b)] Let U be a random variable which is independent of X and Y for which $E(U) = 0$ and $E(U^2) = \sigma_u^2$. Suppose one is to observe, instead of the pair (X,Y), the pair (W, Y), where W= X + U. We may think of X as being observed with some measurement error U. Define $\alpha_0^w$ and $\beta_0^w$ as the minimizers of $E([Y- (\alpha + \beta W)]^2)$, where the expectation is with respect to the joint distribution of (W, Y). Let $\lambda = \sigma_x^2/(\sigma_x^2 + \sigma_u^2)$. Derive expression for $\alpha_0^w$ and $\beta_0^w$ which only involve $\alpha_0, \beta_0, \lambda, \mu_x, \beta_0$.
    \begin{align*}
    E(WY) &= E((X+U)Y) = E(XY) + E(UY) = (\sigma_{xy} + \mu_x\mu_y), \qquad \text{independence of U, Y}\\
    Cov(W,Y) &= E(WY) - E(W)E(Y) = \sigma_{xy} + \mu_x\mu_y - \mu_x\mu_y =\sigma_{xy} \\
    Var(Y) &= E(Y^2) - E(Y)^2, \qquad E(Y^2) = \sigma_y^2 + \mu_y^2\\
    Var(W) &= Var(X+U) = = Var(X) + Var(U) = \sigma_x^2 + \sigma_u^2, \qquad E(X^2) = \sigma_x^2 + \mu_x^2\\
    E(W) &= E(X) = \mu_x\\
    E(W^2) &= Var(W) + E(W)^2 = \sigma_x^2 + 2\mu_x^2 \\
     E[Y-(\alpha + \beta W)]^2 &= E[Y^2 + (\alpha + \beta W)^2 -2(\alpha + \beta W)Y ]\\
     &= E(Y^2) + (\alpha^2 + \beta^2 E(W^2) + 2\alpha\beta E(W)) - 2(\alpha E(Y) + \beta E(WY))\\
     &= \sigma_y^2+\mu_y^2 + \alpha^2 + \beta^2 (\sigma_x^2 + 2\mu_x^2) + 2\alpha\beta \mu_x -2\alpha \mu_y -2 \beta (\sigma_{xy}+ \mu_x\mu_y)\\
     \frac{\partial g(\alpha,\beta)}{\partial \alpha} &= 2\alpha + 2\beta \mu_x - 2\mu_y = 0\\
     \frac{\partial g(\alpha,\beta)}{\partial \beta} &=2\beta (\sigma_x^2 + 2\mu_x^2) + 2\alpha \mu_x -2(\sigma_{xy}+\mu_x\mu_y) = 0\\
     \alpha &= \mu_y - \beta \mu_x\\
     \beta^w &= \frac{Cov(W,Y)}{Var(W)} = \frac{\sigma_{xy}}{\sigma_x^2 + \sigma_u^2} = \lambda \beta_0
    \end{align*} 
    \item[(c)] Suppose $Y=\alpha_0 + \beta_0 X + \epsilon$, where $E(\epsilon) = 0$ and $Var(\epsilon) = \sigma_\epsilon^2$. Show that we may write $Y=\alpha_0^w + \beta_0^w W + \eta$, where $E(\eta) = 0$ and $Var(\eta) = \sigma_\eta^2$, and find expressions for $\sigma_\eta^2$ only in terms of $\sigma_\epsilon^2, \lambda, \sigma_x^2, \beta$.\\
    \begin{align*}
    Y &=\alpha_0 + \beta_0 X + \epsilon\\
    &= \alpha_0^w + \beta_0^w W + (\alpha_0 - \alpha_w - \beta_0^w W + \beta_0 X + \epsilon)\\
    \eta &= \alpha_0 - \alpha_w - \beta_0^w W + \beta_0 X + \epsilon\\
    \alpha_0 &= \mu_y - \beta \mu_x\\
    \alpha_0^w &= \mu_y - \lambda \beta_0\mu_x, \qquad X= W-U\\
    \beta_0 &= \frac{\sigma_{xy}}{\sigma_x^2}, \qquad \beta_0^{w} = \lambda \beta_0\\
    \eta &=  \mu_y - \beta \mu_x -  \mu_y + \lambda \beta_0\mu_x + \beta_0 (W-U) -\lambda \beta_0 W + \epsilon\\
    &= \beta_0\mu_x(\lambda - 1) + \beta_0W (1-\lambda) -\beta_0 U + \epsilon\\
    E(\eta) &= \beta_0\mu_x(\lambda - 1)+ \beta_0 (1-\lambda) \mu_x =0\\
    \sigma_\eta^2 &= Var(\eta) = Var(\beta_0\mu_x(\lambda - 1) + \beta_0 W (1-\lambda) -\beta_0 U + \epsilon)\\
    &= beta_0^2(1-\lambda)^2Var(W) - \beta_0^2 Var(U) -2 \beta_0^2 (1-\lambda) Cov(W,U) + Var(\epsilon)\\
    &= \sigma_\epsilon^2 + \beta_0^2(1-\lambda)^2 (\sigma_x^2 + \sigma_u^2) + \beta_0^2\sigma_u^2 - 2\beta_0^2(1-\lambda) Cov(X+U, U)\\
    Cov(X+U, U) &= Cov(X,U) + Var(U) = \sigma_u^2\\
    \sigma_\eta^2 = \sigma_\epsilon^2 + \lambda\beta_0^2\sigma_u^2
    \end{align*}     
    \item[(d)] Consider a dataset with n independent realizations $(W_1, Y_1), ..., (W_n,Y_n)$ of $(W,Y)$ and another dataset with n independent realizations $(X_1, \Tilde{Y}_1),.. , (X_n, \Tilde{Y}_n)$ of $(X,Y)$, where $W= X+U$. Let $\hat{\alpha}^w, \hat{\beta}^w$ be the minimizers of $\frac{1}{n} \sum_{i=1}^{n} [Y_i - (\alpha+ \beta W_i)]^2$, so is $(\hat{\alpha}, \hat{\beta})$. \\
    \begin{itemize}
        \item [(i)] For large n, give an expression for $E(\hat{\beta}^w)$ and comment on whether $\hat{\beta}^w$ is a good estimator of $\beta_0$ in terms of bias and consistency.\\
        Similar as above, we have 
     \begin{align*}
    g(\alpha, \beta) &=\frac{1}{n} \sum_{i=1}^{n} [Y_i - (\alpha+ \beta W_i)]^2\\
    \hat{\alpha} &= \bar{Y} - \hat{\beta}^w \bar{X}, \qquad \bar{Y} = \frac{1}{n}\sum Y_i,\bar{X} = \frac{1}{n}\sum X_i \\
    \hat{\beta}^w &= \frac{\sum_{i=1}^n (W_i - \bar{W})(Y_i - \bar{Y})}{\sum_{i=1}^n (W_i-\bar{W})^2}, \qquad \bar{W} = \frac{1}{n} \sum W_i \\
    \hat{\beta}^w &= \frac{\hat{\sigma_{wy}}}{\hat{\sigma_w^2}} \xrightarrow[n]{ \infty} \frac{\sigma_{wy}}{\sigma_w^2(w)} = \lambda \beta_0\\
    \end{align*}      
    $\hat{\beta}^w$ is asymptotically bias for $\beta_0$, also it is not consistent.\\
    \item[(ii)] Suppose $\hat{\beta}^w$ is used to test the hypothesis: $H_0: \beta_0=0$ vs. $H_1: \beta_0 \neq 0$. Using the results in part (c), and for large n, derive an inequality only in terms of , as to when the t-stat for this hypothesis for the model in part (c) based on measurement error is smaller than the t-stat based on the model without measurement error.\\
    
    \item[(iii)] Examine the t-stat in part (ii) as a function of $\sigma_u^2$, and use the results in (ii) to examine the properties of this t- stat in terms of power and sample size as $\sigma_u^2 \rightarrow 0$ and $\sigma_u^2 \rightarrow \infty$ compared to a test stat based on $\hat{\beta}$.
    \end{itemize}
    \item[(e)] Suppose $(X,Y)^T \sim N_2(\mu, \Sigma)$, where $\mu= (\mu_x, \mu_y)'$ and $\Sigma = \begin{bmatrix}
           \sigma_x^2  &  \sigma_{xy}  \\
            \sigma_{xy} &   \sigma_{y}^2\\
         \end{bmatrix}$. i) Derive the conditional distribution of $Y|X$ and show that $E(Y|X) = \alpha_0 + \beta_0 X$. \\
         Construct $Z= Y+ AX$ so that Z is independent of X, $A= -\frac{\sigma_{xy}}{\sigma_x^2}$. Then the conditional distribution $Y|X$ will be only related to $X$.
    \end{itemize}
    

\end{document}
