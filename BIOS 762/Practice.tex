\documentclass{homework}
\usepackage{amsmath}
\title{BIOS762 - Practice}
\author{Mingwei Fei}

\begin{document}

\maketitle

\exercise
\textbf{Chain Rule} Suppose that $y_1, ... y_n$ are independent bi-variate observations, where $y=(y_{i1}, y_{i2})'$ is a bivariate binary random vector such that $y_{ij}$ takes values 0 and 1 for $j=1,2$. Suppose that $y_i \sim QE(\theta, \lambda)$, where $QE(\theta, \lambda)$ is a bivariate distribution of quadratic exponential form 
\[
        p(y_i|\theta, \lambda) = (\Delta(\theta, \lambda))^{-1} exp\{y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\}
    \]
where $\Delta(\theta, \lambda)$ is the normalizing constant and $C(y_{i1},y_{i2})$ is a function that does not depend on $\theta=(\theta_1, \theta_2)$.
\begin{itemize}
    \item [(a)] Find an explicit expression for $\Delta(\theta, \lambda)$.
     \[
        \sum{p(y_i|\theta, \lambda)} = 1, \text{so we have }
        (\Delta(\theta, \lambda))^{-1} \sum{exp\{y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\}} = 1.  
        \]
       \[  
        exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\}, 
         \]
        Then we have
       \[  
        (\Delta(\theta, \lambda)) = exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\}
    \]
     \item [(b)] Derive the marginal distribution of $y_{i1}$ and the conditional distribution of $y_{i2}$ given $y_{i1}$. Specify a sufficient and necessary condition for $y_{i1}$ and $y_{i2}$ to be independent. \\
     The marginal distribution of $y_{i1}$:
\begin{align*}
    p(y_1|\theta, \lambda) &= (\Delta(\theta, \lambda))^{-1} \left(exp\{y_{i1}\theta_1 + \theta_2 + y_{i1}\lambda - C(y_{i1},1)\} + exp\{y_{i1}\theta_1 - C(y_{i1},0)\} \right)
\end{align*}
         
     The conditional distribution of $y_{i2}$ given $y_{i1}$:
     \[
        p(y_2|y_1) = \frac{p(y_1,y_2)}{p(y_1)} = \frac{(\Delta(\theta, \lambda))^{-1} exp\{y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\}}{(\Delta(\theta, \lambda))^{-1} (exp\{y_{i1}\theta_1 + \theta_2 + y_{i1}\lambda - C(y_{i1},1)\} + exp\{y_{i1}\theta_1 - C(y_{i1},0)\})}.  
        \]
        Then we have
        \[
        p(y_2|y_1) =  \frac{ exp\{y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\}}{exp\{y_{i1}\theta_1 + \theta_2 + y_{i1}\lambda - C(y_{i1},1)\} + exp\{y_{i1}\theta_1 - C(y_{i1},0)\}}.  
        \]
        For $y_{i1}$ and $y_{i2}$ to be independent, we need to have\\
        \[
        p(y_2|y_1) = p(y_2) \hspace{1cm} or  \hspace{1cm} 
        \]
For example, $C(y_{i1},y_{i2}) = H(y_1)H(y_2)$ so that we can cancel the $y_1$ item from the nominator and denominator. Also we need $\lambda = 0$ so that the conditional probability does not depend on $y_1$.\\
        
        
      \item[(c)] Let $\mu = E(y_i), \eta_{12} = E(y_{i1}y_{i2}), \sigma_{12}=Cov(y_{i1},y_{i2})$. Derive explicit expression for $\mu, \eta_{12}, \sigma_{12}$ based on the distribution.\\
      Derive the expectation of exponential family distribution by Bartlett identities. It is the transformation from working with $log(pi)$ to $y_i$.
      \begin{align*}
        E_{\xi}[\partial_j ln] &= 0\\
        E_{\xi}[\partial^2_{j,k} ln] + E_{\xi}[\partial_{j} ln \partial_{k} ln ] &= 0, \qquad E_{\xi}[\partial_{j} ln \partial_{k} ln ] = - E_{\xi}[\partial^2_{j,k} ln]\\
        I_n(\xi) &= E_{\xi}[\partial_{\xi} ln ^{\otimes^2}] = -E_{\xi}[\partial^2_{\xi} ln] = n \phi b_{\theta}^2(\theta)\\
        p(\theta, \phi) &= exp(\phi[y\theta - b(\theta) - c(y)] - \frac{1}{2}s(y_i, \phi))\\
        \int p(\theta, \phi) &= \int exp \left(\phi[y\theta - b(\theta) - c(y)] - \frac{1}{2}s(y_i, \phi) \right) dy\\
        1 &= exp[- \phi b(\theta)] \int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy\\
        exp[ \phi b(\theta)] &= \int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy\\
        \phi b(\theta) &= log \int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy
\end{align*}
Then when we take second derivative in terms of $b(\theta)$
      \begin{align*}
        \frac{\partial b(\theta)}{\partial \theta} &= \frac{1}{\phi} \partial_{\theta} \{ log \int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy \}\\
        &= \frac{1}{\phi} \frac{\int \phi y exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy}{\int exp\left( \phi[y\theta  -c(y)]- \frac{1}{2}s(y_i, \phi)\right) dy}\\
        &= \int y exp \left(\phi[y\theta - b(\theta) - c(y)] - \frac{1}{2}s(y_i, \phi) \right) dy= E[y]\\
        \frac{\partial^2 b(\theta)}{\partial \theta^2} &= \int \phi (y^2 - y \frac{\partial b(\theta)}{\partial \theta}) exp \left(\phi[y\theta - b(\theta) - c(y)] - \frac{1}{2}s(y_i, \phi) \right) dy\\
        \phi^{-1} \frac{\partial^2 b(\theta)}{\partial \theta^2} &= E[YY^T] - E[Y]E[Y]^T= Var(Y)
\end{align*}
Further, we can derive expectation and variance from cumulant function, or from MGF.
\begin{align*}
        log(p(y_i|\theta, \lambda)) &= -log(\Delta(\theta, \lambda)) + y_{i1}\theta_1 + y_{i2}\theta_2 + y_{i1}y_{i2}\lambda - C(y_{i1},y_{i2})\\
         b(\theta) &= log(\Delta(\theta, \lambda)) = log(exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\})\\
         \mu_1 = E(y_{i1}) &=\frac{\partial b(\theta)}{\partial \theta_1} = \Delta(\theta, \lambda)^{-1} (exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\})\\
         \mu_2 &= E(y_{i2}) =\frac{\partial b(\theta)}{\partial \theta_2} = \Delta(\theta, \lambda)^{-1} (exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_2 - C(0,1)\})
\end{align*}
    Because $y_{i1}y_{i2}$ is the sufficient statistics for $\lambda$\\
    \[
       \lambda = \frac{\partial b(\theta)}{\partial \lambda} = \Delta(\theta, \lambda)^{-1} exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\}
    \]
    \[
       \sigma_{12} =E[-\frac{\partial b^2(\theta)}{\partial \lambda^2}] =-\partial_{\lambda} [\Delta(\theta, \lambda)^{-1} exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\}]
    \]
    We can calculate covariance matrix from Fisher information by taking inverse, but that's very computing intensive. Instead we use covariance matrix definition:
    Also we can use expectation definition to get:\\
    \[
       \mu_1 = E(y_{i1}) = \Delta(\theta, \lambda)^{-1} exp(\theta_1)(exp\{\theta_2 + \lambda - C(1,1)\} + exp\{- C(1,0)\})
    \]
    \[
       \mu_2 = E(y_{i2}) = \Delta(\theta, \lambda)^{-1} exp(\theta_2)(exp\{\theta_1 + \lambda - C(1,1)\} + exp\{- C(0,1)\})
    \]
    \[
       \eta_{12} = E(y_{i1}y_{i2}) = P(y_{i1}= y_{i2} = 1) = \Delta(\theta, \lambda)^{-1} (exp\{\theta_1 + \theta_2+ \lambda - C(1,1)\})
    \]
    \begin{align*}
        \sigma_{12} = E(y_{i1}y_{i2}) - E(y_{i1})E(y_{i2}) 
    \end{align*}
       
    
    \begin{equation}
    \begin{split}
=   \Delta(\theta, \lambda)^{-1} (exp\{\theta_1 + \theta_2+ \lambda - C(1,1)\}) 
       - \Delta(\theta, \lambda)^{-1} exp(\theta_1)(exp\{\theta_2 + \lambda - C(1,1)\} + exp\{- C(1,0)\})\\
       \Delta(\theta, \lambda)^{-1} exp(\theta_2)(exp\{\theta_1 + \lambda - C(1,1)\} + exp\{- C(0,1)\})\\
       =\Delta(\theta, \lambda)^{-1}exp(\theta_1+\theta_2)\{ exp(\lambda - C(1,1))
       - \Delta(\theta, \lambda)^{-1} [exp(\theta_2+\lambda - C(1,1)+ exp(-C(1,0)]\\
       [exp(\theta_1+\lambda - C(1,1))+ exp(-C(0,1)] \}
\end{split}
\end{equation}
\item[(d)] Suppose consider a reparameterization from $(\theta,\lambda)$ to $(\mu,\eta_{12})$. Calculate the Jacobian transformation denoted as $V = \frac{\partial(\theta, \lambda)}{\partial(\mu, \eta_{12})}$. Use $V^{-1}$ to characterize the covariance matrix of $(y_1, y_2, y_1y_2)^{'}$ and specify sufficient and complete condition that the transform is one to one. \\
We need to find the link between $(\theta,\lambda)$ to $(\mu,\eta_{12})$, and the link is $\Delta(\theta, \lambda)$:\\
\begin{align*}
\frac{\partial \mu}{\partial \theta} &= \frac{\partial^2 b(\theta)}{\partial \theta} = \frac{\partial^2 log \Delta(\theta)}{\partial \theta}, \text{which is variance-covariance of } \theta\\
     \Delta(\theta, \lambda) &= exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\}   + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\}
\end{align*}

    Let  $A_{11} = exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\},  A_{10} = exp\{\theta_1 - C(1,0)\}, A_{01} =exp\{\theta_2 - C(0,1)\}, A_{00} =exp\{- C(0,0)\}$\\
    Then
     \[
      \frac{\partial (A_{00},A_{01},A_{1,0},A_{11})}{\partial (\theta,\lambda) } = 
      \begin{pmatrix}
  0 & 0 & 0 \\ 
  0 & A_{01} & 0 \\
  A_{10} & 0 & 0 \\
  A_{11} & A_{11} & A_{11}\\
\end{pmatrix}
    \] 
 \[
       V^{-1} = \frac{\partial (\mu,\eta_{12})}{\partial (\theta,\lambda) } = \frac{\partial (\mu,\eta_{12})}{\partial (A_{00},A_{01},A_{10},A_{11})) } \frac{\partial (A_{00},A_{01},A_{10},A_{11}))}{\partial (\theta,\lambda) }
    \]
    \[
       \mu_1 = \frac{A_{11} + A_{10}}{A_{00}+ A_{11} + A_{10}+ A_{01}}, \mu_2 = \frac{A_{11} + A_{01}}{A_{00}+ A_{11} + A_{10}+ A_{01}}, \eta_{12} = \frac{A_{11}}{A_{00}+ A_{11} + A_{10}+ A_{01}} 
    \]
    
 \[
       \frac{\partial (\mu,\eta_{12})}{\partial (A_{00},A_{01},A_{1,0},A_{11})) } = 
       \begin{pmatrix}
  \frac{-(A_{11} + A_{10})}{(A_{00}+ A_{11} + A_{10}+ A_{01})^2} & \frac{-(A_{11} + A_{10})}{\Delta^2} & \frac{(A_{01} + A_{00})}{\Delta^2} & \frac{(A_{01} + A_{00})}{\Delta^2} \\ 
  \frac{-(A_{11} + A_{01})}{\Delta^2} & \frac{(A_{10} + A_{00})}{\Delta^2} &  \frac{-(A_{11} + A_{01})}{\Delta^2} & \frac{(A_{10} + A_{00})}{\Delta^2} \\
  \frac{-(A_{11})}{\Delta^2} & \frac{-(A_{11})}{\Delta^2}&  \frac{-(A_{11})}{\Delta^2} & \frac{(A_{01} +A_{10} + A_{00})}{\Delta^2} \\
\end{pmatrix}
    \]
    So we have
    \[
       V^{-1} =  \begin{pmatrix}
  \frac{-(A_{11} + A_{10})}{(A_{00}+ A_{11} + A_{10}+ A_{01})^2} & \frac{-(A_{11} + A_{10})}{\Delta^2} & \frac{(A_{01} + A_{00})}{\Delta^2} & \frac{(A_{01} + A_{00})}{\Delta^2} \\ 
  \frac{-(A_{11} + A_{01})}{\Delta^2} & \frac{(A_{10} + A_{00})}{\Delta^2} &  \frac{-(A_{11} + A_{01})}{\Delta^2} & \frac{(A_{10} + A_{00})}{\Delta^2} \\
  \frac{-(A_{11})}{\Delta^2} & \frac{-(A_{11})}{\Delta^2}&  \frac{-(A_{11})}{\Delta^2} & \frac{(A_{01} +A_{10} + A_{00})}{\Delta^2} \\
\end{pmatrix} \begin{pmatrix}
  0 & 0 & 0 \\ 
  0 & A_{01} & 0 \\
  A_{10} & 0 & 0 \\
  A_{11} & A_{11} & A_{11}\\
\end{pmatrix}
    \]
    \[
       V^{-1} = \begin{pmatrix}
  \frac{(A_{01}+A_{00})(A_{10}+A_{11})}{\Delta^2} & \frac{-A_{10}A_{01} + A_{00}A_{11}}{\Delta^2} & \frac{(A_{01}+A_{00})A_{11}}{\Delta^2} \\ 
  \frac{-A_{10}A_{01} + A_{00}A_{11}}{\Delta^2} & \frac{(A_{10}+A_{00})(A_{01}+A_{11})}{\Delta^2} & \frac{(A_{10}+A_{00})(A_{11})}{\Delta^2} \\
  \frac{ (A_{00}+ A_{01})A_{11}}{\Delta^2} & \frac{A_{10}A_{11} + A_{00}A_{11}}{\Delta^2} & \frac{(A_{10}+A_{00}+A_{01})(A_{11})}{\Delta^2}  \\
\end{pmatrix} 
    \]
    \[
      = \begin{pmatrix}
  \frac{(1-\mu_1)\mu_1}{\Delta^2} & \frac{\eta_{12}-\mu_1\mu_2}{\Delta^2} & \frac{(1-\mu_1)\eta_{12}}{\Delta^2} \\ 
  \frac{\eta_{12}-\mu_1\mu_2}{\Delta^2} & \frac{(1-\mu_2)(\mu_2)}{\Delta^2} & \frac{(1-\mu_2)(\eta_{12})}{\Delta^2} \\
  \frac{(1-\mu_1)\eta_{12}}{\Delta^2} & \frac{(1-\mu_2)\eta_{12}}{\Delta^2} & \frac{\eta_{12}(1-\eta_{12})}{\Delta^2}  \\
\end{pmatrix} 
    \]
    Since $(y_1, y_2, y_1y_2)'$ are sufficient statistics for $(\theta_1, \theta_2, \lambda)$, so 
   \[ 
    Cov(y_1, y_2, y_1y_2) = \frac{\partial^2 log(\Delta(\theta, \lambda))}{\partial(\theta,\lambda)\partial(\theta, \lambda)^T} = V^{-1}
    \]
    So $V^{-1}$ is semi-positive.
    \item[(e)] Suppose that we also observe a $p \times 1$ column vector $x_i$ for each $i$ and that conditionally on $x_i$, $y_i \sim QE(\theta_i, \lambda_i)$, where $\theta_i = (\theta_{i1}, \theta_{i2})'$ and $\lambda_i$ may depend on $x_i$, for $i=1,2,.. n$. Consider the model\\
   \[ 
    E[y_i|x_i] = \mu_i = (\mu_{i1}, \mu_{i2} )' = \mu(x_i, \beta)
    \]    
   \[ 
    E[(y_{i1}-\mu_{i1})(y_{i2}-\mu_{i2})|x_i] = \sigma_{i12} = \sigma(x_i, \beta, \alpha)
    \]     
    $\alpha$ is unknown scalar, and $\beta$ is unknown $p\times 1$ vector. Derive likelihood score equation and fisher information for $\alpha, \beta$. Please clarify whether the score equations explicitly involve $C(y_{i1}, y_{i2})$. \\
    $\theta = \{(\theta_{i1}, \theta_{i2}), i=1,2,.. n \}, \lambda = (\lambda_1, \lambda_2, ...\lambda_n)$\\
   \[ 
    p[y_{i}|\theta_i, \lambda_i] = \Delta(\theta_i, \lambda_i){-1} exp(y_{i1}\theta_{i1} + y_{i2}\theta_{i2} + y_{i1}y_{i2}\lambda_{i} - C(y_{i1},y_{i2}))
    \] 
   \[ 
    ln(\theta, \lambda) = \sum_{i=1}^{n} \{-log (\Delta(\theta_i, \lambda_i)) (y_{i1}\theta_{i1} + y_{i2}\theta_{i2} + y_{i1}y_{i2}\lambda_{i} - C(y_{i1},y_{i2})) \}
    \] 
   \[ 
   ln(\theta, \lambda) = \sum_{i=1}^{n} \{-log (\Delta(\theta_i, \lambda_i)) + y_i' \begin{bmatrix}
           \theta \\
           \lambda \\
         \end{bmatrix}\}
    \]  
    The Likelihood score equation:\\
   \[ 
    \frac{\partial ln(\beta, \alpha)}{\partial \beta} =  \sum_{i=1}^{n} \frac{-1}{\Delta(\theta, \lambda)} \frac{\partial \Delta}{\partial (\theta, \lambda)} \frac{\partial (\theta, \lambda)}{\partial (\mu, \eta)} \frac{\partial (\mu, \eta)}{\partial (\beta, \alpha)} +  \frac{\partial (\theta, \lambda)}{\partial (\mu, \eta)} \frac{\partial (\mu, \eta)}{\partial (\beta, \alpha)} y_i'
    \]
   \[ 
     = \sum_{i=1}^{n} \frac{-1}{\Delta(\theta, \lambda)}[\frac{\partial \Delta}{\partial (\theta, \lambda)} V_i D]^T + [V_i D]^T y_i
    \]
   \[ 
     = \sum_{i=1}^{n} [\frac{\partial \Delta}{\partial (\theta, \lambda)}\frac{-1}{\Delta(\theta, \lambda)} + y_i]^T + [V_i D]^T
    \]
    
    \[
      \Delta(\theta, \lambda) = exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} + exp\{\theta_2 - C(0,1)\} + exp\{- C(0,0)\}
    \] 
   \[ 
   \frac{\partial \Delta}{\partial (\theta, \lambda)}^T  = \begin{bmatrix}
           \frac{\partial \Delta}{\theta_1} \\
           \frac{\partial \Delta}{\theta_2} \\
           \frac{\partial \Delta}{\lambda} \\
         \end{bmatrix} \frac{1}{\Delta}
    \]   
   \[ 
    = \begin{bmatrix}
           exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_1 - C(1,0)\} \\
           exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} + exp\{\theta_2 - C(0,1)\}\\
           exp\{\theta_1 + \theta_2 + \lambda - C(1,1)\} \\
         \end{bmatrix} \frac{1}{\Delta} = \begin{bmatrix}
           \mu_1 \\
           \mu_2\\
           \eta_{12} \\
         \end{bmatrix}
    \] 
   \[ 
  D^T= \frac{\partial (\mu, \eta)}{\partial (\beta, \alpha)}  = \begin{bmatrix}
           \frac{\partial \mu_{i1}}{\partial \beta} & 0 \\
           \frac{\partial \mu_{i2}}{\partial \beta} & 0\\
           \frac{\partial \eta_{i12}}{\partial \beta} & \frac{\partial \eta_{i12}}{\partial \alpha}\\
         \end{bmatrix} 
    \]     
    The fisher information
   \[ 
   I (\alpha, \beta)= -E[SCn(\beta, \alpha)']  = \sum D_i^T V_i^T D_i
    \]    
    
    \item[(f)] If $y_{i1}, y_{i2}$ are continuous variables, will the results change?\\
    No, it won't affect. 
\end{itemize}

\exercise
\textbf{Two way ANOVA with interaction} Consider the two way ANOVA table with interaction, given by
   \[ 
   Y_{ijk}  = \mu + \alpha_i + \eta_j + \gamma_{ij} + \epsilon_{ijk},
    \] 
 where $i=1,2,...a, j=1,2.., b$ and $k=1,..N$. Further suppose that the $\epsilon_{ijk}$ are i.i.d and $\epsilon_{ijk} \sim N(0, \sigma^2)$, where $\sigma^2$ is unknown. Let $M_{\alpha}, M_{\eta}$ and $M_{\gamma}$ denote the orthogonal operations for the $\alpha, \eta$ spaces, and interaction space, respectively.  
\begin{itemize}
    \item [(a)] Show that $\sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}\gamma_{ij}$ is estimable, and find the UMVUE and the variance of the UMVUE, where the $c_{ij}$'s are real numbers that satisfy $\sum_{i=1}^{a} c_{ij}= \sum_{j=1}^{b} c_{ij}= 0$.\\
    Let 
   \[ 
   \beta = (\mu, \alpha_1, \alpha_2,.. \alpha_a, \eta_1, \eta_2,.. \eta_b, \gamma_{11}, \gamma_{12},... \gamma_{ab}),
    \]     
   \[ 
   X = (J_a \otimes J_b \otimes J_N, I_a \otimes J_b \otimes J_N, J_a \otimes I_b \otimes J_N, I_a \otimes I_b\otimes J_N),
    \]  
    The two way ANOVA model could be written as 
    \[ 
   Y = X\beta + \epsilon, \epsilon \sim N(0, \sigma^2)
    \]   
    The contrast 
       \[ 
    \lambda^T \beta = \sum_{i=1}^a \sum_{j=1}^b c_{ij}\gamma_{ij},
    \]
    where 
    \[ 
    \lambda^T = (0, ... 0_{a+b+1}, c_{11}, ...c_{ab}),
    \]
    We need to find $\rho$ such that $\lambda^T = \rho^T X$ to show estimability.\\
    Let 
     \[ 
    \rho^T = \frac{1}{N} (c_{11}J_N^T,c_{12}J_N^T, ... c_{ab}J_N^T,),
    \]  
    we have
    \[ 
    \rho^T X = \frac{1}{N} (c_{11}J,c_{12}J, ... c_{ab})\otimes J_N^T) (J_a \otimes J_b \otimes J_N, I_a \otimes J_b \otimes J_N, J_a \otimes I_b \otimes J_N, I_a \otimes I_b\otimes J_N),
    \]   
    \begin{equation}
    \begin{split}
    \rho^T X = \frac{1}{N} \{ [(c_{11}, ... c_{ab})(J_a \otimes J_b)]\otimes J_N^TJ_N, \\
    [(c_{11}, ... c_{ab})(I_a \otimes J_b)]\otimes J_N^TJ_N, \\
    [(c_{11}, ... c_{ab})(J_a \otimes I_b)]\otimes J_N^TJ_N, \\
    [(c_{11}, ... c_{ab})(I_a \otimes I_b)]\otimes J_N^TJ_N \}
    \end{split}
    \end{equation}
Since $\sum_{i=1}^{a} c_{ij} = \sum_{i=1}^{b} c_{ij} = 0$ Then\\
    \[ 
    [(c_{11}, ... c_{ab})(J_a \otimes J_b)]= 0, [(c_{11}, ... c_{ab})(I_a \otimes I_b)] = 0, (c_{11}, ... c_{ab})(I_a \otimes J_b) = 0
    \] 
    And
    \[ 
    [(c_{11}, ... c_{ab})(I_a \otimes I_b)] = (c_{11}, c_{12},... c_{ab})
    \] 
    Therefore, 
    \[ 
    \rho^T X = [0, 0_a, 0_b, (c_{11}, ... c_{ab})] = \lambda^T
    \]
    Thus, $\lambda^T \beta = \sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}\gamma_{ij}$ is estimable.\\
    
    The UMVUE of $\lambda^T \beta$ is $\rho^T MY$, where $M= I_a \otimes I_b \otimes P_N$, where 
    \[ 
    P_N = \frac{1}{N} J_{N}^{N}
    \]    
    Therefore, we have
    \[ 
    \rho^T MY = \frac{1}{N} [(c_{11}, ... c_{ab})]\otimes J_N^T] [(I_{ab} \otimes P_N)] Y
    \]
    \[ 
    \rho^T MY = \frac{1}{N} [(c_{11}, ... c_{ab})]\otimes J_N^T] Y = \sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}\bar{Y}_{ij.}
    \]
    The variance of UMVUE is
    \[ 
    Var(\rho^T MY) = Var(\sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}\bar{Y}_{ij.}) =\sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}^2Var(\bar{Y}_{ij.}) = \sum_{i=1}^{a} \sum_{j=1}^{b} c_{ij}^2 \frac{\sigma^2}{N}
    \] 
    
    \item[(b)] Using Kronecker product and notation, derive the orthogonal projection operator for the interaction space, denoted by $M_{\gamma}$.\\
    Let $s$ be an arbitrary index. Define $J_s$ as the $s \times 1$ vector of ones, $P_s = \frac{1}{N} J_s J_s^{'}$ and $Q_s = I_s -P_s$, where $I_s$ is the $s \times s$ identity matrix. Thus $P_s$ is the orthogonal projection operator onto $C(J_s)$ and $Q_s$ is the orthogonal projection operator onto $C(J_s)^{\perp}$.   
    Computing $M_\gamma$. The interaction space is given by $C(Q_a \otimes Q_b \otimes P_N)$. This yields
    \[ 
    M_\gamma = Q_a \otimes Q_b \otimes P_N
    \]    
    Compute $M_\mu$
    \[ 
    M_\mu = (J_a \otimes J_b \otimes J_N) [(J_a \otimes J_b \otimes J_N)^{T}(J_a \otimes J_b \otimes J_N)]^{-1} (J_a \otimes J_b \otimes J_N)^{T}
    \]  
    \[ 
     = (J_a \otimes J_b \otimes J_N) [(J_a ^{'}\otimes J_b^{'} \otimes J_N^{'})(J_a \otimes J_b \otimes J_N)]^{-1} (J_a^{'} \otimes J_b^{'} \otimes J_N^{'})
    \] 
    \[ 
     = (J_a \otimes J_b \otimes J_N) [(J_a ^{'}J_a\otimes J_b^{'}J_b \otimes J_N^{'}J_N)]^{-1}(J_a^{'} \otimes J_b^{'} \otimes J_N^{'})
    \]
    \[ 
     = (J_a \otimes J_b \otimes J_N) (abN)^{-1} (J_a^{'} \otimes J_b^{'} \otimes J_N^{'})
    \]
    \[ 
     = \frac{1}{a}J_aJ_a^{'} \otimes \frac{1}{b}J_bJ_b^{'} \otimes \frac{1}{N}J_NJ_N^{'} 
    \]
    \[ 
     = P_a \otimes P_b \otimes P_N
    \]
    
    Compute $M_\gamma$, the $\gamma$ space is $(Q_a \otimes Q_b \otimes J_N)$, thus
     \[ 
    M_\gamma = (Q_a \otimes Q_b \otimes J_N) [(Q_a \otimes Q_b \otimes J_N)^{T}(Q_a \otimes Q_b \otimes J_N)]^{-1} (Q_a \otimes Q_b \otimes J_N)^{T}
    \] 
    \[ 
     = (Q_a \otimes Q_b \otimes J_N) [(Q_a ^{'}Q_a\otimes Q_b^{'}Q_b \otimes J_N^{'}J_N)]^{-1}(Q_a^{'} \otimes Q_b^{'} \otimes J_N^{'})
    \]
    \[ 
     = (Q_a \otimes Q_b \otimes J_N) [(Q_a^{-}\otimes Q_b^{-1} \otimes N^{-1}](Q_a^{'} \otimes Q_b^{'} \otimes J_N^{'})
    \] 
    \[ 
     = (Q_a \otimes Q_b \otimes P_N) 
    \]
    Now $M = M_{\mu} + M_{\alpha} + M_{\eta} + M_{\gamma}$, we have
    \[ 
     M =(P_a \otimes P_b \otimes P_N) + (Q_a \otimes P_b \otimes P_N) + (P_a \otimes Q_b \otimes P_N)  + (Q_a \otimes Q_b \otimes P_N) 
    \]  
    \[ 
     M =(P_a + Q_a) \otimes P_b \otimes P_N + (P_a + Q_a) \otimes Q_b \otimes P_N  = I_a \otimes I_b \otimes P_N
    \]
    The error space is $I-M$
    \[ 
     I-M = I_a \otimes I_b \otimes I_N - I_a \otimes I_b \otimes P_N = I_a \otimes I_b \otimes Q_N
    \] 
    \item[(c)] Derive the simply possible scalar expression for $E[Y'(M_\alpha + M_\eta)Y]$.\\
    \[ 
     E[Y'(M_\alpha + M_\eta)Y] = tr((M_\alpha + M_\eta)\Sigma) + \mu'(M_\alpha + M_\eta)\mu
    \]
    where
    \[ 
     \mu = E[Y] = \mu \otimes J_a \otimes J_b \otimes J_N + \alpha \otimes J_b \otimes J_N + J_a \otimes \eta \otimes J_N + \gamma \otimes J_N
    \] 
    $\alpha = (\alpha_1, \alpha_2,... \alpha_a)^T, \eta = (\eta_1, \eta_2,... \eta_b)^T, \gamma= (\gamma_{11},.. \gamma_{ab})^T$. And $\Sigma = \sigma^2 I_{ab}$.\\
    Therefore, 
    \[ 
     E[Y'(M_\alpha + M_\eta)Y] = tr((M_\alpha + M_\eta)\Sigma) + \mu'(M_\alpha + M_\eta)\mu 
    \]
    \[ 
     = tr(M_\alpha \Sigma) + tr(M_\eta\Sigma) + \mu'M_\alpha \mu + \mu' M_\eta \mu 
    \]    
    \[ 
    \begin{split}
     \mu'M_\alpha \mu = (\mu J_a \otimes J_b \otimes J_N + \alpha \otimes J_b \otimes J_N + J_a 
     \otimes \eta \otimes J_N + \gamma \otimes J_N)^T (Q_a \otimes P_b \otimes P_N)\\
     (\mu J_a \otimes J_b \otimes J_N + \alpha \otimes J_b \otimes J_N 
     + J_a \otimes \eta \otimes J_N + \gamma \otimes J_N)   
    \end{split}
    \] 
    because $Q_a J_a = 0$
    \[ 
    \begin{split}
     \mu'M_\alpha \mu = (\alpha \otimes J_b \otimes J_N + \gamma \otimes J_N)^T (Q_a \otimes P_b \otimes P_N)     (\alpha \otimes J_b \otimes J_N + \gamma \otimes J_N)  \\ 
     = (\alpha \otimes J_b \otimes J_N)^T (Q_a \otimes P_b \otimes P_N) (\alpha \otimes J_b \otimes J_N) + 2(\gamma \otimes J_N)^T (Q_a \otimes P_b \otimes P_N) (\alpha \otimes J_b \otimes J_N) \\ +  (\gamma \otimes J_N)^T(Q_a \otimes P_b \otimes P_N)(\gamma \otimes J_N)\\
     = (\alpha^T Q_a \alpha) \otimes (J_b^T P_b J_b) \otimes (J_N^T P_N J_N) + 2(\gamma \otimes J_N)^T (\alpha Q_a) \otimes (P_bJ_b) \otimes (P_NJ_N) + (\gamma^T (Q_a \otimes P_b) \gamma) \otimes (J_N^T P_N J_N)\\
     = (\alpha^T Q_a \alpha) \otimes (J_b^T P_b J_b) \otimes (J_N^T P_N J_N) + 2[\gamma^T (\alpha Q_a \otimes P_bJ_b)] \otimes (J_N^T P_NJ_N) + (\gamma^T (Q_a \otimes P_b) \gamma) \otimes (J_N^T P_N J_N)\\ 
    \end{split}
    \]
    break down into each term
    \[ 
    \begin{split}
     \alpha'Q_a\alpha = \alpha^T [I- \frac{1}{a}J_a^a] \alpha  = [(I-\frac{1}{a}J_a^a)\alpha]^T[(I-\frac{1}{a}J_a^a)\alpha] \\
     = [(\alpha -\bar \alpha J_a]^T[(\alpha -\bar \alpha J_a] = \sum_{i=1}^{n} (\alpha_i - \bar \alpha)^2\\
     J_b^T P_b J_b = J_b^T \frac{1}{b} J_b^b J_b = b\\
     J_N^T P_N J_N = N\\
     (\alpha^T Q_a \alpha) \otimes (J_b^T P_b J_b) \otimes (J_N^T P_N J_N) =bN \sum_{i=1}^{n} (\alpha_i - \bar \alpha)^2
    \end{split}
    \] 
    
    \item[(d)] Derive the F-test for the hypothesis: $H_0: \sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}\gamma_{ij} =4$, and state its distribution under the null and alternative hypothesis.\\
    The orthogonal operator projection for $M\rho$ is\\
    \[ 
    M_{MP} = (M\rho) [(M\rho)^T (M\rho)]^{-} (\rho'M) = (M\rho) [\rho^T M\rho]^{-} (\rho'M)
    \]
     The F-test is given by:\\
    \[ 
    F = \frac{(\rho'MY - 4)' (\rho'M\rho)^{-} (\rho'MY -4)/ r(M_{MP})}{MSE}
    \] 
    Also because $M\rho = \rho$,
    \[ 
    \rho'MY = \rho'Y = \sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}\bar{Y}_{ij.}
    \] 
    \[ 
    \rho'M\rho = \rho'\rho =  \sum_{i=1}^{a}\sum_{j=1}^{b} \frac{c_{ij}^2}{N}
    \]
    \[ 
    MSE = \frac{1}{abN - ab} \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^N (Y_{ijk} - \bar{Y}_{ij.})^2
    \]
    Thus,
    \[ 
    F =\frac{\frac{N}{\sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}^2} (\sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}\bar{Y}_{ij.} - 4)^2} {\frac{1}{abN - ab} \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^N (Y_{ijk} - \bar{Y}_{ij.})^2} \sim F[1, ab(N-1), \gamma]
    \] 
    Under $H_0, \gamma = 0$, and under $H_1, \gamma = \frac{(\sum_{i=1}^{a}\sum_{j=1}^{b}c_{ij}^2)N}{2\sigma^2 \sum_{i=1}^{a}\sum_{j=1}^{b} c_{ij}^2 }$ 
    
    \item[(e)] Using only Kronecker product development and notation, obtain the simplest possible expression for $M_\alpha + M_\eta$.\\
Compute $M_\alpha$, the $\alpha$ space is $(Q_a \otimes J_b \otimes J_N)$, thus
     \[ 
    M_\alpha = (Q_a \otimes J_b \otimes J_N) [(Q_a \otimes J_b \otimes J_N)^{T}(Q_a \otimes J_b \otimes J_N)]^{-1} (Q_a \otimes J_b \otimes J_N)^{T}
    \] 
    \[ 
     = (Q_a \otimes J_b \otimes J_N) [(Q_a ^{'}Q_a\otimes J_b^{'}J_b \otimes J_N^{'}J_N)]^{-1}(Q_a^{'} \otimes J_b^{'} \otimes J_N^{'})
    \]
    \[ 
     = (Q_a \otimes J_b \otimes J_N) [(Q_a^{-}\otimes b^{-1} \otimes N^{-1}](Q_a^{'} \otimes J_b^{'} \otimes J_N^{'})
    \] 
    \[ 
     = (Q_a \otimes P_b \otimes P_N) 
    \]  
    Here $Q_a$ is symmetric, semi-definite.\\
    Compute $M_\eta$, the $\eta$ space is $(J_a \otimes Q_b \otimes J_N)$, thus
     \[ 
    M_\eta = (J_a \otimes Q_b \otimes J_N) [(J_a \otimes Q_b \otimes J_N)^{T}(J_a \otimes Q_b \otimes J_N)]^{-1} (J_a \otimes Q_b \otimes J_N)^{T}
    \] 
    \[ 
     = (J_a \otimes Q_b \otimes J_N) [(J_a ^{'}J_a\otimes Q_b^{'}Q_b \otimes J_N^{'}J_N)]^{-1}(J_a^{'} \otimes Q_b^{'} \otimes J_N^{'})
    \]
    \[ 
     = (J_a \otimes Q_b \otimes J_N) [(a^{-}\otimes Q_b^{-1} \otimes N^{-1}](J_a^{'} \otimes Q_b^{'} \otimes J_N^{'})
    \] 
    \[ 
     = (P_a \otimes Q_b \otimes P_N) 
    \] 
    \[ 
     M_\alpha + M_\eta =(Q_a \otimes P_b \otimes P_N) + (P_a \otimes Q_b \otimes P_N) 
    \]     
\end{itemize}

Note that \texttt{*} can be used instead of \verb|\cdot|, and \verb|\R| instead of \verb|\mathbb{R}|. (For a normal asterisk, use \verb|\ast|.) Of course, there are also macros for the natural numbers etc. Commands such as \verb|\abs{}| and \verb|\set{}| can be used to create (scaled) delimiters. For example,
\[
    \abs{\frac{1}{1 - \lambda h}} \le 1
    \qquad\text{and}\qquad
    \bigcup_{i=1}^n \; \set{z \in \C \mid \abs*{z - a_{ii}} \le {\sum\nolimits_{j \ne i}} \abs*{a_{ij}}}.
\]
The starred version of these commands disables the auto-scaling.

\exercise
\textbf{Segmented linear regression} Assume one function in a certain range of X and another in a different range. For a general segmented linear regression:
    \[ 
    \begin{split}
     Y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \qquad  x_i \leq t \\
     Y_i = \alpha_0 + \alpha_i x_i + \epsilon_i, \qquad x_i > t
    \end{split}
    \] 
with independent, identically distributed normal errors, $\epsilon \sim N(0, \sigma^2), i= 1,..n$.\\
The model is continuous if $\beta_0 + \beta_1t = \alpha_0 + \alpha_1t$, and discontinuous otherwise. Assume that t is known, Let $I(z)$ be the indicator function, and define $(z)_{+} = max(0,z)$. Consider fitting the regression model:

    \begin{equation}
     Y_i = \delta_0 + \delta_1 x_i + \delta_2I(x_i-t) + \delta_3 (x_i-t)_{+} \epsilon_i, 
    \end{equation}

with independent, identically distributed normal errors, $\epsilon \sim N(0, \sigma^2), i= 1,..n$.\\
\begin{itemize}
    \item [(a)] Give the relations between $\beta_0, \beta_1, \alpha_0, \alpha_1$, and $\delta_0,... \delta_3$.\\
    Compare the model at $X_i>t, X_i \leq t$, we can find the relations. 
    \item [(b)] Write the model (3.1) for the form $Y=X\beta + \epsilon$, clearly identify all the components and \textbf{derive} the UMVUE of $(\beta_1, \alpha_1)'$ using appropriate projections.\\
    \[ 
    \begin{split}
     Y = (Y_1, Y_2,.. Y_n) \\
     \beta = (\delta_0, \delta_1, \delta_2, \delta_3)\\
     X=  \begin{bmatrix}
           1 & x_1 & I(x_1 - t) & (x_1 -t)_{+} \\
           1 & x_2 & I(x_2 - t) & (x_2 -t)_{+}\\
           . & . & . & .\\
           1 & x_n & I(x_n - t) & (x_n -t)_{+}\\
         \end{bmatrix} 
    \end{split}
    \]     
    Derive UMVUE, we need to write the likelihood function:\\
    \[ 
    \begin{split}
    L(\beta, \sigma) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} exp\{\frac{-(y_i-x_i\beta)^2}{2\sigma^2}\} \\
    \textbf{or we can write in matrix form}\\
    L(\beta, \sigma) = \frac{1}{\sqrt{2\pi}\sigma}^n exp\{\frac{-(Y-X\beta)^T(Y-X\beta)}{2\sigma^2}\} \\
    ln L(\beta, \sigma) = n -ln(\sqrt{2\pi}\sigma) + \frac{-(Y-X\beta)^T(Y-X\beta)}{2\sigma^2} \\
    \frac{\partial lnL}{\partial \beta} = \frac{X^T(Y-X\beta)}{\sigma^2} = 0\\
    X^TY = X^TX\beta\\
    \frac{\partial lnL}{\partial \sigma} = \frac{-n}{\sigma} + \frac{(Y-X\beta)^T(Y-X\beta)}{\sigma^3} = 0\\
    \sigma^2 = \frac{(Y-X\beta)^T(Y-X\beta)}{n}
    \end{split}
    \]     
    Assume X is full rank, then we have 
    \[ 
    \begin{split}
    \hat{\beta} = (X^TX)^{-1}X^TY = (\hat{\delta_0}, \hat{\delta_1},\hat{\delta_2}, \hat{\delta_3})\\
    \end{split}
    \]   
    Since $X^TY$ is sufficient and complete statistics for $\beta$ based on exponential family form of the likelihood, so $\hat{\beta}$ is the UMVUE of $\beta$. And hence, the UMVUE of $\alpha_1, \beta_1$ is 
    \[ 
    \begin{split}
    \hat{\beta_1} = \hat{\delta_1},\\
    \hat{\alpha_1}=\hat{\delta_1}+ \hat{\delta_3}\\
    \end{split}
    \] 
    \item[(c)] Use model (3.1) and appropriate projections to derive the F test for the null hypothesis:\\
    \begin{itemize}
        \item [(i)] $H_0:$ continuity of the segmented regression: $\beta_0 + \beta_1 t = \alpha_0 + \alpha_1 t$. 
    \[ 
    \begin{split}
    H_0: \beta_0 + \beta_1 t = \alpha_0 + \alpha_1 t,\\
    \beta_0 - \alpha_0 = (\alpha_1 - \beta_1)t\\
    \delta_0 - (\delta_0 + \delta_2 - \delta_3t) = (\delta_1 + \delta_3- \delta_1)t\\
    \delta_2 = 0
    \end{split}
    \]    
    So the $H_0: \delta_2 = 0, \lambda^T \beta = 0, \lambda^T = (0,0,1,0)$\\
    The F-test takes the form:
    \[ 
    \begin{split}
    F-test = \frac{[\lambda^T\hat{\beta}-0]' (\lambda^T (X^TX)^{-} \lambda)^{-1} [\lambda^T\hat{\beta}-0]/r(M_{\lambda}) }{MSE},\\
    M_{\lambda} = MP [(MP)'(MP)]^{-} P'M= MP [P'MP]^{-} P'M= MP[P' X(X'X)^{-}X' P]^{-1} P'M\\
   = MP (\lambda^T (X^TX)^{-} \lambda)^{-1} P'M\\
    = \frac{[\lambda^T (X^TX)^{-1}X^TY]^T (\lambda^T (X^TX)^{-} \lambda)^{-1} [\lambda^T (X^TX)^{-1}X^TY]}{\sigma^2} \\
    = \frac{[\lambda^T (X^TX)^{-1}X^TY]^T [\lambda^T (X^TX)^{-1}X^TY]}{\sigma^2 (\lambda^T (X^TX)^{-} \lambda)} 
    = \frac{Y^T [X (X^TX)^{-1} \lambda \lambda^T (X^TX)^{-1}X^T]Y}{\sigma^2 (\lambda^T (X^TX)^{-} \lambda)} \\
    \lambda \lambda^T = \begin{bmatrix}
           0 \\
           0\\
           1\\
           0\\
         \end{bmatrix} (0,0,1,0) = \begin{bmatrix}
           0 & 0& 0 & 0 \\
           0 & 0& 0 & 0\\
           0 & 0 & 1 & 0\\
           0 & 0& 0 & 0 \\
         \end{bmatrix} 
    \end{split}
    \]
    Note that $M_{\Lambda} = [X (X^TX)^{-1} \lambda \lambda^T (X^TX)^{-1}X^T]$ is orthogonal projection with rank 1, since $\lambda \lambda^T$ is rank 1.
    \[ 
    \begin{split}
    F-test = Y^T M_{\Lambda} Y,\\
    \sigma^2 = \frac{Y^T (I-M) Y}{n-4}\\
    \end{split}
    \] 
    $M= X(X'X)^{-1}X'$ is opo onto $C(X)$, we claim\\
     \[ 
    \begin{split}
    F-test =\frac{Y^T M_{\Lambda} Y/1}{Y^T (I-M) Y/(n-4)} \sim F(1, n-4, \gamma) ,\\
    \gamma = \frac{\mu' M_{\Lambda} \mu}{2\sigma^2} \\
    \end{split}
    \]    
    To show independence in F-test, we need to prove $M_{\Lambda} (I-M) = 0$.\\
    \item[(ii)] $H_0:$ identity of the two segments: $\beta_0 = \alpha_0, \beta_1 = \alpha_1$. \\
    The null hypothesis could be written as:
    \[ 
    \begin{split}
    H_0: \beta_0 = \alpha_0, \qquad \delta_0 = \delta_0 + \delta_2 - \delta_3t\\
    \beta_1 = \alpha_1, \qquad \delta_1 = \delta_1+\delta_3\\
    H_0: \delta_2 - \delta_3t= 0,\qquad \delta_3 = 0\\
    \Lambda^T = \begin{bmatrix}
           0 & 0& 1 & -t \\
           0 & 0& 0 & 1\\
         \end{bmatrix}
    \end{split}
    \]    
    The orthogonal projection operator for $H_0$ we can write $\Lambda^T = P^T X$. Let 
    \[ 
    \begin{split}
    M_{MP} = MP[(MP)'(MP)]^{-1} (MP)' = MP[P'MP]^{-1}P'M \\
    F-test = (\Lambda \hat{\beta})^TCov(\Lambda \hat{\beta})^{-1}(\Lambda \hat{\beta})\\
     = \frac{Y'M_{MP}Y/r(M_{MP})}{MSE},\\
    \end{split}
    \] 
    \end{itemize}
    
    \item[(d)] Derive a joint 95$\%$ confidence region for $(\beta_1-\alpha_1, \beta_0+\beta_1, \sigma^2)$\\
     \[ 
    \begin{split}
    \beta_1-\alpha_1 = \delta_1 - (\delta_1+\delta_3) = -\delta_3\\
    \beta_0 + \beta_1 = \delta_0 + \delta_1\\
    \end{split}
    \]    
    Thus we want a joint confidence region for 
     \[ 
    \begin{split}
    (-\delta_3, \delta_0+\delta_1, \sigma^2)' = (\Lambda^T\beta, \sigma^2)^T\\
    \Lambda^T = \begin{bmatrix}
           0 & 0& 0 & -1 \\
           1 & 1 & 0 & 0\\
         \end{bmatrix}\\
    \hat{\beta} = (X'X)^{-1}X'Y
    \end{split}
    \]     
    
    $\Lambda^T \hat{\beta} $ is the UMVUE of  $\Lambda^T \beta $ by Gauss-Markov theorem. \\
    Since $\Lambda^T \beta$ is estimable, we can find a matrix P so that
     \[ 
    \begin{split}
    \Lambda^T = P' X\\
    \Lambda^T \beta = P' X \beta = P' MY , \qquad M= X(X'X)^{-1}X'\\
    M_{MP} = (MP) [(MP)^T(MP)]^{-1} (MP)^T\\
    \end{split}
    \]  
    $M_{MP}$ is the appropriate opo for testing $H_0: \Lambda^T \beta = 0$.\\
     \[ 
    \begin{split}
    M_{MP} (Y- X\beta) \sim N(0, \sigma^2 M_{MP})\\
    \frac{\lVert M_{MP}(Y- X\beta) \rVert }{\sigma^2} \sim \chi^2 (r(M_{MP}))\\
    r(M_{MP}) = 2\\
    \end{split}
    \]     
    $M_{MP}Y$ is the UMVUE of $M_{MP}X \hat{\beta}$ \\
    The UMVUE of $\sigma^2$ is
     \[ 
    \begin{split}
    \frac{\lVert (I-M)Y \rVert }{\sigma^2} \sim \chi^2 (n-4)\\
    r(M_{MP}) = 2\\
    \end{split}
    \]
    Thus $M_{MP} \perp (I-M)$\\
    A joint 95$\%$ confidence region for $(\Lambda^T\beta, \sigma^2)^T$\\
     \[ 
    \begin{split}
    P\{ \chi^2_{a}(2) < \frac{\lVert M_{MP}(Y- X\beta) \rVert }{\sigma^2} < \chi^2_{1-a}(2)\}, \qquad
    P\{ \chi^2_{b}(n-4) < \frac{\lVert (I-M)Y \rVert }{\sigma^2} < \chi^2_{1-b}(n-4)\}\\
    = P\{ \chi^2_{a}(2) < \frac{\lVert M_{MP}(Y- X\beta) \rVert }{\sigma^2} < \chi^2_{1-a}(2)\}
    P\{ \chi^2_{b}(n-4) < \frac{\lVert (I-M)Y \rVert }{\sigma^2} < \chi^2_{1-b}(n-4)\} \\
    = 1- \alpha
    \end{split}
    \]    
    where a and b are chosen so that $(1-2a)(1-2b) = 1-\alpha$
    \item[(e)] Under model (3.1), derive a  95$\%$ prediction region for a future $q \times 1$ future response vector taken at the $q \times 4$ future covariate matrix $X_f$.\\
    The prediction could be considered as bayesian rule, so the $\sigma_{new}^2 = \sigma^2 (1+ X_f(X'X)^{-1}X_f^T)$.\\
     \[ 
    \begin{split}
    Cov(X_f \beta) = Cov(X_f (X'X)^{-1}X'Y) = [X_f (X'X)^{-1}X'] Cov(Y) [X_f X(X'X)^{-1}]^T\\
    = [X_f (X'X)^{-1}X'] Cov(Y) [X(X'X)^{-1} X_f^{T}] = \sigma^2[X_f (X'X)^{-1} X_f^T]\\
    \sigma_{new}^2 = \sigma^2 (1+ X_f(X'X)^{-1}X_f^T)\\
    \sigma^2 = \frac{\lVert (I-M)Y \rVert}{(n-4)}
    \end{split}
    \]     
    Thus a 95$\%$ prediction region for a future  $q \times 1$ vector $Y_f$\\
     \[ 
    \begin{split}
    \{ Y_f: \frac{((\hat{Y_f}-Y_f)^T (X_f (X'X)^{-1} X_f^T)^{-1} (\hat{Y_f}-Y_f))}{MSE} \leq F(.95, q, n-4) \}
    \end{split}
    \]     
\end{itemize}



\exercise
\textbf{Split plot design with CBD} Consider the factorial problem: 3 different irrigation levels, 4 different corn varieties, response: biomass, available resources: 6 plots of land. By definition, we can not vary the irrigation level on a too small scale. We are "forced" to use "large" experimental units for the irrigation level factor. Assume that we can use a specific irrigation level on each of the 6 plots.\\
Randomly assign each irrigation level to 2 of the plots (the so called whole plots or main plots). In every of the plots, randomly assign the 4 different corn varieties to the so called split plots. Two independent randomization are being performed. We call irrigation level the whole-plot factor and corn variety the split-plot factor.\\
Whole plots are the experimental units for the whole-plot factor(irrigation level), Split plots are the experimental units for the split-plot factor. In the split-plot world, whole plots act as blocks. \\
We use a mixed model formulation with two different erors\\
\begin{align*}
Y_{ijk} &= \mu + \alpha_i + \eta_{k(i)} + \beta_j + (\alpha\beta)_{ij} + \epsilon_{k(ij)}\\
\eta_{k(i)} &= N(0, \sigma_{\eta}^2) \qquad \text{whole-plot error} \\
\epsilon_{k(ij)} &= N(0, \sigma^2) \qquad \text{split-plot error} \\
\alpha_i &= \text{fixed effect of irrigation}\\
\beta_j &= \text{fixed effect of corn variety}\\
(\alpha\beta)_{ij} &= \text{fixed interaction between irrigation and corn variety}
\end{align*} 
This means the observations in the same whole plot share the same whole-plot error $\eta_{k(i)}$.\\

\textbf{Split plot design with RCBD} In this example, we have assumed that managing levels of irrigation and fertilizer require the same effort. Now suppose varying the level of irrigation is difficult on a small scale and it makes more sense to apply irrigation levels to larger areas of land. \\

In such situations, we can divide each land into two large fields (whole plots) and apply irrigation amounts to each field randomly. And then divide each of these large fields into smaller fields (subplots) and apply fertilizer randomly within the whole plots. \\

In this strategy, each land contains two whole plots and irrigation amount is assigned to each whole plot randomly using RCBD (i.e. lands are treated as blocks and irrigation amount is assigned randomly within each block to the whole plots). Each whole plot contains two subplots and fertilizer type is assigned to each subplot using RCBD (i.e. whole plots are treated as blocks and fertilizer type is assigned randomly within each whole plot to the subplots).\\

When some factors are more difficult to vary than others at the levels of experimental units, it is more efficient to assign more difficult-to-change factors to larger units (whole plots) and then apply the easier-to-change factor to smaller units (subplots). This is known as the split-plot design. \\

It is important to notice that in a split-plot design, randomization is a two-stage process. Levels of one factor (say, factor A) are randomized over the whole plots within each block, and the levels of the other factor (say, factor B) are randomized over the subplots within each whole plot. This restriction in randomization results in two different error terms: one appropriate for comparisons at the whole plot level and one appropriate for comparisons at the subplot level. \\

The appropriate error for whole plot level in split-plot RCBD is whole plot factor $\times$ block interaction. In other words, the analysis at the whole plot level is essential of a one-way ANOVA with blocking (i.e. one observation per block-treatment combination). From the perspective of the whole plot, the subplots are simply subsamples and it is reasonable to average them when testing the whole plot effects (i.e. factor A effects).\\
The subplot factor (i.e. factor B) is always compared within the whole plot factor. \\
\begin{tabular}{l r r}
Source & & DF  \\\hline
Blocks &  &r-1\\
Factor 	A & & a -1\\
 & whole plot effect error  & (r-1)(a-1)\\
Factor B & & b -1\\
Factor A $\times$ B &  & (a-1)(b -1)\\
 & subplot effect error  & a(r-1)(b-1)\\
Total &   & abr -1\\
\hline
\end{tabular}\\
The statistical model associated with the split-plot design with whole plots arranged as RCBD is 
\begin{align*}
Y_{ijk} = \mu + \alpha_i + \gamma_k + (\alpha\gamma)_{ik} + \beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}
\end{align*}
which $\gamma_k$ for $k=1,2...r$ are block effects, $\alpha_i$ for $i=1,2,.. a$ are whole plot effects, and $\beta_j$ for $j=1,2..b$ are subplot effects. \\

\textbf{Question} Consider the split-plot design ANOVA model given by
\begin{align*}
Y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_k  + (\epsilon)_{ij}^{(1)}  + (\alpha\gamma)_{ik}+ \epsilon_{ijk}^{(2)}
\end{align*}
where $i = 1.. a; j = 1.. b;$ and $k = 1... c$. Here, $\alpha_i$ = ith whole plot treatment effect, $\beta_j$ = jth block effect, $\gamma_k$ = kth subplot treatment effect, and $(\alpha\gamma)_{ik}$ = whole plot treatment and subplot treatment interaction. Further we assume that the $(\epsilon)_{ij}^{(1)}$ are iid $N(0; \sigma_1^2)$ random variables, the $\epsilon_{ijk}^{(2)}$ are iid $N(0, \sigma_2^2)$ random variables, and the $(\epsilon)_{ij}^{(1)}$ are independent of the $\epsilon_{ijk}^{(2)}$ for all i; j; k.\\

\begin{itemize}
    \item [(a)] Write components in $Y= X\beta + \epsilon$\\
 \begin{align*}
Y &= (Y_{111}, Y_{112}, ... Y_{11c}, Y_{121},... Y_{12c}, ..Y_{1bc}, Y_{211}, ... Y_{abc})^T_{abc\times 1}\\
\text{Let} J_a &= (1,1,...1)^T_{a\times 1}, I_a= diag(1)_{a\times a}\\
X &= [J_a \otimes J_b \otimes J_c \qquad I_a \otimes J_b \otimes J_c \qquad J_a \otimes I_b \otimes J_c \qquad J_a \otimes J_b \otimes I_c \qquad I_a \otimes I_b \otimes I_c]\\
\beta &= [\mu \qquad \alpha_1 \qquad ..\alpha_a \qquad \beta_1.. \beta_b \qquad \gamma_1.. \gamma_c \qquad (\alpha \gamma)_{11}... \qquad (\alpha \gamma)_{ac}]^T\\
\epsilon &= [\epsilon_{11} + \epsilon_{111} \qquad \epsilon_{11}+\epsilon_{112} ... \qquad \epsilon_{11}+ \epsilon_{11c} \qquad \epsilon_{12}+\epsilon_{121} ... \qquad \epsilon_{12}+\epsilon_{12c} \qquad ... \epsilon_{ab}+\epsilon_{abc}]^T_{abc \times 1}\\
\end{align*}   
Understand the covariance structure
 \begin{align*}
Var(\epsilon_{ij} + \epsilon_{ijk}) &= Var(\epsilon_{ij}) + Var(\epsilon_{ijk}) = \sigma_1^2 + \sigma_2^2\\
Cov(\epsilon_{ij} + \epsilon_{ijk_1}, \epsilon_{ij} + \epsilon_{ijk_2 }) &= Var(\epsilon_{ij}) + Cov(\epsilon_{ijk_1}, \epsilon_{ijk_2}) = \sigma_1^2\\
\Sigma &= \begin{bmatrix}
           \sigma_1^2 + \sigma_2^2 &  \sigma_1^2 &\sigma_1^2... & 0 & 0 \\
            &  \sigma_1^2 + \sigma_2^2 & \sigma_1^2 ... &0 & 0\\
             &   & \sigma_1^2 + \sigma_2^2 & ... &0 \\
             &.. &.. &.. &..\\
         \end{bmatrix} = \sigma_1^2 I_a \otimes I_b \otimes J_c + \sigma_2^2 I_a \otimes I_b \otimes I_c\\
\end{align*} 
\item [(b)] Show that $\sum_{i=1}^{a} \sum_{k=1}^{c} q_{ik}(\alpha\gamma)_{ik}$ is estimable, and find its UMVUE and the variance of the UMVUE, where the $q_{ik}$'s are real numbers that satisfy\\
\begin{align*}
\sum_{i=1}^{a} q_{ik} = \sum_{k=1}^{c} q_{ik} =0
\end{align*} 
Need to find $\rho^T X = \lambda^T$, summing over $\beta$
\begin{align*}
\lambda^T &= (0_{a+b+c+1} \qquad q_{11} \qquad q_{12} \qquad .... q_{ac})\\
\lambda^T \beta &= \sum_{i=1}^{a} \sum_{k=1}^{c} q_{ik}(\alpha\gamma)_{ik}\\
\rho^T &= \frac{1}{b} (q_{11} \otimes J_b^T \qquad q_{12}  \otimes J_b^T \qquad ... q_{ac} \otimes J_b^T)\\
\end{align*} 
Since $\sum_{i=1}^{a} q_{ik} = \sum_{k=1}^{c} q_{ik} =0, \rho^T X = \lambda^T $, then $\lambda^T $ is estimable. 
The UMVUE of the $\lambda^T \beta$
\begin{align*}
\rho^T MY &= \rho^T Y = \sum_{i=1}^{a} \sum_{k=1}^{c} q_{ik} \bar{Y}_{i.k} \qquad \text{Since} \rho \in C(X)\\
Var(\rho^T MY) &= \rho^T M Var(Y)  M\rho = \rho^T Var(Y) \rho = \rho^T \Sigma \rho\\
\end{align*} 
\item[(c)] Using only Kronecker product development and notation, i) derive the orthogonal projection operator for the $\alpha$ space, denoted by $M_{\alpha}$ and, ii) derive the orthogonal
projection operator for the $(\alpha\gamma)$ interaction space, denoted by $M_{\alpha\gamma}$. Express your answer in the simplest possible form.\\
Let
\begin{align*}
P_a &= \frac{1}{a} J_a J_a^T, \qquad P_a \quad \text{o.p.o onto}\quad C(J_a)\\
Q_a &= I_a - P_a, \qquad Q_a \quad \text{o.p.o onto}\quad C(J_a)^{\perp}\\
M_{\alpha} &= (Q_a \otimes J_b \otimes J_c)\left ((Q_a \otimes J_b \otimes J_c)^T (Q_a \otimes J_b \otimes J_c) \right)^{-} (Q_a \otimes J_b \otimes J_c)^T\\
&= Q_a \otimes P_b \otimes P_c\\
M_{\alpha\gamma} &= (Q_a \otimes J_b \otimes Q_c) \left ((Q_a \otimes J_b \otimes Q_c)^T (Q_a \otimes J_b \otimes Q_c) \right)^{-} (Q_a \otimes J_b \otimes Q_c)^T\\
&= Q_a \otimes P_b \otimes Q_c\\
\end{align*} 
\item[(d)]Derive the simplest possible scalar expression for $E[Y'(M_{\alpha} + M_{\gamma})Y]$, where $M_{\gamma}$ denotes the orthogonal projection operator for the space.\\
\begin{align*}
E[Y'(M_{\alpha} + M_{\gamma})Y] &= tr \left((M_{\alpha} + M_{\gamma})\Sigma \right) + E(Y)^T(M_{\alpha} + M_{\gamma}) E(Y)\\
M_{\gamma} &= P_a \otimes P_b \otimes Q_c\\
E(Y) &= X\beta = \mu J_a \otimes J_b \otimes J_c + \alpha \otimes J_b \otimes J_c + J_a \otimes \beta \otimes J_c + J_a \otimes J_b \otimes \gamma + (\alpha\gamma) J_c\\
\end{align*} 
where $\alpha = (\alpha_1, \alpha_2,... \alpha_a)^T, \beta = (\beta_1, \beta_2,.. \beta_b)^T, \gamma = (\gamma_1, \gamma_2, .. \gamma_c)^T, (\alpha\gamma) = (\alpha\gamma_{11}, \alpha\gamma_{12}, ... \alpha\gamma_{ac})^T$.\\
\begin{align*}
M_{\gamma} &= P_a \otimes P_b \otimes Q_c\\
E(Y) &= X\beta = \mu J_a \otimes J_b \otimes J_c + \alpha \otimes J_b \otimes J_c + J_a \otimes \beta \otimes J_c + J_a \otimes J_b \otimes \gamma + (\alpha\gamma) J_c\\
M_{\alpha}\Sigma &= M_{\alpha} (\sigma_1^2 I_a \otimes I_b \otimes J_c + \sigma_2^2 I_a \otimes I_b \otimes I_c) \\
&= \sigma_1^2 (Q_a \otimes P_b \otimes P_c) (I_a \otimes I_b \otimes J_c) + \sigma_2^2 (Q_a \otimes P_b \otimes P_c) (I_a \otimes I_b \otimes I_c)\\
&= \sigma_1^2 (Q_a \otimes P_b \otimes J_c) + \sigma_2^2 (Q_a \otimes P_b \otimes P_c)\\
tr(M_{\alpha}\Sigma) &= \sigma_1^2 \{ tr(Q_a) tr(P_b) tr(J_c) \} + \sigma_2^2 \{ tr(Q_a) tr(P_b) tr(P_c)\}\\
&= \sigma_1^2 [(a-1) \times \frac{1}{b} b \times 1] + \sigma_2^2 [(a-1) \times \frac{1}{b} b \times \frac{1}{c} c]\\
&= (a-1)(\sigma_1^2 + \sigma_2^2)\\
M_{\gamma}\Sigma &= (P_a \otimes P_b \otimes Q_c) (\sigma_1^2 I_a \otimes I_b \otimes J_c + \sigma_2^2 I_a \otimes I_b \otimes I_c)\\
&= \sigma_1^2(P_a \otimes P_b \otimes 0) + \sigma^_2^2 (P_a \otimes P_b \otimes Q_c)\\
tr(M_{\gamma} \Sigma) &= \sigma_2^2 (tr(P_a) tr(P_b) tr(Q_c)) = (c-1) \sigma_2^2\\
E(Y)'(M_{\alpha}+ M_{\gamma})E(Y) &= E(Y)'(Q_a \otimes P_b \otimes P_c + P_a \otimes P_b \otimes Q_c) E(Y) \\
&= \sum\sum\sum [(\alpha_i + \bar{(\alpha\gamma)_{i.}} - \bar{\alpha_{.}} - \bar{(\alpha\gamma)_{..}})^2 + (\gamma_k + \bar{(\alpha\gamma)}_{.k} - \bar{\gamma_{.}} - \bar{(\alpha\gamma)_{..}})^2]\\
\end{align*} 
Overall
\begin{align*}
E[Y'(M_{\alpha} + M_{\gamma})Y] &= (a-1)(\sigma_1^2 + \sigma_2^2) + (c-1) \sigma_2^2\\ 
&+ bc \sum_{i=1}^a (\alpha_i + \bar{(\alpha\gamma)}_{i.} - \bar{\alpha}_{.} - \bar{(\alpha\gamma)}_{..})^2 + ac \sum_{i=1}^b (\gamma_k + \bar{(\alpha\gamma)}_{.k} - \bar{\gamma}_{.} - \bar{(\alpha\gamma)}_{..})^2
\end{align*}
\item[(e)]Using only Kronecker product development, derive the F-test for the hypothesis\\
\begin{align*}
H_0: \sum_{i=1}^a \sum_{k=1}^c q_{ik} (\alpha\gamma)_{ik} = 10
\end{align*}
And state its distribution under the null and alternative hypotheses.\\
\begin{align*}
H_0: & \lambda^T \beta = 10\\
 \lambda^T \hat{\beta} &= \rho^T X \hat{\beta} = \rho^T M Y , \qquad r(\rho) = 1\\
 F &= (\rho^T M Y - 10)^T [Var(\rho^T MY)]^{-1} (\rho^T M Y - 10)\\
 &= \frac{(\rho^T M Y - 10)^2}{\rho^T \Sigma \rho}\\
 r(I-M) &= abc - (a-1 + b-1 + c-1 + (a-1)(b-1) + 1) = abc -ac -b + 1\\
 F \sim F(1, abc -ac -b + 1, \lambda)
\end{align*}
Where under $H_0, \lambda = 0$, under $H_1 $
\begin{align*}
 \lambda &= \frac{E(Y)'M_{M\rho}E(Y)}{2|\Sigma|/abc} = \frac{E(Y)'\rho (\rho'\rho)^{-} \rho' E(Y)}{2|\Sigma|/abc}\\
 M_{M\rho} &= M\rho \left( (M\rho)' (M\rho) \right)^{-} \rho'M = \rho (\rho'\rho)^{-} \rho'
\end{align*}

\item[(f)] Using only Kronecker product development and notation, obtain the simplest possible
expression for $M_{\beta} +M_{\alpha\gamma}$, where $M_{\beta}$ denotes the orthogonal projection operator for the $\beta$ space in model (4).\\
\begin{align*}
M_{\beta} &= P_a \otimes Q_b \otimes P_c\\
M_{\beta} + M_{\alpha\gamma} &= P_a \otimes Q_b \otimes P_c +Q_a \otimes P_b \otimes Q_c\\
&= P_a \otimes I_b \otimes P_c - P_a \otimes P_b \otimes I_c + I_a \otimes P_b \otimes Q_c
\end{align*}

\item[(g)] Write the full ANOVA table in detail for model (4) including information for all
sources of variation, degrees of freedom, sums of squares, mean squares, and expected
mean squares. Write all the sums of squares and expected mean squares in both
Kronecker product form and scalar form.\\
\begin{tabular}{l r r r r r r }
& Kronecker & & & \\\hline
Source & SS & EMS  \\\hline
$M_\mu=P_a \otimes P_b \otimes P_c$ &  Y'($M_\mu$)Y & E(Y)'($M_\mu$)E(Y)  \\
$M_\alpha=Q_a \otimes P_b \otimes P_c$	&  Y'($M_\alpha$)Y & E(Y)'($M_\alpha$)E(Y) \\
$M_\beta= P_a \otimes Q_b \otimes P_c$	& Y'($M_\beta$)Y & E(Y)'($M_\beta$)E(Y) \\
$M_\gamma=P_a \otimes P_b \otimes Q_c$	& Y'($M_\gamma$)Y & E(Y)'($M_\gamma$)E(Y) \\
$M_\alpha\gamma =Q_a \otimes P_b \otimes Q_c$	& Y'($M_{\alpha\gamma}$)Y & E(Y)'($M_{\alpha\gamma}$)E(Y) \\
$M = M_{\mu}+M_{\alpha} + M_{\beta} + M_{\gamma} + M_{\alpha\gamma} $	& Y'(M)Y & E(Y)'(M)E(Y) \\
$= P_a \otimes Q_b \otimes P_c + I_a \otimes P_b \otimes I_c$  \\
$M_e = I-M$	& Y'($M_e$)Y & E(Y)'($M_e$)E(Y) \\
$= I_{abc}- P_a \otimes Q_b \otimes P_c - I_a \otimes P_b \otimes I_c$  \\
Total	& Y'(I)Y & E(Y)'(I)E(Y) \\
\hline
\end{tabular}\\

\begin{tabular}{l r r r r r r }
& Scalar & & & \\\hline
Source & SS & EMS  \\\hline
$M_\mu$ &  $abc\bar{Y}^2_{...}$ & $abc\mu^2$  \\
$M_\alpha$	&  $bc\sum_{i=1}^{a}(\bar{Y}_{i..}- \bar{Y}_{...})^2$ & $bc\sum_{i=1}^{a}(\alpha_i + \bar{(\alpha\gamma)}_{i.}- \bar{\alpha}_{.} - \bar{(\alpha\gamma)}_{..})^2$ \\
$M_\beta$	&  $ac\sum_{j=1}^{b}(\bar{Y}_{.j.}- \bar{Y}_{...})^2$ & $ac\sum_{j=1}^{b}(\beta_i - \bar{\beta}_{.})^2$ \\
$M_\gamma$	& $ab\sum_{k=1}^{c}(\bar{Y}_{..k}- \bar{Y}_{...})^2$ & $ab\sum_{k=1}^{c}(\gamma_k + \bar{(\alpha\gamma)}_{.k}- \bar{\gamma}_{.} - \bar{(\alpha\gamma)}_{..})^2$ \\
$M_\alpha\gamma$	& $b\sum_{i=1}^{a}\sum_{k=1}^{c}(\bar{Y}_{i.k}- \bar{Y}_{i..}- \bar{Y}_{..k} + \bar{Y}_{...})^2$ & $b\sum_{i=1}^{a}\sum_{k=1}^{c}(\alpha\gamma_{ik} - \bar{(\alpha\gamma)}_{i.} - \bar{(\alpha\gamma)}_{.k} - \bar{(\alpha\gamma)}_{..})^2$ \\
$M_e = I-M$	& $\sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{c}(Y_{ijk}-\bar{Y}_{i.k} -\bar{Y}_{.j.}  +\bar{Y}_{...})^2$ & E(Y)'($M_e$)E(Y) \\
Total	& Y'(I)Y & E(Y)'(I)E(Y) \\
\hline
\end{tabular}\\

\begin{align*}
    M &= M_{\mu}+M_{\alpha} + M_{\beta} + M_{\gamma} + M_{\alpha\gamma} = P_a \otimes Q_b \otimes P_c + I_a \otimes P_b \otimes I_c\\
    I-M &= I_{abc}- P_a \otimes Q_b \otimes P_c - I_a \otimes P_b \otimes I_c \\
    Y'(I-M)Y & = \sum_{i=1}^{a}\sum_{j=1}^{b}\sum_{k=1}^{c}(Y_{ijk}-\bar{Y}_{i.k} -\bar{Y}_{.j.}  +\bar{Y}_{...})^2
\end{align*}
\end{itemize}

\exercise
Let D denote having a certain disease and E denote having exposure to a certain risk factor. The attributable risk (AR) is the proportion of disease cases attributable to that exposure.\\
\begin{itemize}
    \item [(a)] Let $P(\bar{E}) = 1-P(E)$. Explain why 
\begin{align*}
     AR = [P(D) - P(D|\bar{E})]/P(D)
\end{align*}
Attributable risk = Incidence risk among an exposed group - Incidence risk among a non-exposed group\\
\begin{align*}
     P(D) &= P(D, E) + P(D, \bar{E})\\
     P(E)P(D|\bar{E}) &= [1-P(\bar{E})]P(D|\bar{E}) = P(D|\bar{E}) - P(D, \bar{E})
\end{align*}
The above formula changed the product of probability to subtraction of two probabilities.\\

\item [(b)] Show that AR relates to the relative risk RR by  
    \begin{align*}
     AR = [P(E)(RR-1)]/[1+ P(E)(RR-1)]\\
\end{align*}
\end{itemize}

\exercise
\textbf{Expectation, Variance Calculation for conditional/joint/marginal distribution}Consider a pair of random variables $(X, Y$ and let $\mu_x = E(X), \sigma_x^2= Var(X), \mu_y = E(Y), \sigma_y^2 = Var(Y), \sigma_{xy} = Cov(X,Y)$.
\begin{itemize}
    \item [(a)] Define $(\alpha_0, \beta_0)$ as the minimizers of $E[Y-(\alpha + \beta X)]^2$, where $\alpha$ and $\beta$ are real numbers and the expectation is with respect to the joint distribution of (X,Y). Derive explicit expression for $(\alpha_0, \beta_0)$ .
    \begin{align*}
    Cov(X,Y) &= E(XY) - E(X)E(Y), \qquad E(XY) = \sigma_{xy} + \mu_x\mu_y\\
    Var(Y) &= E(Y^2) - E(Y)^2, \qquad E(Y^2) = \sigma_y^2 + \mu_y^2\\
    Var(X) &= E(X^2) - E(X)^2, \qquad E(X^2) = \sigma_x^2 + \mu_x^2\\
     E[Y-(\alpha + \beta X)]^2 &= E[Y^2 + (\alpha + \beta X)^2 -2(\alpha + \beta X)Y ]\\
     &= E(Y^2) + (\alpha^2 + \beta^2 E(X^2) + 2\alpha\beta E(X)) - 2(\alpha E(Y) + \beta E(XY))\\
     &= \sigma_y^2+\mu_y^2 + \alpha^2 + \beta^2 (\sigma_x^2 + \mu_x^2) + 2\alpha\beta \mu_x -2\alpha \mu_y -2 \beta (\sigma_{xy}+ \mu_x\mu_y)\\
     \frac{\partial g(\alpha,\beta)}{\partial \alpha} &= 2\alpha + 2\beta \mu_x - 2\mu_y = 0\\
     \frac{\partial g(\alpha,\beta)}{\partial \beta} &=2\beta (\sigma_x^2 + \mu_x^2) + 2\alpha \mu_x -2(\sigma_{xy}+\mu_x\mu_y) = 0\\
     \alpha &= \mu_y - \beta \mu_x\\
     \beta &= \frac{\sigma_{xy}}{\sigma_x^2}
    \end{align*}    
    \item[(b)] Let U be a random variable which is independent of X and Y for which $E(U) = 0$ and $E(U^2) = \sigma_u^2$. Suppose one is to observe, instead of the pair (X,Y), the pair (W, Y), where W= X + U. We may think of X as being observed with some measurement error U. Define $\alpha_0^w$ and $\beta_0^w$ as the minimizers of $E([Y- (\alpha + \beta W)]^2)$, where the expectation is with respect to the joint distribution of (W, Y). Let $\lambda = \sigma_x^2/(\sigma_x^2 + \sigma_u^2)$. Derive expression for $\alpha_0^w$ and $\beta_0^w$ which only involve $\alpha_0, \beta_0, \lambda, \mu_x, \beta_0$.
    \begin{align*}
    E(WY) &= E((X+U)Y) = E(XY) + E(UY) = (\sigma_{xy} + \mu_x\mu_y), \qquad \text{independence of U, Y}\\
    Cov(W,Y) &= E(WY) - E(W)E(Y) = \sigma_{xy} + \mu_x\mu_y - \mu_x\mu_y =\sigma_{xy} \\
    Var(Y) &= E(Y^2) - E(Y)^2, \qquad E(Y^2) = \sigma_y^2 + \mu_y^2\\
    Var(W) &= Var(X+U) = = Var(X) + Var(U) = \sigma_x^2 + \sigma_u^2, \qquad E(X^2) = \sigma_x^2 + \mu_x^2\\
    E(W) &= E(X) = \mu_x\\
    E(W^2) &= Var(W) + E(W)^2 = \sigma_x^2 + 2\mu_x^2 \\
     E[Y-(\alpha + \beta W)]^2 &= E[Y^2 + (\alpha + \beta W)^2 -2(\alpha + \beta W)Y ]\\
     &= E(Y^2) + (\alpha^2 + \beta^2 E(W^2) + 2\alpha\beta E(W)) - 2(\alpha E(Y) + \beta E(WY))\\
     &= \sigma_y^2+\mu_y^2 + \alpha^2 + \beta^2 (\sigma_x^2 + 2\mu_x^2) + 2\alpha\beta \mu_x -2\alpha \mu_y -2 \beta (\sigma_{xy}+ \mu_x\mu_y)\\
     \frac{\partial g(\alpha,\beta)}{\partial \alpha} &= 2\alpha + 2\beta \mu_x - 2\mu_y = 0\\
     \frac{\partial g(\alpha,\beta)}{\partial \beta} &=2\beta (\sigma_x^2 + 2\mu_x^2) + 2\alpha \mu_x -2(\sigma_{xy}+\mu_x\mu_y) = 0\\
     \alpha &= \mu_y - \beta \mu_x\\
     \beta^w &= \frac{Cov(W,Y)}{Var(W)} = \frac{\sigma_{xy}}{\sigma_x^2 + \sigma_u^2} = \lambda \beta_0
    \end{align*} 
    \item[(c)] Suppose $Y=\alpha_0 + \beta_0 X + \epsilon$, where $E(\epsilon) = 0$ and $Var(\epsilon) = \sigma_\epsilon^2$. Show that we may write $Y=\alpha_0^w + \beta_0^w W + \eta$, where $E(\eta) = 0$ and $Var(\eta) = \sigma_\eta^2$, and find expressions for $\sigma_\eta^2$ only in terms of $\sigma_\epsilon^2, \lambda, \sigma_x^2, \beta$.\\
    \begin{align*}
    Y &=\alpha_0 + \beta_0 X + \epsilon\\
    &= \alpha_0^w + \beta_0^w W + (\alpha_0 - \alpha_w - \beta_0^w W + \beta_0 X + \epsilon)\\
    \eta &= \alpha_0 - \alpha_w - \beta_0^w W + \beta_0 X + \epsilon\\
    \alpha_0 &= \mu_y - \beta \mu_x\\
    \alpha_0^w &= \mu_y - \lambda \beta_0\mu_x, \qquad X= W-U\\
    \beta_0 &= \frac{\sigma_{xy}}{\sigma_x^2}, \qquad \beta_0^{w} = \lambda \beta_0\\
    \eta &=  \mu_y - \beta \mu_x -  \mu_y + \lambda \beta_0\mu_x + \beta_0 (W-U) -\lambda \beta_0 W + \epsilon\\
    &= \beta_0\mu_x(\lambda - 1) + \beta_0W (1-\lambda) -\beta_0 U + \epsilon\\
    E(\eta) &= \beta_0\mu_x(\lambda - 1)+ \beta_0 (1-\lambda) \mu_x =0\\
    \sigma_\eta^2 &= Var(\eta) = Var(\beta_0\mu_x(\lambda - 1) + \beta_0 W (1-\lambda) -\beta_0 U + \epsilon)\\
    &= beta_0^2(1-\lambda)^2Var(W) - \beta_0^2 Var(U) -2 \beta_0^2 (1-\lambda) Cov(W,U) + Var(\epsilon)\\
    &= \sigma_\epsilon^2 + \beta_0^2(1-\lambda)^2 (\sigma_x^2 + \sigma_u^2) + \beta_0^2\sigma_u^2 - 2\beta_0^2(1-\lambda) Cov(X+U, U)\\
    Cov(X+U, U) &= Cov(X,U) + Var(U) = \sigma_u^2\\
    \sigma_\eta^2 = \sigma_\epsilon^2 + \lambda\beta_0^2\sigma_u^2
    \end{align*}     
    \item[(d)] Consider a dataset with n independent realizations $(W_1, Y_1), ..., (W_n,Y_n)$ of $(W,Y)$ and another dataset with n independent realizations $(X_1, \Tilde{Y}_1),.. , (X_n, \Tilde{Y}_n)$ of $(X,Y)$, where $W= X+U$. Let $\hat{\alpha}^w, \hat{\beta}^w$ be the minimizers of $\frac{1}{n} \sum_{i=1}^{n} [Y_i - (\alpha+ \beta W_i)]^2$, so is $(\hat{\alpha}, \hat{\beta})$. \\
    \begin{itemize}
        \item [(i)] For large n, give an expression for $E(\hat{\beta}^w)$ and comment on whether $\hat{\beta}^w$ is a good estimator of $\beta_0$ in terms of bias and consistency.\\
        Similar as above, we have 
     \begin{align*}
    g(\alpha, \beta) &=\frac{1}{n} \sum_{i=1}^{n} [Y_i - (\alpha+ \beta W_i)]^2\\
    \hat{\alpha} &= \bar{Y} - \hat{\beta}^w \bar{X}, \qquad \bar{Y} = \frac{1}{n}\sum Y_i,\bar{X} = \frac{1}{n}\sum X_i \\
    \hat{\beta}^w &= \frac{\sum_{i=1}^n (W_i - \bar{W})(Y_i - \bar{Y})}{\sum_{i=1}^n (W_i-\bar{W})^2}, \qquad \bar{W} = \frac{1}{n} \sum W_i \\
    \hat{\beta}^w &= \frac{\hat{\sigma_{wy}}}{\hat{\sigma_w^2}} \xrightarrow[n]{ \infty} \frac{\sigma_{wy}}{\sigma_w^2(w)} = \lambda \beta_0\\
    \end{align*}      
    $\hat{\beta}^w$ is asymptotically bias for $\beta_0$, also it is not consistent.\\
    \item[(ii)] Suppose $\hat{\beta}^w$ is used to test the hypothesis: $H_0: \beta_0=0$ vs. $H_1: \beta_0 \neq 0$. Using the results in part (c), and for large n, derive an inequality only in terms of , as to when the t-stat for this hypothesis for the model in part (c) based on measurement error is smaller than the t-stat based on the model without measurement error.\\
    
    \item[(iii)] Examine the t-stat in part (ii) as a function of $\sigma_u^2$, and use the results in (ii) to examine the properties of this t- stat in terms of power and sample size as $\sigma_u^2 \rightarrow 0$ and $\sigma_u^2 \rightarrow \infty$ compared to a test stat based on $\hat{\beta}$.
    \end{itemize}
    \item[(e)] Suppose $(X,Y)^T \sim N_2(\mu, \Sigma)$, where $\mu= (\mu_x, \mu_y)'$ and $\Sigma = \begin{bmatrix}
           \sigma_x^2  &  \sigma_{xy}  \\
            \sigma_{xy} &   \sigma_{y}^2\\
         \end{bmatrix}$. i) Derive the conditional distribution of $Y|X$ and show that $E(Y|X) = \alpha_0 + \beta_0 X$. \\
         Construct $Z= Y+ AX$ so that Z is independent of X, $A= -\frac{\sigma_{xy}}{\sigma_x^2}$. Then the conditional distribution $Y|X$ will be only related to $X$.
    \end{itemize}
    
\exercise
\textbf{linear model} Suppose that Y is a $4 \times 1$ vector with $E(Y ) = \mu, \mu \in C(E)$, where E is the set $E = \{ \mu: \mu' = (\beta_1 + \beta_2 - \beta_3, \beta_2 + \beta_3, -\beta_2 - \beta_3, -\beta_1-\beta_2 + \beta_3) \} $ where the $\beta_i$ are real numbers, $i = 1, 2, 3$. Further assume that $Cov(Y) = \sigma^2I_{4 \times 4}$, where $\sigma^2$ is unknown.\\
\begin{itemize}
    \item [(a)] Derive $\hat\mu$, the ordinary least squares estimate of $\mu$, by carrying out the
appropriate projection.\\
E(Y) is in the column space of C(E), we need to find the o.p.o on C(E). Also $Cov(Y) = \sigma^2 I_{4 \times 4}$, we can use ordinary least squares estimator as i.i.d.
\begin{align*}
    \mu' &= (\beta_1 + \beta_2 - \beta_3, \beta_2 + \beta_3, -\beta_2 - \beta_3, -\beta_1-\beta_2 + \beta_3) = X \beta\\
    \beta &= (\beta_1, \beta_2, \beta_3)^T\\
    X &= \begin{bmatrix}
           1  & 1 & -1  \\
           0 &  1 & 1\\
           0  & -1 & -1  \\
           -1 &  -1 & 1\\
         \end{bmatrix}\\
    C(X) &= X_1 = \begin{bmatrix}
           1  & 1   \\
           0 &  1 \\
           0  & -1  \\
           -1 &  -1 \\
         \end{bmatrix}\\
    M_{\mu} &= X_1(X_1'X_{1})^{-1} X_{1}^T = \begin{bmatrix}
           1  & 1   \\
           0 &  1 \\
           0  & -1  \\
           -1 &  -1 \\
         \end{bmatrix}\begin{bmatrix}
           1  & -1/2   \\
           -1/2 &  1/2 \\
         \end{bmatrix}\begin{bmatrix}
           1 & 0 & 0 & -1    \\
           1 & 1 & -1 & -1
         \end{bmatrix}\\
       \hat{\mu} &= M_{\mu} Y =  1/2 \begin{bmatrix}
           1  & 0 & 0 & -1   \\
           0 &  1 & -1 & 0 \\
           0  & -1 & 1 & 0 \\
           -1 &  0 & 0 & 1 \\
         \end{bmatrix} (y_1, y_2, y_3, y_4)^T = 1/2 (y_1- y_4, y_2-y_3, y_3-y_2, y_4-y_1)^T
\end{align*}
\item[(b)] Find the BLUE of $\beta_2 - \beta_3$ or show that it is nonestimable.
\begin{align*}
    \lambda &= (0, 1,-1)^T, \qquad \lambda^T \beta = \beta_2 - \beta_3\\
    \lambda^T &= \rho^T X = (\rho_1, \rho_2, \rho_3, \rho_4)  \begin{bmatrix}
           1  & 1 & -1  \\
           0 &  1 & 1\\
           0  & -1 & -1  \\
           -1 &  -1 & 1\\
         \end{bmatrix} = (0, 1,-1)\\
         \rho_1 &= \rho_4, \qquad \rho_2 - \rho_3 = 1, \qquad \rho_2 - \rho_3 = -1
\end{align*}
The contradict of $\rho_2 - \rho_3$ indicate that  $\beta_2 - \beta_3$ is not estimable.
\item[(c)] Consider testing $H_0: \beta_2 + \beta_3 = 0$  versus $H_1: \beta_2 + \beta_3 \neq 0$ . Let $E_0$ denote the set E assuming that $H_0$ is true. Explicitly give the sets $E_0$ and $E \cap E_0^{\perp}$.\\
Find the $M_{0}, M_{1}$ are the o.p.o onto C(X) for $H_0, H_1$.$\textbf{Not on C(Y)}$.Then the $C(M_{0}), C(M_{1})$ and the sets $E_0$ and $E \cap E_0^{\perp}$ relationship needs attention.
\begin{align*}
    \lambda &= (0, 1, 1)^T, \qquad \lambda^T \beta = \beta_2 + \beta_3\\
    \lambda^T &= \rho^T X = (\rho_1, \rho_2, \rho_3, \rho_4)  \begin{bmatrix}
           1  & 1 & -1  \\
           0 &  1 & 1\\
           0  & -1 & -1  \\
           -1 &  -1 & 1\\
         \end{bmatrix} = (0, 1, 1)\\
         \rho &= (1, 2, 1, 1)^T
\end{align*}
$ \beta_2 + \beta_3$ is estimable with one $\rho = (1, 2, 1, 1)^T$. Then we can have $H_0: \rho^T M Y = 0$ that $\rho^T M \perp C(E_0)$. 
\begin{align*}
    M_1 &= ( M\rho) [(M\rho)^T ( M\rho)]^{-1} ( M\rho)^T\\
    M\rho &=  \rho_N= (0, 1, -1, 0)^T \\
    M_1 &= \rho_N [(\rho_N)^T (\rho_N)]^{-1} (\rho_N)^T =1/2 \begin{bmatrix}
           0  & 0 & 0 & 0  \\
           0 &  1 & -1 & 0\\
           0  & -1 & 1 & 0 \\
           0 & 0 & 0 & 0\\
         \end{bmatrix} \\
         & = 1/2 (0, 1, -1, 0)^T
\end{align*}
And the complement of $M_0 = M- M_1$ 
\begin{align*}
    M_0 & = M- M_1 = 1/2 \begin{bmatrix}
           1  & 0 & 0 & -1  \\
           0 & 0 & 0 & 0\\
           0 & 0 & 0 & 0\\
           -1  & 0 & 0 & 1  \\
         \end{bmatrix} \\
         & = 1/2 (1, 0,  0, -1)^T
\end{align*}
Also we can just look at C(Y) column space
\begin{align*}
    E_0 & = span \{(\beta_1+2\beta_2, 0, 0, -\beta_1-2\beta_2)^T \}= span \{(1, 0,  0, -1)^T \}
\end{align*}
\item[(d)] Assuming normality for Y , construct the simplest possible expression for
the F statistic for the hypothesis $H_0: \mu \in E_0$ versus $H_1: \mu \not\in E_0$, where $E_0$ is
specified in part (c), and give the distribution of the F statistic under the null and
alternative hypotheses.\\
\begin{align*}
    M\rho &=  \rho_N= (0, 1, -1, 0)^T \in M, \qquad r(\rho_N) = 1\\
    M_{\rho} &= \rho_N [(\rho_N)^T (\rho_N)]^{-1} (\rho_N)^T=1/2 \begin{bmatrix}
           0  & 0 & 0 & 0  \\
           0 &  1 & -1 & 0\\
           0  & -1 & 1 & 0 \\
           0 & 0 & 0 & 0\\
         \end{bmatrix}\\
    MSE &= \lVert (I-M)Y \rVert = 1/2 (y_1+y_4)^2 + 1/2 (y_2 + y_4)^2 \\
    F &= \frac{Y^T M_{\rho} Y/r(\rho)}{MSE} = \frac{2(y_2 - y_3)^2}{(y_1+y_4)^2 + (y_2 + y_4)^2} \sim F(1,2, \gamma), \qquad r(M-M_{\rho}) = 1, r(I-M) = 2
\end{align*} 
In which, under $H_0, \gamma = 0$, and under $H_1$.
\begin{align*}
    \gamma &= \frac{\lVert (M_1) X\beta \rVert}{2 \lVert (I-M)Y \rVert/2}\\
    &= \frac{(\beta_2 + \beta_3)^2}{\sigma^2}
\end{align*} 
\item[(e)] Assuming normality for Y , construct an exact 95$\%$ confidence interval for $\beta_2 + \beta_3$.\\
From part(d), we have
\begin{align*}
 \lambda' = (0, 1, 1)\\
 \rho &= (1, 1, 0, 1)^T\\
 \lambda^T \beta &=  \rho M Y = M_1 Y= 1/2 (y_2 - y_3)\\
  F &= \frac{\lVert \lambda^T \beta \rVert/r(\rho)}{\sigma^2} = \frac{\lambda'\beta [\lambda'[X'X]^{-1} \lambda]^{-1} (\lambda'\beta)^T }{\sigma^2} \sim F(1,2, \gamma)\\
  [\lambda'[X'X]^{-1} \lambda]^{-1} & = 2\\
 &\{\beta: \frac{\lambda'\beta [\lambda'[X'X]^{-1} \lambda]^{-1} (\lambda'\beta)^T }{\sigma^2} \leq F(0.95, 1,2) \} 
\end{align*} 

\end{itemize}
\exercise
Consider the linear model $Y = X\beta + Z\gamma + \epsilon$, where $E(\epsilon) = 0$ and $Cov(\epsilon) = V$, V is assumed known and positive definite, and $(\beta, \gamma)$ are unknown. Further, let $A =X(X'V^-X)^-X'V^{-1}$, X is $n \times p$, Z is $n \times q$, and both X and Z may be less than full rank. Let C(H) denote the usual label for the column space of an arbitrary matrix H.
\begin{itemize}
    \item [(a)] Show that $(I-A)'V^{-1}(I-A) = (I-A)'V^{-1} = V^{-1}(I-A)$. \\
    Lets review the o.p.o onto C(X), and a projection $A =X(X'V^-X)^-X'V^{-1}$.
\begin{align*}
 V &= QQ^{T}\\
 P &= Q^{-1}X [(Q^{-1}X)' (Q^{-1}X)]^{-} (Q^{-1}X)^{T} = Q^{-1}X [X' V^{-1}X]^- X^{T} Q^{-1} '
\end{align*}     
We need to transform P to A, so by definition, $AX = X$. 
\begin{align*}
 P Q^{-1}X &= Q^{-1}X \\
 Q^{-1}X [(Q^{-1}X)' (Q^{-1}X)]^{-} (Q^{-1}X)^{T} Q^{-1}X &= Q^{-1}X\\
 Q^{-1}X [X' V^{-1}X]^{-} X^T Q^{-1}'Q^{-1}X &= Q^{-1}X\\
 Q^{-1} A X &= Q^{-1}X\\
\end{align*}  
Then we have $AX = X$, then A is a projection onto C(X). \textbf{Need attention that A is not a o.p.o. with respective to $x'y$ (not symmetric) it is only a projection. But is o.p.o to $x' V^{-1}y$ }.\\
For $\rho'A Y = \rho' MY$, that $M= X(X'X)^{-} X^{T}$, we need to have $C(X) = C(VX), C(X) = C(V^{-1}X)$, then $A =X(X'V^-X)^-X'V^{-1}$.\\
Show that A, I-A are projections
\begin{align*}
A^2 &= A\\
X(X'V^-X)^-X'V^{-1} X(X'V^-X)^-X'V^{-1} &= X(X'V^-X)^-X'V^{-1}\\
(I-A)^2 &= I-A \\
(I-A)(I-A) &= I-A-A + A^2 = I-A
\end{align*}  
Then transform the equation
\begin{align*}
(I-A)'V^{-1}(I-A)(I-A) &= (I-A)'V^{-1}(I-A), \\
(I-A)'V^{-1}(I-A) &= (I-A)'V^{-1} 
\end{align*} 
To show $(I-A)'V^{-1}(I-A) = V^{-1}(I-A)$
\begin{align*}
(I-A)'(I-A)' &= [(I-A)(I-A)]^T = (I-A)^T\\
(I-A)'(I-A)'V^{-1}(I-A) &= (I-A)'V^{-1}(I-A) \\
(I-A)'V^{-1}(I-A) &= V^{-1}(I-A) 
\end{align*} 
\item[(b)] Show that A is the projection operator onto C(X) along $C(V^{-1} X)^{\perp}$.
We need to show $AX=X, x \in C(X) $, and $Aw = 0, w \in C(V^{-1} X)^{\perp}, C(V^{-1}X) = C(X)$
We have shown that $AX=X$ in above (a), then let $w \in C(V^{-1} X)^{\perp}$
\begin{align*}
(V^{-1} X)'w &= 0, \qquad X' V^{-1} w = 0\\
Aw & =w, \qquad  X' V^{-1} A w = 0 \\
X' V^{-1} A &= 0, \qquad A \perp  C(V^{-1} X)
\end{align*}
\item[(c)]Let B denote the projection operator onto C(X,Z) along $C(V^{-1} (X,Z))^{\perp}$.
Assume that all matrix inverses exist. Show that
\begin{align*}
B &= A + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1}
\end{align*}
To show B is a projection, $B^2 = B$, but if need to show projection onto C(X,Z), then show $BX=X, x \in C(X,Z)$. If need to show projection along, then show$ Bw = 0, w \in C(V^{-1} (X,Z))^{\perp}$. 
\begin{align*}
B^2 &= \left(A + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1} \right) \left(A + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1} \right) \\
&= \left(A + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1} \right) = B\\
A(I-A) & = 0\\
\end{align*}
Show  $B (X, Z) = (BX, BZ) = (X, Z)$.
\begin{align*}
BX &= \left(A + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1} \right) X\\
&= AX + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1} (I-A)  X =X , \qquad \text{part (a)}\\
BZ &= \left(A + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1} \right) Z\\
&= AZ + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1} (I-A)Z = AZ + (I-A)Z = Z, \quad \text{part (a)}
\end{align*}
Next show projection along $C(V^{-1} (X,Z))^{\perp}$. 
\begin{align*}
(V^{-1} (X,Z))'w &= 0, \qquad X' V^{-1} w = 0, Z' V^{-1} w = 0\\
Bw &= \left(A + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1} \right) w= 0 \\
&= Aw + (I-A)Z [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)'V^{-1} w = 0\\
(X,Z)' V^{-1} B &= 0, \qquad B \perp  C(V^{-1} (X,Z))
\end{align*}
\item[(d)] Show that $(\hat{\gamma}, \hat{\beta})$ are generalized BLUE's for the linear model, where $(\hat{\gamma}, \hat{\beta})$ satisfy
\begin{align*}
\hat{\gamma} &= [Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)' V^{-1} (I-A)Y\\
X\hat{\beta} &= A(Y-Z \hat{\gamma})
\end{align*}
To show projection operator onto C(Z) is $[Z'(I-A)' V^{-1} (I-A)Z]^{-1} Z'(I-A)' V^{-1}$, also it is along $C(V^{-1} Z)^{\perp}$.\\
By Gauss-Markov theorem, the least square estimates of $\gamma, \beta$ are BLUE's. 
\begin{align*}
Y &= X\beta + Z\gamma \\
AY &= AX\beta + AZ\gamma = X\beta + AZ\gamma \\
(I-A)Y &= (I-A)Z \gamma\\
\end{align*}
We can construct projection operation regarding C(Z).
\begin{align*}
Q^{-1}(I-A)Y &= Q^{-1}(I-A)Z \gamma\\
P_z &= Q^{-1}(I-A)Z [(Q^{-1}(I-A)Z)' (Q^{-1}(I-A)Z)]^{-} (Q^{-1}(I-A)Z)' \\
P_z Q^{-1}(I-A)Z &= Q^{-1}(I-A)Z \\
\end{align*}
\begin{align*}
[Z'(I-A)'V^{-1}(I-A)Z]^{-}Z'(I-A)'V^{-1} Z&=  Z, \qquad \text{also use part (a)}\\
C &= [Z'(I-A)'V^{-1}(I-A)Z]^{-}Z'(I-A)'V^{-1}, \qquad CZ = Z
\end{align*}
We have C a projection operator onto C(Z), also we can prove orthogonality to $C(V^{-1} Z)^{\perp}$.
Then we have proved.
\item[(e)] Suppose that $\epsilon \sim N_n(0, V )$ and V is known. Further, suppose that
 $\gamma, \beta$ are both estimable. From first principles, derive the likelihood ratio test for
the hypothesis $H_0: \gamma= 0 $, where $\gamma, \beta$ are both unknown, and state the exact
distribution of the test statistic under the null and alternative hypotheses.\\
To derive the likelihood ratio test, we link with F-test and construct independent chi-square statistics. The nominator is the square difference between two models, and the denominator is MSE.\\
\begin{align*}
Y &= X\beta + Z\gamma + \epsilon, \qquad Y&= W\delta + \epsilon, \\
W &= (X, Z), \delta = (\beta, \gamma)^T, \qquad V = Q Q^{T}, X_{n \times p}, Z_{n \times q}, 
\end{align*}
Assume rank(W) = p+q, \textbf{Pay attention to define matrix rank}
\begin{align*}
Q^{-1}Y &= W = Q^{-1}X \beta + Q^{-1}Z \gamma \sim N(0, I) \\
Q^{-1}Y &= \Tilde{Y}, Q^{-1}Z = \Tilde{Z}, \qquad Q^{-1}X = \Tilde{X}\\
\Tilde{Y} &= \Tilde{W} \delta + \Tilde{\epsilon},  \qquad \Tilde{\epsilon} \sim N(0,I)
\end{align*}
The likelihood ratio test
\begin{align*}
LRT &= \frac{Sup_{H_0} L(\delta| W)}{Sup_{H_1}L(\delta| W)} 
\end{align*}
The likelihood under $H_0$
\begin{align*}
\delta_0 &= (\beta_0, 0)', \qquad W_0 = (\Tilde{X}) \\
L(\beta_0|W) &=\frac{1}{\sqrt{2\pi}} exp \left(\frac{-1}{2} (\Tilde{Y} - W_0\delta_0)'(\Tilde{Y} - W_0\delta_0) \right) \\
&= \frac{1}{\sqrt{2\pi}} exp \left(\frac{-1}{2} (\Tilde{Y} - \Tilde{X}\hat{\beta})'(\Tilde{Y} - \Tilde{X}\hat{\beta}) \right)\\
\Tilde{X}\hat{\beta} &= M_0 W_0, \qquad \hat{\beta} \text{ satisfy}\\
M_0 &= \Tilde{X}[(\Tilde{X})'(\Tilde{X})]^{-1}\Tilde{X}' 
\end{align*}
Thus the numerator is 
\begin{align*}
L(\beta_0|W) &= \frac{1}{\sqrt{2\pi}} exp \left(\frac{-1}{2} (\Tilde{Y}' (I- M_0) \Tilde{Y} \right)
\end{align*}
The likelihood under $H_1$
\begin{align*}
L(\delta|W) &=\frac{1}{\sqrt{2\pi}} exp \left(\frac{-1}{2} (\Tilde{Y} - W\delta)'(\Tilde{Y} - W\delta) \right)\\
\Tilde{W} \hat{\delta} &= M \Tilde{W} \qquad \hat{\delta} \text{ satisfy}\\
M &= \Tilde{W}[(\Tilde{W})'(\Tilde{W})]^{-1}\Tilde{W}' 
\end{align*}
Thus the denominator is 
\begin{align*}
L(\delta|W) &= \frac{1}{\sqrt{2\pi}} exp \left(\frac{-1}{2} (\Tilde{Y}' (I- M) \Tilde{Y} \right)
\end{align*}
The LRT test 
\begin{align*}
-2 \Lambda &= \frac{\left(\Tilde{Y}' (I- M_0) \Tilde{Y} \right)}{\left(\Tilde{Y}' (I- M) \Tilde{Y} \right)} = \left(\Tilde{Y}' (M -M_0) \Tilde{Y} \right)
\end{align*}
Thus we reject the $H_0$, if $\left(\Tilde{Y}' (M -M_0) \Tilde{Y} \right) > c$.
$M-M_0$ is an orthogonal projection as $(M-M_0)(M-M_0) = (M-M_0)$ and $M-M_0$ is symmetric. So $r(M-M_0) = q$ when X and Z are full rank.\\
\begin{align*}
\left(\Tilde{Y}' (M -M_0) \Tilde{Y} \right) &\sim (H_0) \chi^2(q)\\
\left(\Tilde{Y}' (M -M_0) \Tilde{Y} \right) &\sim (H_1) \chi^2(q, \theta)\\
\theta &= \frac{\lVert (M-M_0) \Tilde{W} \delta \rVert}{2}
\end{align*}
Thus we reject $H_0$ at level $\alpha$ if $\left(\Tilde{Y}' (M -M_0) \Tilde{Y} \right) > \chi^2_{\alpha}(q)$
\item[(f)] Suppose that $\epsilon \sim N_n(0, \sigma^2 R )$, where R is known and positive definite, and
$\beta, \gamma, \sigma^2$ are all unknown. Further, assume that $\beta, \gamma$ are both estimable. Derive an exact joint 95$\%$ confidence region for $\beta, \gamma, \sigma^2$.\\
To write the distribution of $\beta, \gamma, \sigma^2$, such as F distribution or Chi-square (if $\sigma^2$ is known) distribution $M_{\beta}/ \sigma^2, M_{\gamma}/ \sigma^2$.
\begin{align*}
Y &= X\beta + Z\gamma + \epsilon, \qquad Y = W\delta + \epsilon, \\
W &= (X, Z), \delta = (\beta, \gamma)^T, \qquad R = Q Q^{T}, X_{n \times p}, Z_{n \times q}, 
\end{align*}
Assume rank(W) = p+q, \textbf{Pay attention to define matrix rank}
\begin{align*}
Q^{-1}Y &= W = Q^{-1}X \beta + Q^{-1}Z \gamma \sim N(0, \sigma^2 I)\\
Q^{-1}Y &= \Tilde{Y}, Q^{-1}Z = \Tilde{Z}, \qquad Q^{-1}X = \Tilde{X}\\
\Tilde{Y} &= \Tilde{W} \delta + \Tilde{\epsilon},  \qquad \Tilde{\epsilon} \sim N(0,\sigma^2 I)
\end{align*}
We have the F test for $\beta$  
\begin{align*}
\lambda' &= (1,0,0), \qquad \rho'\Tilde{W} = \lambda', \\
F_{\beta} &= \frac{(\Tilde{Y}- \Tilde{W} b)'M_{MP} (\Tilde{Y}- \Tilde{W} b)/r(M_{MP})}{\sigma^2}\\
 &= \frac{(\Tilde{Y}- \Tilde{W} b)' (M\rho) [\rho' M \rho]^{-} (M\rho)' (\Tilde{Y}- \Tilde{W} b)/r(M_{MP})}{\sigma^2} \\
 &= \frac{( \rho'M \Tilde{Y}- \rho'M \Tilde{W} b)'  [\rho' M \rho]^{-} ( \rho'M \Tilde{Y}-  \rho'M \Tilde{W} b/r(M_{MP})}{\sigma^2} \\
&=  \frac{(\lambda' \hat{\delta} - \lambda' \delta)'  [\rho' M \rho]^{-} (\lambda' \hat{\delta} - \lambda' \delta)/r(M_{MP})}{\sigma^2} \\
&=  \frac{(\hat{\beta} - \beta)'  [\rho' M \rho]^{-} (\hat{\beta} - \beta)/r(M_{MP})}{\sigma^2}, \qquad r(M_{MP}) = p
\end{align*}
We have the chi-square distribution for $\sigma^2$ 
\begin{align*}
\frac{Y'(I-M) Y}{\sigma^2} & \chi^2(n-p-q) \\
\end{align*}
The degrees of freedom is $n-p-q$ since both $(\beta, \gamma)$ are estimable. \\
Furthermore, we have the chi-square distribution for $\delta$, because $M \perp (I-M)$
\begin{align*}
\frac{ \lVert M Y - \Tilde{W} \delta \rVert}{\sigma^2} & \sim \chi^2(p+q) \\
P\{ \chi^2_{a/2}{(p+q)} &\leq \frac{\lVert MY - \Tilde{W} \delta  \rVert}{\sigma^2} \leq \chi^2_{1-a/2}{(p+q)}, \quad
\chi^2_{b/2}{(n-p-q)} \leq \frac{\lVert (I-M) Y \rVert}{\sigma^2} \leq \chi^2_{1-b/2}{(n-p-q)} \}\\
&= 1- \alpha
\end{align*}
Such that $(1-a)(1-b) = 1-\alpha$. \\
The region is given
\begin{align*}
\{(\delta, \sigma^2): \chi^2_{a/2}{(p+q)} &\leq \frac{\lVert MY -\Tilde{W} \delta \rVert}{\sigma^2} \leq \chi^2_{1-a/2}{(p+q)}, \quad
\chi^2_{b/2}{(n-p-q)} \leq \frac{\lVert (I-M) Y \rVert}{\sigma^2} \leq \chi^2_{1-b/2}{(n-p-q)} \}\\
&= 1- \alpha
\end{align*}
\end{itemize}

\exercise
\textbf{Hypergeometric distribution} Dervie the hypergeometric distribution \\
\begin{align*}
    p(n_{11}|n_{1.}, n_{.1}, n, \Xi) &=  \frac{p(n_{11}, n_{1.}, n_{.1}, |n)}{p( n_{1.}, n_{.1}, |n)} \\
    &= \frac{n!}{n_{11}!n_{12}!n_{21}!n_{22}!} \Xi^{n_{11}} \\
   \frac{n!}{n_{11}!n_{12}!n_{21}!n_{22}!} &=  \frac{n! n_{1.}! (n-n_{1.})!}{n_{1.}! (n-n_{1.})! n_{11}!n_{12}!n_{21}!n_{22}!} \\
    &= {n \choose n_{1.}} {n_{1.} \choose n_{11}} {n-n_{1.} \choose n_{.1}-n_{11}} 
\end{align*}

\setcounter{exercise}{7}
\exercise*
\dots{} or skip a few, using the \verb|\setcounter{exercise}{x}| command.

For more information, refer to \url{https://github.com/gijs-pennings/latex-homework}.

\end{document}
