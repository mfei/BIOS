% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[thinc]{esdiff}
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\title{Hypergeometric}
\author{Mingwei Fei}

\begin{document}
	
	\maketitle
	
\section{Hypergeometric vs. Binomial}
	Hypergeometric is drawing objects without replacement, while binomial is drawing objects with replacement. So the binomial distribution has the probability p for each drawn, while the hypergeometric probability mass function:
	
	\begin{align*}
		p_X(k) &= p(X=k) = \frac{{K \choose k} {N-K \choose n-k}}{ {N \choose n}}
	\end{align*}
	Here we already know how many K objects totally, and we would like to draw k out of K. The difference of replacement vs. no replacement is the probability. With replacement, we could get $p= \frac{K}{N}$ for each time, while without replacement, we get the probability by using the designed drawn divided by all possible drawns. That's how we derive the concepts of nuisance parameters as well.
	
	Hypergeometric distribution is derived from binomial, $a \choose b$ is a binomial coefficients, which means two outcomes, yes or no. 

	Hypergeometric distribution focuses on the range of k, and the sum of all probabilities of p(k) is 1, so we need to know all possible values for k. $ 0 \leq k \leq min(n, K)$.
	

\subsection{Contingency Table- Likelihood function is the Key}
Consider a $I \times J$ contingency table of cell counts, where each cell count is denoted by $n_{ij}, i=1,..I, j=1,..J$, and thus $n_{ij}$ denotes the cell count of ith row and jth column, and $n_{ij} \sim Poisson (\mu_{ij})$ and independent. Further, let $n= \sum_{j=1}^J \sum_{i=1}^I n_{ij}$ denote the grand total.

\begin{itemize}
	\item [(a)] Derive the joint distribution of $(n_{11}, n_{12},... n_{ij})$ conditional on grand total n.
	By poisson distribution of each cell counts
	\begin{align*}
	n &= \sum_{i=1}^I \sum_{j=1}^J n_{ij} \sim \frac{exp(-\mu) \mu^n }{n!}, \qquad \mu= \sum_{i=1}^I \sum_{j=1}^J \mu_{ij}\\ 
	p(n_{11},..n_{ij}|n) &= \frac{\prod_{i=1}^I \prod_{j=1}^J \frac{exp(-\mu_{ij})  {\mu_{ij}}^{n_{ij}}}{n_{ij}!}}{\frac{exp(-\mu) \mu^n }{n!}} \\
	&= {n \choose n_{11} n_{12} ... n_{ij}} \frac{\prod_{i=1}^I \prod_{j=1}^J {\mu_{ij}}^{n_{ij}}}{\mu^n } \\
	&= {n \choose n_{11} n_{12} ... n_{ij}} \prod_{i=1}^I \prod_{j=1}^J \left( \frac{\mu_{ij}}{\mu } \right)^{n_{ij}}
	\end{align*}	
The joint distribution is Multinomial ($n; \pi_{11}, \pi_{12},.. \pi_{IJ}$), where $\pi_{ij} = \frac{\mu_{ij}}{\sum_{i=1}^I \sum_{j=1}^J \mu_{ij} }$
	\item [(b)] Suppose all of the rows margins are assumed fixed. Derive the joint distribution of $(n_{11}, n_{12},... n_{ij})$.
\begin{align*}
	n_{i+} &= \sum_{j=1}^J n_{ij}\\
	n_{i+} & \sim Poisson (\sum_{j=1}^J \mu_{ij})\\
	p(n_{11},..n_{ij}|n_{i+}) &= \prod_{i=1}^I \prod_{j=1}^J \frac{exp(-\mu_{ij})  {\mu_{ij}}^{n_{ij}}}{n_{ij}!} \Bigg{/} \prod_{i=1}^I \frac{exp(-\mu_i) \mu_i^{n_{i+}}}{n_{i+}!}\\
	&= \prod_{i=1}^I {n_{i+} \choose n_{ij}} \prod_{i=1}^I \prod_{j=1}^J \left( \frac{\mu_{ij}}{\sum_{j=1}^J \mu_{ij}} \right)^{n_{ij}}
\end{align*}
	\item [(c)] Suppose all of the columns margins are assumed fixed. Derive the joint distribution of $(n_{11}, n_{12},... n_{ij})$.
\begin{align*}
	n_{+j} &= \sum_{i=1}^I n_{ij}\\
	n_{+j} & \sim Poisson (\sum_{i=1}^I \mu_{ij})\\
	p(n_{11},..n_{ij}|n_{+j}) &= \prod_{i=1}^I \prod_{j=1}^J \frac{exp(-\mu_{ij})  {\mu_{ij}}^{n_{ij}}}{n_{ij}!} \Bigg{/} \prod_{j=1}^J \frac{exp(-\mu_i) \mu_i^{n_{+j}}}{n_{+j}!}\\
	&= \prod_{j=1}^J {n_{+j} \choose n_{ij}} \prod_{i=1}^I \prod_{j=1}^J \left( \frac{\mu_{ij}}{\sum_{i=1}^I \mu_{ij}} \right)^{n_{ij}}
\end{align*}


\subsection{Non-central Hypergeometric distribution}	
Suppose that $I=2$ and $J=2$, and both the rows margins and column margins are fixed. Derive the joint distribution of $(n_{11}|n_{1+}, n_{+1} n)$, where $n_{1+} = n_{11} + n_{12}, n_{+1} = n_{11}+ n_{21}$.

\begin{align*}
	p(n_{11}|n_{1+}, n_{+1} n) &= \frac{p(n_{11}, n_{1+}, n_{+1} n)}{p(n_{1+}, n_{+1} n)}\\
		p(n_{ij}) &= \prod_{i=1}^2 \prod_{j=1}^2 \frac{exp(-\mu_{ij}) \mu_{ij}^{n_{ij}}}{n_{ij}!} \\
		&= \frac{exp(-\mu_{11})\mu_{11}^{n_{11}} }{n_{11}!} \frac{exp(-\mu_{12})\mu_{12}^{n_{12}}}{n_{12}!} \frac{exp(-\mu_{21})\mu_{21}^{n_{21}}}{n_{21}!} \frac{exp(-\mu_{22})\mu_{22}^{n_{22}}}{n_{22}!}\\
		n_{12} &= n_{1+} - n_{11}, \qquad n_{21} = n_{+1} - n_{11}, \\ n_{22} &= n - n_{12} - n_{21} - n_{11} = n- n_{1+} - n_{+1} + n_{11}\\
		p(n_{11}, n_{1+}, n_{+1} n) &= \frac{exp(-\mu_{11})\mu_{11}^{n_{11}} }{n_{11}!} \frac{exp(-\mu_{12})\mu_{12}^{n_{1+} - n_{11}}}{(n_{1+} - n_{11})!} \frac{exp(-\mu_{21})\mu_{21}^{n_{+1} - n_{11}}}{(n_{+1} - n_{11})!} \frac{exp(-\mu_{22})\mu_{22}^{n- n_{1+} - n_{+1} + n_{11}}}{(n- n_{1+} - n_{+1} + n_{11})!}
\end{align*}	
The Jacobian transformation matrix 
\begin{align*}
	J &=  \begin{pmatrix}
	\diffp{{n_{11}}}{{n_{11}}} & \diffp{{n_{11}}}{{n_{1+}}} & \diffp{{n_{11}}}{{n_{+1}}} & \diffp{{n_{11}}}{{n}}\\
	\diffp{{n_{12}}}{{n_{11}}} & \diffp{{n_{12}}}{{n_{1+}}} & \diffp{{n_{21}}}{{n_{+1}}} & \diffp{{n_{22}}}{{n}}\\
	\diffp{{n_{21}}}{{n_{11}}} & \diffp{{n_{21}}}{{n_{1+}}} & \diffp{{n_{21}}}{{n_{+1}}} & \diffp{{n_{22}}}{{n}}\\
	\diffp{{n_{22}}}{{n_{11}}} & \diffp{{n_{22}}}{{n_{1+}}} & \diffp{{n_{22}}}{{n_{+1}}} & \diffp{{n_{22}}}{{n}} \\
\end{pmatrix}= \begin{pmatrix}
1 & 0 & 0 & 0\\
-1 & 1 & 0 & 0\\
-1 & 0 & 1 & 0\\
1 & -1 & -1 & 1\\
\end{pmatrix}\\
\lVert J \rVert &= 1
\end{align*}
Then we can get the $p(n_{1+}, n_{+1}, n)$ by summing over $n_{11}$. We have $n_{11} <= n_{1+}, n_{11} <= n_{+1}$, and $n_{11} >= -n + n_{1+} + n_{+1}$. 		
\begin{align*}
	p(n_{11}, n_{1+}, n_{+1} n) &= \frac{exp(-\mu_{11})\mu_{11}^{n_{11}} }{n_{11}!} \frac{exp(-\mu_{12})\mu_{12}^{n_{1+} - n_{11}}}{(n_{1+} - n_{11})!} \frac{exp(-\mu_{21})\mu_{21}^{n_{+1} - n_{11}}}{(n_{+1} - n_{11})!} \frac{exp(-\mu_{22})\mu_{22}^{n- n_{1+} - n_{+1} + n_{11}}}{(n- n_{1+} - n_{+1} + n_{11})!}\\
	&= \frac{exp(-\sum_{i=1}^2 \sum_{j=1}^2 \mu_{ij}) \left( \frac{\mu_{11} \mu_{22}}{\mu_{12} \mu_{21}}\right) ^{n_{11}} \left(\frac{\mu_{12}}{\mu_{22}} \right)^{n_{1+}} \left(\frac{\mu_{21}}{\mu_{22}} \right)^{n_{+1}} \mu_{22}^{n}} {n_{11}! (n_{1+} - n_{11})! (n_{+1} - n_{11})! (n- n_{1+} - n_{+1} + n_{11})!}\\
	p(n_{1+}, n_{+1} n) &= \sum_{ \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}} \frac{exp(-\sum_{i=1}^2 \sum_{j=1}^2 \mu_{ij}) \left( \frac{\mu_{11} \mu_{22}}{\mu_{12} \mu_{21}}\right) ^{n_{11}} \left(\frac{\mu_{12}}{\mu_{22}} \right)^{n_{1+}} \left(\frac{\mu_{21}}{\mu_{22}} \right)^{n_{+1}} \mu_{22}^{n}} {n_{11}! (n_{1+} - n_{11})! (n_{+1} - n_{11})! (n- n_{1+} - n_{+1} + n_{11})!}
\end{align*}
So we can have 
\begin{align*}
	p(n_{11}|n_{1+}, n_{+1} n) &= \frac{p(n_{11}, n_{1+}, n_{+1} n)}{p(n_{1+}, n_{+1} n)}\\
	 &= \frac{exp(-\sum_{i=1}^2 \sum_{j=1}^2 \mu_{ij}) \left( \frac{\mu_{11} \mu_{22}}{\mu_{12} \mu_{21}}\right) ^{n_{11}} \left(\frac{\mu_{12}}{\mu_{22}} \right)^{n_{1+}} \left(\frac{\mu_{21}}{\mu_{22}} \right)^{n_{+1}} \mu_{22}^{n}} {n_{11}! (n_{1+} - n_{11})! (n_{+1} - n_{11})! (n- n_{1+} - n_{+1} + n_{11})!} \\
	 & \Bigg{/} \sum_{x \in \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}} \frac{exp(-\sum_{i=1}^2 \sum_{j=1}^2 \mu_{ij}) \left( \frac{\mu_{11} \mu_{22}}{\mu_{12} \mu_{21}}\right) ^{x} \left(\frac{\mu_{12}}{\mu_{22}} \right)^{n_{1+}} \left(\frac{\mu_{21}}{\mu_{22}} \right)^{n_{+1}} \mu_{22}^{n}} {x! (n_{1+} - x)! (n_{+1} - x)! (n- n_{1+} - n_{+1} + x)!}
\end{align*}	
Which we can rewrite 
\begin{align*}
	p(n_{11}|n_{1+}, n_{+1} n) &= {n_{1+} \choose n_{11}} {n - n_{1+} \choose n_{+1}-n_{11}} \left( \frac{\pi_{11} \pi_{22}}{\pi_{12} \pi_{21}} \right)^{n_{11}}\\
	& \Bigg{/}  \sum_{x \in \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}} {n_{1+} \choose x} {n - n_{1+} \choose n_{+1}-x} \left( \frac{\pi_{11} \pi_{22}}{\pi_{12} \pi_{21}}\right) ^x
\end{align*}

	Dervie the hypergeometric distribution:
	
	For a fixed sample size n, the joint distribution of the cell counts in the $2 \times 2$ table is given by
\begin{align*}
	p &=  \frac{n!}{n_{11}! n_{12}! n_{21}! n_{22}!} \pi_{11}^{n_{11}} \pi_{12}^{n_{12}} \pi_{21}^{n_{21}} \pi_{22}^{n_{22}}\\
	\psi &= \frac{\pi_{11} \pi_{22}}{\pi_{12}!\pi_{21}!} 
\end{align*}	
Let $\psi$ be the parameter of interest, and $\pi_{21}, \pi_{12}$ are the nuisance parameters. By looking at the sufficient statistics of $\pi_{12}, \pi_{21}$, which is $n_{12} = n_{1+} - n_{11} , n_{21} = n_{+1} - n_{11}$. We have a distribution of $n_{11}$ which is the parameter of interest. 

There are two ways to get the distribution of conditional probability, one is directly use the conditional probability definition, while the other is to use the conditional log-likelihood formula. Which way should we go will depend on the situation. 

If it is easier to get the log-likelihood, then go with the log-likelihood function. But for hypergeometric distribution, it is easier to just use definition as the binomial coefficient is not easy to deal with in log form.

Method 2: Use multinomial distribution definition. When we fixed $n_{1+}, n_{+1}$ which is equal to fix $n_{12}, n_{21}$
\begin{align*}
	p(n_{11}, n_{1.}, n_{.1}| n) &= \frac{n!}{n_{11}!n_{12}!n_{21}!n_{22}!} \psi^{n_{11}} \pi_{12}^{n_{1+}} \pi_{21}^{n_{+1}} \pi_{22}^{n-n_{1+}-n_{+1}}
	 \\
   p(n_{11}| n_{1.}, n_{.1}, n)	&=   \frac{p(n_{11}, n_{1.}, n_{.1}| n)}{p( n_{1.}, n_{.1}| n)}\\
   &= \frac{n! n_{1.}! (n-n_{1.})!}{n_{1.}! (n-n_{1.})! n_{11}!n_{12}!n_{21}!n_{22}!} \\
	&= {n \choose n_{1.}} {n_{1.} \choose n_{11}} {n-n_{1.} \choose n_{.1}-n_{11}} 
\end{align*}
The marginal distribution of $p( n_{1.}, n_{.1}| n)$
\begin{align*}
	p( n_{1.}, n_{.1}| n) &= \sum_{N_{11} \in \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}} \frac{n!}{n_{11}!n_{12}!n_{21}!n_{22}!} \psi^{n_{11}} \pi_{12}^{n_{1+}} \pi_{21}^{n_{+1}} \pi_{22}^{n-n_{1+}-n_{+1}}
	\\
\end{align*}
We don't change $n_{11}$ in this formula in order to construct the hypergeometric coefficients in the conditional probability. Most of the terms could be canceled and left $n_{11}$ 

\begin{align*}
	p(n_{11}| n_{1.}, n_{.1}, n)	&=   \frac{p(n_{11}, n_{1.}, n_{.1}| n)}{p( n_{1.}, n_{.1}| n)}\\
	&= \frac{n!}{n_{11}!n_{12}!n_{21}!n_{22}!} \psi^{n_{11}} \pi_{12}^{n_{1+}} \pi_{21}^{n_{+1}} \pi_{22}^{n-n_{1+}-n_{+1}}
	\\
	& \Bigg{/} \sum_{x \in \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}} \frac{n!}{n_{11}!n_{12}!n_{21}!n_{22}!} \psi^{x} \pi_{12}^{n_{1+}} \pi_{21}^{n_{+1}} \pi_{22}^{n-n_{1+}-n_{+1}}\\
	&= {n_{1.} \choose n_{11}} {n-n_{1.} \choose n_{.1}-n_{11}} \psi^{n_{11}} \Bigg{/} P_0(\psi) \\
	P_0(\psi) &= \sum_{x \in \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}} {n_{1.} \choose x} {n-n_{1.} \choose n_{.1}-x} \psi^{x}
\end{align*}


The $n_{1+}, n_{+1}$ could be considered as the K from above PDF, that we already know the total row and column, and see what the probability is in each draw. 


\subsection{HG distribution exponential family}
Although the hypergeometric distribution looks ugly, the characteristics are the same as other distribution. Here need to be aware that, the $\psi$ is the random variable, while $n_{11}$ is y.

\subsubsection{Exponential family}
\begin{align*}
	P(n_{11}| n_{1+}, n_{+1}, n, \psi) &= exp\{ n_{11} log \psi - log P_0(\psi) + const \} \\
	\theta &= log \psi, \qquad \phi = 1, \qquad b(\theta) = log P_0(\psi) = log P_0(exp(\theta))
\end{align*}

\subsubsection{M(t), K(t)}
\begin{align*}
	M(t) &= E[exp(ty)] = \int_{ \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}} exp(ty)exp\{ y log \psi - log P_0(\psi) + c \} dy\\
	&= \int_{ \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}} exp\{ y (\theta + t) - log P_0(exp(\theta)) + c \} dy\\
	&=\int_{ \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}}\\
	& exp\{ y (\theta + t) - log P_0(exp(\theta + t)) + log P_0(exp(\theta + t)) - log P_0(exp(\theta))+ c \} dy \\
	M(t) &=exp\{\phi b(\theta + t/\phi) - b(\theta)\}= exp(b(\theta + t) - b(\theta))\\
	&= P_0(exp(\theta + t))/ P_0(exp(\theta))\\
	M(t) &= P_0(exp(t)exp(\theta)) /P_0(exp(\theta)) =  P_0(exp(t)\psi) /P_0(\psi)
\end{align*}
The cumulant moment generating function
\begin{align*}
	K(t) &= log M(t) = log P_0(exp(t)\psi) - log P_0(\psi)
\end{align*}

\subsubsection{$\mu, \sigma^2$}

\begin{align*}
	\mu &= \partial_t K(t) = \frac{P_0(exp(t)\psi)'}{P_0(exp(t)\psi)} \Bigg{|}_{t=0} = \frac{P_1(\psi)}{P_0(\psi)}\\
	\sigma^2 &= \diffp{{\partial_t K(t)}}{t} \Bigg{|}_{t=0}
	 = \frac{P_2(\psi)}{P_0(\psi)} - \mu_i^2 \\
	P_{j}(\psi) &= \int_{x \in \max{(0, -n + n_{1+} + n_{+1})}}^{\min{(n_{1+}, n_{+1})}} {n_{1+} \choose x}{n-n_{1+} \choose n_{+1}-x} \psi^x x^j
\end{align*}

\subsubsection{Conditional MLE}
The conditional maximum likelihood estiamte (CMLE) of $\psi$ is not calculated directly from the conditional distribution of $\psi$. While we get it from $p(n_{11}| n_{1+}, n_{+1}, n, \psi)$.

We should be able to get the MLE from the log-likelihood conditional.

$\hat{\psi}_c$ is the solution to 
\begin{align*}
	n_{11} &= P_1(\hat{\psi}_c)/P_0(\hat{\psi}_c) = \mu\\
	log P(n_{11}| n_{1+}, n_{+1}, n, \psi) &=  n_{11} log \psi - log P_0(\psi) + c\\
	\diffp{log P}{\psi} &= \frac{n_{11}}{\psi} -\frac{P_0(\psi)'}{P_0(\psi)} = 0\\
	n_{11} &= P_1(\hat{\psi}_c)/P_0(\hat{\psi}_c)
\end{align*}
The variance of $\hat{\psi}_c$ can be approximated by the inverse of the Fisher information matrix $I_n(\hat{\psi}_c)$, which is given
\begin{align*}
	I_n(\hat{\psi}_c) &= E\{[ \partial_{\psi} log P(n_{11}| n_{1+}, n_{+1}, n, \hat{\psi}_c)]^2 \} = \frac{Var(n_{11}| n_{1+}, n_{+1}, n, \hat{\psi}_c)}{\hat{\psi}_c^2}
\end{align*}

\item[(e)] Let $\pi_{ij}$ denote the cell probability and assume n is fixed. Consider testing $H_0: \pi_{ij} = \pi_{i+} \pi_{+j}, i=1,..I, j=1,..J$. Derive the MLE of $\pi_{ij}$ under $H_0$.

The $H_0$ could be written as 
\begin{align*}
	H_0 &: \pi_{ij} = \pi_{i+} \pi_{+j}
\end{align*}

The multinomial distribution of $\pi_{ij}$
\begin{align*}
	p(\pi_{ij}) &= {n \choose n_{11} n_{12} n_{21} n_{22}} \pi_{ij}^{n_{ij}} , \sum_{i=1}^I \sum_{j=1}^J \pi_{ij} = 1
\end{align*}
The log-likelihood function
\begin{align*}
	log p(\pi_{ij}) &= log {n \choose n_{11} n_{12} n_{21} n_{22}} +  n_{ij} log \pi_{ij} , \sum_{i=1}^I \sum_{j=1}^J \pi_{ij} = 1
\end{align*}
Under $H_0$, the log-likelihood
\begin{align*}
	log p(\pi_{ij}) &= log {n \choose n_{11} n_{12} n_{21} n_{22}} +  n_{ij} log \pi_{i+} \pi_{+j} , \sum_{i=1}^I \pi_{i+} = 1, \sum_{j=1}^J \pi_{+j} = 1 
\end{align*}
By Lagrangian multiplier theorem,
\begin{align*}
	ln(\pi_{ij}) &=n log {n \choose n_{11} n_{12} n_{21} n_{22}} +\sum_{i=1}^I \sum_{j=1}^J n_{ij} log \pi_{i+} \pi_{+j} + \lambda ( \sum_{i=1}^I \sum_{j=1}^J \pi_{ij} - 1),\\
	&= n log {n \choose n_{11} n_{12} n_{21} n_{22}} +\sum_{i=1}^I \sum_{j=1}^J n_{ij} log \pi_{i+} + \sum_{j=1}^J \sum_{i=1}^I n_{ij} log \pi_{+j} - \lambda ( \sum_{i=1}^I \pi_{i+} - 1)
\end{align*}
Take first derivative of log-likelihood
\begin{align*}
	\diffp{ln}{{\pi_{i+}}} &= \frac{\sum_{j=1}^J n_{ij}}{\pi_{i+}} + \lambda = 0 \\
	\hat{\pi}_{i+} &= \frac{\sum_{j=1}^J n_{ij}}{\lambda}\\
	\sum_{i=1}^I \pi_{i+} &= 1, \qquad \lambda = \sum_{j=1}^J \sum_{i=1}^I n_{ij}\\
	\hat{\pi}_{i+} &= \frac{n_{i+}}{n}
\end{align*}
Similarly, we have $\hat{\pi}_{+j} = \frac{n_{+j}}{n}$, the MLE of $\pi_{ij}$ under $H_0$ is 
\begin{align*}
	\hat{\pi}_{ij} &= \hat{\pi}_{i+} \hat{\pi}_{+j} = \frac{n_{i+} n_{+j}}{n^2}
\end{align*}

\item[(f)] Derive the likelihood ratio test for the hypothesis in part (e) and derive its asymptotic distribution under $H_0$.
From part (e), we have the parameter estimates under $H_0$. While under alternative hypothesis, we have $\mu_{ij} = n_{ij}$. 
\begin{align*}
	LRT_n &= 2(LR(\pi_{H_1}) - LR(\pi_{H_0})) =2\left( \sum_{i=1}^I \sum_{j=1}^J n_{ij} log \pi_{ij} - \sum_{i=1}^I \sum_{j=1}^J n_{ij} log \pi_{i+} \pi_{+j} \right)\\
	&= 2\left( \sum_{i=1}^I \sum_{j=1}^J n_{ij} log \frac{\pi_{ij}}{\pi_{i+} \pi_{+j} }   \right)\\
	&= 2\left( \sum_{i=1}^I \sum_{j=1}^J n_{ij} log \frac{n_{ij} n}{n_{i+} n_{+j} }   \right) \sim \chi^2_{(I-1)(J-1)} 
\end{align*}
Note that the full model has $(IJ-1)$ parameters, and the null hypothesis has $(I-1)+ (J-1)$ parameters.
\begin{align*}
	df &= I \times J-1 - (I-1) - (J-1)\\
	&= (I-1)(J-1)
\end{align*}

\item[(g)] Suppose that $\pi_{11}, \pi_{12}$ are parameters of interest and the rest of the parameters are treated as nuisance. Derive the conditional likelihood of $(\pi_{11}, \pi_{12})$ and the conditional MLE's of  $(\pi_{11}, \pi_{12})$.
If not specified, we treat as general contingency table that total n is fixed. If only $\pi_{11}, \pi_{12}$ are parameters of interest and the rest of the parameters are treated as nuisance, then we will set the rest of the parameters as one parameter, and get its distribution, which is to find the sufficient statistics for rest of the parameters.
Write the Multinomial distribution in exponential family distribution.\\
We can find marginal distribution by summing over along all possible values of $(n_{11}, n_{12})$. Note that $n_{11} \leq \min{n_{1+} - n_{12}, n_{+1}}$ for a given value of $n_{12}$. Similarly, $n_{12} \leq \min{n_{1+}- n_{11}, n_{+1}}$ for a given value of $n_{11}$. \\
Additionally,
\begin{align*}
	n & \geq n_{1+} + n_{+1} + n_{+2} - n_{11} - n_{12} \\
	n_{11} + n_{12} & \geq \max{ 0, n_{+1} + n_{1+} + n_{+2}}
\end{align*}
Let
\begin{align*}
	S(n_{11}, n_{12}) &= \{(n_{11}, n_{12}): n_{11} + n_{12} \geq \max{ 0, n_{+1} + n_{1+} + n_{+2}},\\
	&  n_{11} \leq \min{(n_{1+} - n_{12}, n_{+1})}, n_{12} \leq \min{(n_{1+}- n_{11}, n_{+1})}   \} 
\end{align*}

The conditional distribution
\begin{align*}
	p(n_{11}, n_{12}|n_{13}, ...n_{IJ}, n) &= \frac{p(n_{ij}}{p(S_n)}\\
	&= \frac{\frac{1}{n_{11}! n_{12}! } \pi_{11}^{n_{11}} \pi_{12}^{n_{12}}}{\sum_{(x, y \in S_n)} \frac{1}{x! y!} \pi_{11}^x \pi_{12}^y}
\end{align*}
And $\hat{\pi}_{11}, \hat{\pi}_{12}$ are the CMLE that maximize $p(n_{11}, n_{12}|n_{13}, ...n_{IJ}, n)$.

\end{itemize}




\end{document}