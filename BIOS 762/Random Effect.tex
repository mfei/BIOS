% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[thinc]{esdiff}
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Random Effect Model}
\author{Mingwei Fei}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{Bayesian Statistics - Random Effect}
When we don't consider a fixed effect, then the random variable is a distribution. For a distribution, there is a probability function that we need to multiply. 

The Bayesian statistics is also a conditional distribution, based on the data (marginal distribution). So either we integral out the random variables, or use the conjugate distribution probability to get the marginal distribution.


\section{Expectation and Variance for marginal distribution}

There are many questions of calculating expectation and covariance of marginal distribution when we only know the conditional distribution. If only ask for calculating expectation and variance, we can do that without getting the marginal distribution

\subsection{Example 1}
Suppose that conditional on the random variables $b_i, y_i \sim N(b_i + x'\beta, 1), i=1,..n$, where $x_i$ is a $p \times 1$ vector of covariates for the ith subject, and $\beta$ is a $p \times 1$ vector of regression coefficients. Further assume the $b_i$ are i.i.d $N(0, \tau^2)$ random variables, and $\beta, \tau^2$ are unknown. Let $y= (y_1, y_2, .. y_n)'$, derive the marginal distribution of $y, E(y), Cov(y)$. 






\subsection{Example 2}
Suppose that $y_i| \mu_i, i=1,..n, j=1,2$, follows the model. $\mu_i$ random variable and independent identical distributed. Let $z_i = exp(\mu_i)$ for all i and define $\gamma$ to be the coefficient of variation of $z_i$, that is

\begin{align*}
	y_{ij}|\mu_i & \sim Poisson (\lambda_{ij})\\
    log (\lambda_{ij}) &= x'_{ij} \beta + \mu_i\\
    \gamma &= \frac{\sqrt{Var(z_i)}}{E(z_i)}
\end{align*}

Show that, marginally, $Var(y_{ij}) = \mu_{ij} (1+ \gamma^2 \mu_{ij})$ and $Cov(y_{ij}, y_{ik}) = \gamma^2 \mu_{ij}\mu_{ik}$ for $k \neq k$, where $\mu_{ij} = E(y_{ij})$.

We need to recognize that this is the contingency table $I \times 2$, and we use the Poisson distribution for each cell counts.

\begin{align*}
    Var(y_{ij}) &= Var[E(y_{ij}|\mu_i)] + E[Var(y_{ij}|\mu_{i})], \qquad E(y_{ij}|\mu_i) = \lambda_{ij}, Var(y_{ij}|\mu_i) = \lambda_{ij}\\
    &= Var(\lambda_{ij}) +E(\lambda_{ij}), \qquad \lambda_{ij} = exp(x^T_{ij} \beta + \mu_i) = exp(\mu_i) exp(x^T_{ij} \beta)\\
    &= Var( exp(\mu_i) exp(x^T_{ij} \beta)) + E( exp(\mu_i) exp(x^T_{ij} \beta)) \\
   E( exp(\mu_i) exp(x^T_{ij} \beta)) &= exp(x^T_{ij} \beta) E(exp(\mu_i))\\
   Var( exp(\mu_i) exp(x^T_{ij} \beta)) &= exp(2 x^T_{ij}\beta) Var(exp(\mu_i))\\
\end{align*}
While
\begin{align*}
	E(y_{ij}) &= E[E(y_i|\mu_i)] = \mu_{ij} = exp(x^T_{ij} \beta) E(exp(\mu_i))\\
	Var(y_{ij}) &= \mu_{ij} + exp(2 x^T_{ij}\beta) \frac{Var(exp(\mu_i))}{E(exp(\mu_i))^2} E(exp(\mu_i))^2\\
	&= \mu_{ij} + \mu_{ij}^2 \gamma^2 = \mu_{ij} (1+ \mu_{ij}\gamma^2)
\end{align*}

The covariance $Cov(y_{ij}, y_{ik})$
\begin{align*}
	Cov(y_{ij}, y_{ik}) &= E[y_{ij} y_{ik}] - E[y_{ij}] E[y_{ik}]\\
	&= E[E[(y_{ij}|\mu_i) (y_{ik}|\mu_k)]] - \mu_{ij} \mu_{ik} \\
	& =E[\lambda_{ij} \lambda_{ik}] - \mu_{ij} \mu_{ik} \\
	&= E[exp(x_{ij}^T \beta + x_{ik}^T \beta + 2\mu_i) ] - \mu_{ij} \mu_{ik} \\
	&= exp(x_{ij}^T \beta + x_{ik}^T \beta) E(exp(2 \mu_i)) - \mu_{ij} \mu_{ik}\\
	&= exp(x_{ij}^T \beta + x_{ik}^T \beta) (Var(exp(\mu_i)) + E(exp(\mu_i))^2 ) - \mu_{ij} \mu_{ik}\\
	&= exp(x_{ij}^T \beta + x_{ik}^T \beta) (E(exp(\mu_i))^2 \gamma^2) + \mu_{ij}\mu_{ik} - \mu_{ij} \mu_{ik}\\
	&= \mu_{ij}\mu_{ik} \gamma^2
\end{align*}

The sample mean of n i.i.d. chi-squared variables of degree k is distributed according to a gamma distribution with shape $\alpha$  and scale $\theta$  parameters:
\begin{align*}
	\bar{X} &= \frac{1}{n} \sum_{i=1}^n X_i \sim Gamma(\alpha = nk/2, \theta = 2/n)
\end{align*}

\section{Conditional distribution}
Suppose that $y_i \sim N(\mu_i, \sigma^2)$, and independent for $i=1,..n$, where $\mu_i$ and $\sigma^2$ are unknown.

	
\subsection{$\sigma^2$ as nuisance parameter}
 Suppose that $\mu_i = x_i^T \beta$, where $x_i$ is a $p \times 1$ vector of covariates for the ith subject, $i=1,..n$. Suppose $\beta$ is the vector parameter of interest and $\sigma^2$ is nuisance parameter. Derive the conditional likelihood of $\beta$ and a closed form estimate of the conditional MLE of $\beta$, and compare the result to the MLE of $\beta$ of full likelihood.
	
	
\begin{align*}
	p{(Y, \mu_i, \sigma^2)} &= \left( \frac{1}{\sqrt{2\pi}} \right)^n \left( \frac{1}{\sigma}\right)^n exp\{ \frac{(Y-\mu)^T (Y-\mu)}{2 \sigma^2} \}
\end{align*}	
	
We would like to construct the conditional distribution based on $\sigma^2$, so the key is to get $\sigma^2$ distribution.

Method 1: We know the chi-square distribution and the variance. First, we can use $M= X(X^T X)^{-1}X^T$ to get the $\beta$.
\begin{align*}
	X\beta &= M Y \\
	\hat{\beta} &= (X^T X)^{-1}X^TY
\end{align*}	
Consider $\sigma^2$ as nuisance parameter, and the sufficient statistics for $\sigma^2$ is 
$S_n = Y^T(I-M)Y$. Let  
\begin{align*}
	S_n &= \sigma^2 z, \quad z = \frac{Y^T(I-M)Y}{\sigma^2} \\
	Z &= \frac{Y^T (I-M)Y}{\sigma^2 I}  \sim \chi^2_{n-p}
\end{align*}
So we have distribution for $\sigma^2$

\begin{align*}
	p(z) &= \frac{1}{2^{k/2} \Gamma{k/2}} z^{k/2-1} exp(-\frac{z}{2})  \\
	S_n &= \sigma^2 z, \qquad \diffp{z}{{S_n}} = \sigma^{-2} \\
	p(S_n) &= \sigma^{-2} \frac{1}{2^{k/2} \Gamma{(k/2)} } \left(\frac{S_n}{\sigma^2}\right) ^{k/2-1} exp(-\frac{S_n}{2 \sigma^2 })
\end{align*}
The conditional log-likelihood of $\beta$ 	
\begin{align*}
	l_c &= log p(y) - log p(S_n) = - n log \sqrt{2\pi} -n log \sigma I - \frac{(Y- X \beta)^T (Y- X\beta)}{2 \sigma^2} \\
	& - log \sigma I + ((n-p)/2-1) log \frac{S_n}{\sigma^2} - log 2^{(n-p)/2} \Gamma{((n-p)/2)} -  \frac{S_n}{2 \sigma^2 }
\end{align*}
Pay attention that $S_n$ does not depend on $\beta$, so the conditional distribution are S-independent of $\sigma^2$.

Taking first derivative, 
\begin{align*}
	\diffp{l_c}{\beta} &=- X^T(Y- X\beta) - (Y- X\beta)^T X = 0\\
	\hat{\beta} &= (X^TX)^{-1}X^T Y
\end{align*}
Thus the MLE of conditional distribution of $\beta$ is equal to the MLE of $\beta$ of the full likelihood.

\subsubsection{Conditional likelihood}
Method 2: Use the sufficient statistics for $\sigma^2$, which is $ (Y-\mu)^T (Y-\mu) = \sum_{i=1}^n (y_i- \mu_i)^2 $. Fix $\beta$ at $\beta_0$, and let $\mu_0 = X \beta_0$, then we have the non-central $\chi^2$ distribution with $r = n-p$ degrees of freedom  

\begin{align*}
	log p(S(\mu_0)) &= \frac{1}{2\sigma^2} S_n(\mu_0) - \frac{\delta}{2\sigma^2} + (0.5n -1) log S_n(\mu_0) \\
	& +	log \sum_{r=0}^{\infty} \frac{\delta}{2 \sigma^2}^r \frac{r^r}{r!} B(0.5(n-1), r + 0.5)
\end{align*}

The method is the same as above, we also need to get the distribution of $S_n$, just here we assume a fixed $beta_0$ instead of the LSE of $\beta$.


Then we also have log-likelihood, the formula is the same no matter what the distribution of $S_n$.

\begin{align*}
	l_c &= log p(y) - log p(S(\mu_0)) = 
\end{align*}

\subsubsection{Conditional score function}
The conditional score statistic for $\beta$ is 
\begin{align*}
	U_{\beta}(\beta, \sigma^2| \beta_0) &= D^T \diffp{l_c(\mu, \sigma^2| \mu_0)}{\mu} \Bigg|_{\beta_0= \beta} = \frac{1}{\sigma^2} (Y-X\beta)\\
	D & = X\\
	U_{\beta}(\beta, \sigma^2 | \beta_0) &= X^T (Y- X\beta) = 0\\
	\hat{\beta}_c &= (X^T X)^{-1} X^T Y
\end{align*}

\subsection{$\beta$ as nuisance parameter}
 Suppose $\beta$ is nuisance parameter, and $\sigma^2$ is the parameter of interest. Derive the conditional distribution and calculate conditional MLE.

I used to think all $\mu_i$ are equal, so I have $\sum_{i=1}^n y_i \sim N(n\mu, \sigma^2/n)$. This is wrong as it treated as all the $y_i$ comes from one distribution $N(\mu, \sigma^2)$ which is not correct.

Here I have to replace $\mu = x^T \beta$ as the derivative is about $\sigma$, not $\beta$.  $S_n = \sum_{i=1}^n y_i x_i = X^T Y$ is the sufficient statistics for $X \beta$, then 
\begin{align*}
	E(S_n) &= X^T X \beta, \qquad Var(S_n) = \sigma^2 X^T X\\
	S_n & \sim N(X^T X \beta, \sigma^2 X^T X) \\
	p(S_n) &= \frac{1}{\sqrt{2\pi}} \frac{n}{\sigma} exp\{\frac{-(S_n - X^T X \beta)^T (S_n - X^T X \beta)}{2 \sigma^2} \} \\
	l_c &= log p(y) - log p(S(\sigma_0^2))\\
	& =-\frac{n}{2} log(\sigma^2) - \frac{(Y-X \beta)^T(Y- X \beta)}{2 \sigma^2}\\
	& + \frac{p}{2} log(\sigma^2) + \frac{(X^T Y - X^T X \beta)^T (X^T X)^{-1}(X^T Y - X^T X \beta)}{2 \sigma^2}
\end{align*}
When write in matrix form, it would be easier to find the normal distribution of $X^TY$.


Then take the first derivative of $\sigma^2$
\begin{align*}
	\diffp{l_c}{{\sigma^2}} &= -\frac{n}{2 \sigma^2} + \frac{1}{2 (\sigma^2)^2} (Y-X \beta)^T(Y- X \beta)\\
	& + \frac{p}{2 \sigma^2} - \frac{1}{2 (\sigma^2)^2} (X^T Y - X^T X \beta)^T (X^T X)^{-1}(X^T Y - X^T X \beta) = 0
\end{align*}
We could get the MLE of $\beta$ from $p(S_n)$
\begin{align*}
	\diffp{{l_{S_n})}}{{\beta}} &=  \frac{1}{2 (\sigma^2)} 2 (X^T Y - X^T X \beta) = 0\\
	\hat{\beta} &= (X^T X)^{-1} X^ Y
\end{align*}
Then 
\begin{align*}
	\diffp{l_c}{{\sigma^2}} &= -\frac{n-p}{2 \sigma^2} + \frac{1}{2 (\sigma^2)^2} (Y-X \beta)^T(Y- X \beta) - \frac{1}{2 (\sigma^2)^2} (X^T Y - X^T X \beta)^T (X^T X)^{-1}(X^T Y - X^T X \beta) = 0\\
	\sigma_c^2 &= \frac{(Y- X \beta)^T (Y- X \beta)}{n-p} = \frac{Y^T(I-M)Y}{n-p}
\end{align*}
The MLE of $\sigma^2$ from the full data likelihood is 
\begin{align*}
	\hat{\sigma}^2 &= \frac{Y^T(I-M)Y}{n} \leq \hat{\sigma}_c^2
\end{align*}


\subsubsection{$\mu_i = exp(X^T\beta)$}
When the link function changes, how do we find the distribution of sufficient statistics of $\beta$?

\begin{align*}
	p{(Y, \mu_i, \sigma^2)} &= \left( \frac{1}{\sqrt{2\pi}} \right)^n \left( \frac{1}{\sigma^2}\right)^{n/2} exp\{ \frac{(Y- exp(X^T \beta))^T (Y- exp(X^T \beta))}{2 \sigma^2} \}\\
	&= \left( \frac{1}{\sqrt{2\pi}} \right)^n \left( \frac{1}{\sigma^2}\right)^{n/2} exp\{ \frac{(Y^T Y - 2 exp(X^T \beta)^T Y - exp(X^T \beta)^T exp(X^T \beta)}{2 \sigma^2} \}
\end{align*}
So the sufficient statistics for $\beta$ is $\frac{Y}{\sigma^2}$, we need to assume a fixed $\sigma_0^2$
\begin{align*}
	E[Y] &= \frac{exp(X^T \beta)}{\sigma_0^2}, \qquad Var[Y] = \frac{\sigma^2}{(\sigma_0^2)^2} \\
	S_n(\sigma_0^2) & \sim N(\frac{exp(X^T \beta)}{\sigma_0^2}, \frac{\sigma^2}{(\sigma_0^2)^2} )
\end{align*}
The conditional distribution
\begin{align*}
	log p(S_n) &=-log {\sqrt{2\pi}} - log{\sqrt{\frac{\sigma^2}{(\sigma_0^2)^2}}} exp\{\frac{-(S_n - \frac{exp(X^T \beta)}{\sigma_0^2})^T (S_n - \frac{exp(X^T \beta)}{\sigma_0^2})}{2 \frac{\sigma^2}{(\sigma_0^2)^2}} \} \\
	l_c &= log p(y) - log p(S(\sigma_0^2))\\
	&= -\frac{n}{2} log(\sigma^2) - \frac{(Y- exp(X \beta))^T(Y- exp(X \beta))}{2 \sigma^2} \\
	& + \frac{1}{2} log{\frac{\sigma^2}{(\sigma_0^2)^2}} + \{\frac{(S_n - \frac{exp(X^T \beta)}{\sigma_0^2})^T (S_n - \frac{exp(X^T \beta)}{\sigma_0^2})}{2 \frac{\sigma^2}{(\sigma_0^2)^2}} \} 
\end{align*}

\subsubsection{Score function}
The score function and Newton-Raphson algorithm to get the estimate of $\sigma^2$, however we first need to replace the $\beta$ with the MLE from $p(S_n(\sigma^2))$, then use the Newton-Raphson. 

\subsubsection{Hypothesis testing}


\section{Practice}
  




\subsection{b}Consider the following
\begin{itemize}
	\item[(a)] For an arbitrary model, consider the conditional score statistic
	\begin{align*}
		U_{\psi}(\xi) &= \frac{\partial l_c(\xi, \psi_0)}{\partial \psi} |_{\psi_0=\psi}
	\end{align*} 
	Show that the conditional score statistic for any model can be written as
	\begin{align*}
		U_{\psi}(\xi) &= \partial_{\psi} log p(Y|\xi)- E[\partial_{\psi} log p(Y|\xi)|s_{\lambda}(\psi_0)]|_{\psi_0=\psi}
	\end{align*} 
	The conditional score statistic is the derivative of the conditional distribution
	\begin{align*}
		U_{\psi}(\xi) &= \frac{\partial l_c(\xi, \psi_0)}{\partial \psi} |_{\psi_0=\psi}\\
		p(\textbf{Y}| \xi) &= p(\textbf{Y}|s_{\lambda}(\psi_0), \xi) p(s_{\lambda}(\psi_0) | \xi), \qquad p(\textbf{Y}|s_{\lambda}(\psi_0), \xi) = \frac{p(\textbf{Y}| \xi)}{p(s_{\lambda}(\psi_0) | \xi)} \\
		l_c(\xi, \psi_0) &= log p(\textbf{Y}|s_{\lambda}(\psi_0), \xi)= log p(\textbf{Y}| \xi) - log p(s_{\lambda}(\psi_0) | \xi)
	\end{align*}
	Then we need to prove 
	\begin{align*}
		U_{\psi}(\xi) &= \frac{\partial l_c(\xi, \psi_0)}{\partial \psi} |_{\psi_0=\psi} = \partial_{\psi} log p(\textbf{Y}| \xi) - \partial_{\psi} log p(s_{\lambda}(\psi_0) | \xi)\\
		\partial_{\psi} log p(s_{\lambda}(\psi_0) | \xi) &= E[\partial_{\psi} log p(Y|\xi)|s_{\lambda}(\psi_0)]|_{\psi_0=\psi}
	\end{align*}
	We can write
	\begin{align*}
		log p(\textbf{Y}| \xi) &= log  p(\textbf{Y}|s_{\lambda}(\psi_0), \xi) + log p(s_{\lambda}(\psi_0) | \xi)\\
		E \left( \partial_{\psi}[log p(\textbf{Y}| \xi)| s_{\lambda}]\right) &= E \left(\partial_{\psi}[log  p(\textbf{Y}|s_{\lambda}(\psi_0), \xi)|s_{\lambda}]\right) + E \left(\partial_{\psi}[log p(s_{\lambda}(\psi_0), \xi)|s_{\lambda}]\right)
	\end{align*}    
	in which, the integral and expectation can switch, then we have
	\begin{align*}
		E \left(\partial_{\psi}[log  p(\textbf{Y}|s_{\lambda}(\psi_0), \xi)|s_{\lambda}]\right) & = \partial_{\psi} E \left([log  p(\textbf{Y}|s_{\lambda}(\psi_0), \xi)|s_{\lambda}]\right) = \partial_{\psi} E \left([log  p(\textbf{Y}| \xi)]\right)= 0
	\end{align*}      
	So,
	\begin{align*}
		E \left( \partial_{\psi}[log p(\textbf{Y}| \xi)| s_{\lambda}]\right) &= \partial_{\psi}log p(s_{\lambda}(\psi_0),\xi)
	\end{align*}
	Then we show
	\begin{align*}
		U_{\psi}(\xi) &= \partial_{\psi} log p(Y|\xi)- E[\partial_{\psi} log p(Y|\xi)|s_{\lambda}(\psi_0)]|_{\psi_0=\psi}
	\end{align*} 
	\item[(b)] Suppose that $y_1;.. y_n$ are independent and $y_i$ follows a Poisson distribution with mean $exp(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2})$, where $(x_{i1}; x_{i2})$ are covariates, $\lambda = (\lambda_0; \lambda_1)$ is the
	nuisance parameter vector and $\psi$  is the parameter of interest. Derive the conditional
	likelihood of $\psi$   and show that this conditional likelihood is free of $\lambda$.\\
	The joint distribution of $(y_1, · · · , y_n)$ is given by 
	\begin{align*}
		P(Y|\lambda, \psi)&=  exp \left( \sum_{i=1}^n y_i(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - log y_i! \right)
	\end{align*}
	Thus, $S_0 = \sum_{i=1}^n y_i$ is the sufficient and complete statistics for $\lambda_0$, and $S_1 = \sum_{i=1}^n y_i x_{i1}$ is the sufficient and complete statistics for $\lambda_1$.\\
	The conditional distribution of $\psi$ given $S_0, S_1$ is given by
	\begin{align*}
		p(\textbf{Y}, \psi|S=(S_0, S_1)) &= \frac{exp \left( \sum_{i=1}^n y_i(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - log y_i! \right)}{\sum_{y' \in S} exp \left( \sum_{i=1}^n y'_i(\lambda_0 + \lambda_1 x_{i1} +  \psi x_{i2}) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1 x_{i1} +  \psi x_{i2}) - log y'_i! \right)}\\
		&= \frac{exp \left( S_1 \lambda_0 + S_2 \lambda_1 +  S_3 \psi) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1x_{i1} +  \psi x_{i2}) - log y_i! \right)}{\sum_{y' \in S} exp \left( S'_1\lambda_0 + S'_2 \lambda_1 + S'_3 \psi) - \sum_{i=1}^n exp(\lambda_0 + \lambda_1 x_{i1} +  \psi x_{i2}) - log y'_i!\right)} \\
		&= \frac{exp \left( S_3 \psi  - log y_i!\right)}{\sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right)}, \qquad S_3 = \sum_{i=1}^n y_i x_{i2}, S'_3 = \sum_{i=1}^n y'_i x_{i2}
	\end{align*}
	which is independent of $\lambda$. \\
	\item[(c)] Derive the conditional score statistic for part (b) and write out a Newton-Raphson algorithm for obtaining the conditional maximum likelihood estimate of $\psi$  based on $U_{\psi}(\xi)$.\\
	The log likelihood of the conditional distribution is
	\begin{align*}
		l_c(\psi) &= S_3 \psi  - log y_i! -log \left[ \sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right) \right], \qquad S_3 = \sum_{i=1}^n y_i x_{i2}, S'_3 = \sum_{i=1}^n y'_i x_{i2}
	\end{align*} 
	The score function and observed fisher information is
	\begin{align*}
		U_{\psi}(\xi) &= \frac{\partial l_c(\xi, \psi_0)}{\partial \psi} |_{\psi_0=\psi}\\
		&= \psi - \frac{\sum_{y' \in S} S'_3 exp \left( S'_3 \psi - log y'_i! \right)}{\sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right)}\\
		\frac{\partial^2 l_c(\xi, \psi_0)}{\partial \psi^2} &= \left[ \frac{\sum_{y' \in S} S'_3 exp \left( S'_3 \psi - log y'_i! \right)}{\sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right)}\right]^2 - \frac{\sum_{y' \in S} S'^2_3 exp \left( S'_3 \psi - log y'_i! \right)}{\sum_{y' \in S} exp \left( S'_3 \psi - log y'_i! \right)}
	\end{align*}
	The newton-Raphson algorithm
	\begin{align*}
		\psi^{k+1} &= \psi^{k} - \left[\frac{\partial^2 l_c(\psi^{k})}{\partial \psi^2} \right]^{-1} U_{\psi}(\psi^{k})
	\end{align*}
	where $\frac{\partial^2 l_c(\psi^{k})}{\partial \psi^2}, U_{\psi}(\psi^{k})$ are from above equations.
	
	\item[(d)] Now suppose that we only have two random variables $y_1 \sim Poisson(\mu_1)$ and $y_2 \sim
	Poisson(\mu_2)$, where $y_1$ and $y_2$ are independent. We are interested in making inferences on the ratio $\psi = \mu_1/\mu_2$. Let $\xi = (\psi , \lambda)$, where $\lambda$ represents the nuisance parameter.
	\begin{itemize}
		\item [(i)] Show that the log-likelihood function of $\xi$ can be written as
		\begin{align*}
			l(\xi) &= (y_1 + y_2)\lambda + y_1 log (\psi) - exp(\lambda) (1+\psi)
		\end{align*}
		where $\lambda$ is a function of $\mu_2$. Explicitly state what $\lambda$ is.\\
		Write the joint distribution of $y_1, y_2$
		\begin{align*}
			P(y_1, y_2) &= \frac{\mu_1^{y_1} e^{-\mu_1}}{y_1!} \frac{\mu_2^{y_2} e^{-\mu_2}}{y_2!} \\
			log P(y_1, y_2) &= y_1 log \mu_1 - \mu_1 + y_2 \log \mu_2 - \mu_2 - log y_1! - log y_2!\\
			&= y_1 log \frac{\mu_1}{\mu_2} + y_1 log \mu_2 + y_2 log \mu_2 -\mu_1 - \mu_2 -log y_1! - log y_2!\\
			&= y_1 log \frac{\mu_1}{\mu_2} + (y_1+y_2) log \mu_2 - \mu_2(\mu_1/\mu_2 + 1) -log y_1! - log y_2!
		\end{align*}
		where 
		\begin{align*}
			\psi &=log \frac{\mu_1}{\mu_2} \\
			\lambda &= log \mu_2
		\end{align*}
		\item[(ii)] Derive the conditional likelihood of $\psi$  and write out a Newton-Raphson algorithm for obtaining the conditional maximum likelihood estimate of $\psi$ .\\
		From part (a), we see $y_1 + y_2$ is the sufficient statistics for $\lambda$, while $y_1 + y_2 \sim Poission (\mu_1+\mu_2)$ then we have conditional distribution of $\psi$ condition on $S = y_1 + y_2$.
		\begin{align*}
			Y(\psi|S= y_1+y_2,\lambda) &= \frac{exp \left[ y_1 \psi + (y_1+y_2) \lambda - exp(\lambda)(\psi + 1) -log y_1! - log y_2! \right] }{exp \left[ (y_1+y_2) log (\mu_1+\mu_2) - (\mu_1+\mu_2) -log (y_1+y_2)!  \right]}\\
			&= \frac{exp \left[ y_1 \psi + S \lambda - exp(\lambda)(\psi + 1) -log y_1! - log y_2! \right] }{exp \left[ S (\lambda + log(\psi + 1)) -  exp(\lambda)(\psi + 1) -log S!  \right]}\\
			&= \frac{exp \left[ y_1 \psi -log y_1! - log y_2! \right] }{exp \left[ (y_1+ S-y_1) log(\psi + 1)) -log S!  \right]}\\
			&= {S \choose y_1} \left( \frac{\psi}{1+\psi}\right)^{y_1} \left(\frac{1}{1+\psi} \right)^{S-y_1}
		\end{align*}
		The conditional distribution is a binomial, $B(S, \psi/(1+\psi))$.\\
		The score function and observed fisher information 
		\begin{align*}
			log Y(\psi|S,\lambda) &= y_1 log \psi -S log(1+\psi) + log {S \choose y_1} \\
			\partial_{\psi} log Y(\psi|S,\lambda) &= \frac{y_1}{\psi} - \frac{S}{1+\psi} = 0, \qquad \hat{\psi} = y_1/(S-y_1)\\
			\partial^2_{\psi} log Y(\psi|S,\lambda) &= -\frac{y_1}{\psi^2} + \frac{S}{(1+\psi)^2}
		\end{align*}
		The $CMLE = \hat{\psi} = y_1/(S-y_1)$. And the newton-Raphson equation 
		\begin{align*}
			\psi^{k+1} &= \psi^{k} - \left[\frac{\partial^2 l_c(\psi^{k})}{\partial \psi^2} \right]^{-1} U_{\psi}(\psi^{k})\\
			&= \psi^{k} - \left[ -\frac{y_1}{\psi^2} + \frac{S}{(1+\psi)^2}\right]^{-1} \left[\frac{y_1}{\psi} - \frac{S}{1+\psi} \right]|_{\psi = \psi^{k}}\\
			&=  \psi^{k} + \frac{y_1/\psi^{k} - S/(1+\psi^{k})}{y_1/{\psi^{k}}^2 - S/(1+\psi^{k})^2}
		\end{align*}
	\end{itemize}
\end{itemize}




\end{document}
