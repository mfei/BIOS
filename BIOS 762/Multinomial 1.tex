% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[thinc]{esdiff}
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!
\title{BIOS762 - Notes}
\author{Mingwei Fei}

\begin{document}
	
	\maketitle


	
\subsection{Logistic Regression}
Consider independent observations $(X_1, Y_1),..., (X_n; Y_n)$ where $Y_i$ takes values 0 and
1. Suppose that $X_i|(Y_i = m) \sim N(\mu_m, \sigma^2)$ and $P(Y_i = m) = \pi_m$ for $m = 0, 1$, where
$\pi_0 + \pi_1 = 1$, and $0 < \pi_0 < 1$.
\begin{itemize}
	\item [(a)] Show that $P(Y_i = m|X_i), m = 0, 1$, satisfies the logistic model, that is
	\begin{align*}
		logit \left(P(Y_i = 1|X_i,\alpha) \right) = \alpha_0 + \alpha_1 X_i
	\end{align*}
	We have distribution of $P(Y_i = m|X_i), m = 0, 1$ 
	\begin{align*}
		P(Y_i=m |X_i,\alpha) & = \frac{P(Y_i, X_i)}{P(X_i)} = \frac{P(X_i|Y_i) P(Y_i)}{P(X_i)}\\
		P(Y_i=1 |X_i,\alpha) &= \frac{P(X_i|Y_i=1) P(Y_i=1)}{P(X_i)} \\
		&= \frac{exp(-1/2\sigma^2 (x_i-\mu_i)^2) \pi_1}{exp(-1/2\sigma^2 (x_i-\mu_i)^2) \pi_1 + exp(-1/2\sigma^2 (x_i-\mu_1)^2)  \pi_0}\\
		P(Y_i=0 |X_i,\alpha) &= \frac{P(X_i|Y_i=0) P(Y_i=0)}{P(X_i)}\\
		&= \frac{exp(-1/2\sigma^2 (x_i-\mu_1)^2)  \pi_0}{exp(-1/2\sigma^2 (x_i-\mu_i)^2) \pi_1 + exp(-1/2\sigma^2 (x_i-\mu_1)^2)  \pi_0}\\
		logit \left(P(Y_i = 1|X_i,\alpha) \right) &= log \frac{ P(Y_i=1 |X_i,\alpha)}{P(Y_i=0 |X_i,\alpha)} \\
		&= log(\pi_1/\pi_0) - \frac{(x_i-\mu_1)^2}{2\sigma^2} + \frac{(x_i-\mu_0)^2}{2\sigma^2}\\
		&= log(\pi_1/\pi_0) +\frac{\mu_0^2 - \mu_1^2}{2\sigma^2} + \frac{(\mu_1-\mu_0)}{\sigma^2}x_i\\
		\text{In which,}  \alpha &= (\alpha_0, \alpha_1) = \left(log(\pi_1/\pi_0) +\frac{\mu_0^2 - \mu_1^2}{2\sigma^2},  \frac{(\mu_1-\mu_0)}{\sigma^2} \right)^T
	\end{align*}     
	
	\item[(b)] Based on the logistic model in part (a), give the explicit form of the Newton-Raphson
	algorithm for calculating the maximum likelihood estimate of $\alpha$, denoted by $\hat{\alpha} =
	(\hat{\alpha_0}, \hat{\alpha_1})$, and derive the asymptotic covariance matrix of $\alpha$.\\
	$Y_i|X_i$ follows a binomial distribution
	\begin{align*}
		p(Y_i|\alpha) &= P(Y_i=1 |X_i,\alpha)^{I(y_i=1)} P(Y_i=0 |X_i,\alpha)^{I(y_i=0)}\\
		log p(Y_i|\alpha) &= I(y_i=1) log P(Y_i=1 |X_i,\alpha) + I(y_i=0) log P(Y_i=0 |X_i,\alpha)\\
		ln p(Y_i|\alpha) &= \sum_{i=1}^n I(y_i=1) log P(Y_i=1 |X_i,\alpha) + I(y_i=0) log P(Y_i=0 |X_i,\alpha)\\
		&= \sum_{i=1}^n I(y_i=1) log P(Y_i=1) + (1-I(y_i=1)) log (1-P(Y_i=1)) \\
		&= \sum_{i=1}^n I(y_i=1) log P(Y_i=1)/(1-P(Y_i=1)) + log (1-P(Y_i=1))\\
	\end{align*}
	Let $\theta = log P(Y_i=1)/(1-P(Y_i=1))$
	\begin{align*}
		ln p(Y_i|\theta) &= \sum_{i=1}^n I(y_i=1) \theta - log (1 + exp(\theta) )\\
		ln p(Y_i|\alpha) &= \sum_{i=1}^n y_i (\alpha_0 + \alpha_1 x_i) - log \left(1 + exp(\alpha_0 + \alpha_1 x_i) \right)
	\end{align*}    
	Find MLE for $\alpha$
	\begin{align*}
		\frac{\partial ln p(Y_i|\alpha)}{\partial \alpha_0}  &= \sum_{i=1}^n y_i - \left(1 + exp(\alpha_0 + \alpha_1 x_i) \right)^{-1} exp(\alpha_0 + \alpha_1 x_i)\\
		\frac{\partial ln p(Y_i|\alpha)}{\partial \alpha_1}  &= \sum_{i=1}^n y_i x_i- \left(1 + exp(\alpha_0 + \alpha_1 x_i) \right)^{-1} exp(\alpha_0 + \alpha_1 x_i) x_i\\
		\frac{\partial ln^2 p(Y_i|\alpha)}{\partial \alpha_0^2}  &=- \sum_{i=1}^n \frac{exp(\alpha_0 + \alpha_1 x_i)}{[1+ exp(\alpha_0 + \alpha_1 x_i)]^2}, \qquad E[-\frac{\partial ln^2 p(Y_i|\alpha)}{\partial \alpha_0^2}] = n \pi_1 (1-\pi_1) \\
		\frac{\partial ln^2 p(Y_i|\alpha)}{\partial \alpha_1^2}  &=- \sum_{i=1}^n \frac{exp(\alpha_0 + \alpha_1 x_i)}{[1+ exp(\alpha_0 + \alpha_1 x_i)]^2} x_ix_i^T \\
		\frac{\partial ln^2 p(Y_i|\alpha)}{\partial \alpha_0\alpha_1}  &=- \sum_{i=1}^n \frac{exp(\alpha_0 + \alpha_1 x_i)}{[1+ exp(\alpha_0 + \alpha_1 x_i)]^2} x_i\\
		I_n(\alpha) &= -E[ \frac{\partial ln^2 p(Y_i|\alpha)}{\partial \alpha^2}]\\
		&= \begin{bmatrix}
			n \pi_1 (1-\pi_1) &   \sum_{i=1}^n \pi_1 (1-\pi_1)x_i\\
			\sum_{i=1}^n \pi_1 (1-\pi_1)x_i &  \sum_{i=1}^n \pi_1 (1-\pi_1)x_ix_i^T\\
		\end{bmatrix}
	\end{align*}  
	So the N-R algorithm is 
	\begin{align*}
		\alpha_{k+1} = \alpha_{k} - I_n(\alpha_k)^{-1} \frac{\partial ln p(Y_i|\alpha_k)}{\partial \alpha_k}
	\end{align*}      
	The asymptotic distribution of $\alpha$ by CLT and covariance matrix 
	\begin{align*}
		\sqrt{n} (\hat{\alpha} - \alpha) & \xrightarrow[]{d} N \left(0, \Sigma \right) \\
		\Sigma & = \{ \frac{1}{n} I_n(\alpha) \}^{-1}
	\end{align*} 
	\item[(c)] Write down the joint distribution of $\{(X_i Y_i): i=1,2..n \}$ and calculate the
	maximum likelihood estimate of $\theta$, denoted by $\theta_F$ , and its asymptotic covariance
	matrix.\\
	The joint distribution of $\{(X_i Y_i): i=1,2..n \}$
	\begin{align*}
		p(X_i, Y_i) &= P(X_i|Y_i) P(Y_i) \\
		p(Y_i=1, X_i) &= \frac{1}{\sqrt{2\pi}\sigma} exp(-1/2\sigma^2 (x_i-\mu_1)^2) \pi_1\\
		p(Y_i=0, X_i) &=\frac{1}{\sqrt{2\pi}\sigma} exp(-1/2\sigma^2 (x_i-\mu_0)^2)  \pi_0\\
		p(X_i, Y_i) &= P(Y_i=1, X_i)^{I(y_i=1)} P(Y_i=0, X_i)^{I(y_i=0)}\\
		& = {\{\frac{1}{\sqrt{2\pi}\sigma} exp(-1/2\sigma^2 (x_i-\mu_1)^2) \pi_1\}}^{y_i} {\{\frac{1}{\sqrt{2\pi}\sigma} exp(-1/2\sigma^2 (x_i-\mu_0)^2)  \pi_0\}}^{1-y_i}\\
		log p(X_i, Y_i) &= log \frac{1}{\sqrt{2\pi}\sigma} + y_i log\pi_1 + (1-y_i) log(1-\pi_1) - \frac{(x_i-\mu_i)^2}{2\sigma^2} y_i - \frac{(x_i-\mu_0)^2}{2\sigma^2} (1-y_i)
	\end{align*} 
	The log-likelihood function of $\{(X_i Y_i): i=1,2..n \}$
	\begin{align*}
		log p(X, Y) &= nlog \frac{1}{\sqrt{2\pi}\sigma} + \sum_{i=1}^n y_i log\pi_1 + (1-y_i) log(1-\pi_1) - \frac{(x_i-\mu_1)^2}{2\sigma^2} y_i - \frac{(x_i-\mu_0)^2}{2\sigma^2} (1-y_i)
	\end{align*} 
	The MLE of $\theta$ could get by taking derivatives to log-likelihood function
	\begin{align*}
		\frac{\partial ln p(X, Y|\theta)}{\partial \pi_1}  &= \sum_{i=1}^n y_i/\pi_1 - (1-y_i)/(1-\pi_1)=0\\
		\frac{\partial ln p(X, Y|\theta)}{\partial \mu_1}  &= \sum_{i=1}^n \frac{y_i(x_i-\mu_1)}{\sigma^2}=0     \\
		\frac{\partial ln p(X, Y|\theta)}{\partial \mu_0}  &= \sum_{i=1}^n \frac{(1-y_i)(x_i-\mu_0)}{\sigma^2}=0\\
		\frac{\partial ln p(X, Y|\theta)}{\partial \sigma^2}  &= -\frac{n}{2}{1/\sigma^2} + \sum_{i=1}^n \frac{(x_i-\mu_1)^2y_i}{2\sigma^4} + \sum_{i=1}^n \frac{(x_i-\mu_0)^2(1-y_i)}{2\sigma^4} = 0\\
		\hat{\sigma^2} &= \frac{\sum_{i=1}^n [(x_i-\mu_1)^2y_i + (x_i-\mu_0)^2(1-y_i)]}{n}\\
		\hat\pi_1 &= \frac{\sum_{i=1}^n y_i}{n}, \qquad \hat{\mu_1} =  \frac{\sum_{i=1}^n x_iy_i}{\sum_{i=1}^n y_i}, \qquad \hat{\mu_0} =  \frac{\sum_{i=1}^n x_i(1-y_i)}{\sum_{i=1}^n (1-y_i)}
	\end{align*} 
	The Fisher information matrix 
	\begin{align*}
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial \pi_1^2}  &= \sum_{i=1}^n -\frac{y_i}{\pi_1^2} - \frac{(1-y_i)}{(1-\pi_1)^2} , \qquad E[-\frac{\partial ln^2 p(X, Y|\theta)}{\partial \pi_1^2}] =  \frac{1}{\pi_1(1-\pi_1)}\\
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial \mu_1^2}  &= \sum_{i=1}^n -\frac{y_i}{\sigma^2}, \qquad E[-\frac{\partial ln^2 p(X, Y|\theta)}{\partial \mu_1^2} ]= \frac{\pi_1}{\sigma^2}\\
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial \mu_0^2}  &= \sum_{i=1}^n -\frac{(1-y_i)}{\sigma^2}, \qquad E[- \frac{\partial ln^2 p(X, Y|\theta)}{\partial \mu_0^2}] = \frac{1-\pi_1}{\sigma^2}\\
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial (\sigma^2)^2}  &= \frac{n}{2 (\sigma^2)^2} - \sum_{i=1}^n \frac{(x_i-\mu_1)^2y_i}{(\sigma^2)^3} - \sum_{i=1}^n \frac{(x_i-\mu_0)^2(1-y_i)}{(\sigma^2)^3}\\
		E[-\frac{\partial ln^2 p(X, Y|\theta)}{\partial (\sigma^2)^2}] &= \frac{1}{2\sigma^4} \\
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial \pi_1\mu_1}  &=0\\
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial \pi_1\mu_0}  &=0\\
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial \pi_1\sigma}  &=0\\
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial \mu_1\mu_0}  &=0\\
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial \mu_1\sigma}  &= \sum_{i=1}^n - \frac{y_i(x_i-\mu_1)}{(\sigma^2)^2} , \qquad E[-\frac{\partial ln^2 p(X, Y|\theta)}{\partial \mu_1\sigma}] = 0  \\
		\frac{\partial ln^2 p(X, Y|\theta)}{\partial \mu_0\sigma}  &= \sum_{i=1}^n - \frac{(1-y_i)(x_i-\mu_0)}{(\sigma^2)^2} , \qquad E[-\frac{\partial ln^2 p(X, Y|\theta)}{\partial \mu_0\sigma}] = 0 \\
	\end{align*} 
	So we have covariance matrix, by CLT
	\begin{align*}    
		I(\theta) &= E[- \frac{1}{n} \frac{\partial ln^2 p(X,Y|\theta)}{\partial \theta^2}], \qquad
		= \begin{bmatrix}
			\frac{1}{\pi_1(1-\pi_1)} & 0 & 0 & 0\\
			0 & \frac{\pi_1}{\sigma^2} & 0 & 0\\
			0 & 0 & \frac{1-\pi_1}{\sigma^2} & 0\\
			0 & 0 & 0 & \frac{1}{2\sigma^4} \\
		\end{bmatrix}\\
		\sqrt{n} (\hat{\theta} - \theta) & \xrightarrow[]{d} N \left(0, \Sigma \right), \qquad
		\Sigma(\theta) = I(\theta)^{-1} = \begin{bmatrix}
			\pi_1(1-\pi_1) & 0 & 0 & 0\\
			0 & \frac{\sigma^2}{ \pi_1} & 0 & 0\\
			0 & 0 & \frac{\sigma^2}{ 1-\pi_1} & 0\\
			0 & 0 & 0 & 2\sigma^4 \\
		\end{bmatrix}
	\end{align*} 
	\item[(d)] Calculate the asymptotic covariance matrix of $h(\hat\theta^F )$.
	\begin{align*}    
		h(\theta^F ) &= (\alpha_0, \alpha_1)= \left(log(\frac{\pi_1}{1-\pi_1}) +\frac{\mu_0^2 - \mu_1^2}{2\sigma^2},  \frac{(\mu_1-\mu_0)}{\sigma^2} \right)^T\\
		\frac{\partial h(\theta^F)}{\partial \pi_1} & = (\frac{1}{\pi_1}+\frac{1}{1-\pi_1} , 0)^T\\
		\frac{\partial h(\theta^F )}{\partial \mu_1} & = (-\frac{\mu_1}{\sigma^2}, \frac{1}{\sigma^2})^T\\
		\frac{\partial h(\theta^F )}{\partial \mu_0} & = (\frac{\mu_0}{\sigma^2}, -\frac{1}{\sigma^2})^T\\
		\frac{\partial h(\theta^F )}{\partial \sigma^2} & = \left(-\frac{(\mu_0^2 - \mu_1^2)}{2\sigma^4}, -\frac{(\mu_1-\mu_0)}{\sigma^4} \right)^T\\
		\sqrt{n} (h(\hat\theta^F ) - h(\theta^F )) & \xrightarrow[]{d} N \left(0, \Sigma_ h \right)
	\end{align*}   
	By delta method, 
	\begin{align*}    
		\Sigma^{h} &= h(\theta^F )'\Sigma(\theta) (\theta^F )'^T\\
		&=\begin{bmatrix}
			\frac{1}{\pi_1}+\frac{1}{1-\pi_1} & -\frac{\mu_1}{\sigma^2} & \frac{\mu_0}{\sigma^2} &-\frac{(\mu_0^2 - \mu_1^2)}{2\sigma^4}\\
			0 & \frac{1}{\sigma^2} & -\frac{1}{\sigma^2} & -\frac{(\mu_1-\mu_0)}{\sigma^4}\\
		\end{bmatrix}\begin{bmatrix}
			\pi_1(1-\pi_1) & 0 & 0 & 0\\
			0 & \frac{\sigma^2}{ \pi_1} & 0 & 0\\
			0 & 0 & \frac{\sigma^2}{ 1-\pi_1} & 0\\
			0 & 0 & 0 & 2\sigma^4 \\
		\end{bmatrix}\begin{bmatrix}
			\frac{1}{\pi_1}+\frac{1}{1-\pi_1} & 0 \\
			-\frac{\mu_1}{\sigma^2} & \frac{1}{\sigma^2}\\
			\frac{\mu_0}{\sigma^2} & -\frac{1}{\sigma^2}\\
			-\frac{(\mu_0^2 - \mu_1^2)}{2\sigma^4} & -\frac{(\mu_1-\mu_0)}{\sigma^4} \\
		\end{bmatrix}\\
		&= \begin{bmatrix}
			\frac{1}{\pi_1(1-\pi_1)} + \frac{\mu_0}{(1-\pi_1)\sigma^2} + \frac{\mu_1}{\pi_1\sigma^2} + \frac{(\mu_0^2 - \mu_1^2)^2}{2\sigma^4} & -\frac{1}{\sigma^2}(\frac{\mu_0}{1-\pi_1} + \frac{\mu_1}{\pi_1}) + \frac{(\mu_1-\mu_0)(\mu_0^2- \mu_1^2)}{\sigma^4} \\
			-\frac{1}{\sigma^2}(\frac{\mu_0}{1-\pi_1} + \frac{\mu_1}{\pi_1}) + \frac{(\mu_1-\mu_0)(\mu_0^2- \mu_1^2)}{\sigma^4}  & \frac{1}{\sigma^2 \pi_1(1-\pi_1)} + \frac{2(\mu_1-\mu_0)^2}{\sigma^4}\\
		\end{bmatrix}
	\end{align*}   
	\item[(e)] In this part, suppose that $\mu_0= \mu_1$. Show that $Cov(\hat\alpha)^{-1} Cov(h(\hat\theta^F))$ converges to a matrix which does not depend on $\theta$. Interpret this result.\\
	If $\mu_0= \mu_1$, then $\alpha = (\alpha_0, \alpha_1)^T = \left(log(\pi_1/\pi_0) ,  0 \right)^T$
	The covariance matrix of $\alpha$
	\begin{align*}    
		\alpha_0 & = log(\pi_1/\pi_0)\\
		ln p(Y_i|\alpha) &= \sum_{i=1}^n y_i (\alpha_0) - log \left(1 + exp(\alpha_0) \right)\\
		\frac{\partial ln p(Y_i|\alpha)}{\partial \alpha_0}  &= \sum_{i=1}^n y_i -\frac{exp\alpha_0}{1+ exp\alpha_0}\\
		\frac{\partial ln^2 p(Y_i|\alpha)}{\partial \alpha_0^2}  &= \sum_{i=1}^n -\frac{exp\alpha_0}{(1+ exp\alpha_0)^2}\\
		I_n(\alpha) &= E[-\frac{\partial ln^2 p(Y_i|\alpha)}{\partial \alpha_0^2}] = \sum_{i=1}^n \frac{exp\alpha_0}{(1+ exp\alpha_0)^2}\\
		log p(\theta) &= nlog \frac{1}{\sqrt{2\pi}\sigma} + \sum_{i=1}^n y_i log\pi_1 + (1-y_i) log(1-\pi_1) - \frac{(x_i-\mu)^2}{2\sigma^2} \\
		\frac{\partial ln p(\theta)}{\partial \pi_1}  &= \sum_{i=1}^n \frac{y_i}{\pi_1} - \frac{1-y_i}{1-\pi_1} \\
		\frac{\partial ln^2 p(\theta)}{\partial \pi_1^2}  &= \sum_{i=1}^n -\frac{y_i}{\pi_1^2} - \frac{1-y_i}{(1-\pi_1)^2} , \qquad E[-\frac{\partial ln^2 p(\theta)}{\partial \pi_1^2}] = n\frac{\pi_1}{(1-\pi_1))}\\
		\frac{\partial ln p(\theta)}{\partial \mu}  &= \sum_{i=1}^n \frac{x_i-\mu}{\sigma^2}  \\
		\frac{\partial ln^2 p(\theta)}{\partial \mu^2}  &= \sum_{i=1}^n -\frac{1}{\sigma^2}  \\  
		\frac{\partial ln p(\theta)}{\partial \sigma^2}  &=-\frac{n}{2}{1/\sigma^2} + \sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^4} \\
		\frac{\partial ln^2 p(\theta)}{\partial (\sigma^2)^2}  &= \frac{n}{2(\sigma^2)^2} - \sum_{i=1}^n \frac{(x_i-\mu)^2}{\sigma^6}, \qquad E[-\frac{\partial ln^2 p(\theta)}{\partial (\sigma^2)^2}] = \frac{n}{2\sigma^4}  \\ 
		\frac{\partial ln^2 p(\theta)}{\partial \mu\sigma^2}  &= \sum_{i=1}^n- \frac{x_i-\mu}{\sigma^4} , \qquad E[-\frac{\partial ln^2 p(\theta)}{\partial \mu\sigma^2}] = 0
	\end{align*} 
	Then we have Fisher information $I_n(\theta) $
	\begin{align*}   
		I_n(\theta) &= E[-\frac{\partial ln^2 p(\theta)}{\partial \theta^2}] \\
		&= \begin{bmatrix}
			n\frac{\pi_1}{(1-\pi_1))}  & 0 & 0 \\
			0 & \frac{n}{\sigma^2}   &  0 \\
			0 &  0 & \frac{n}{2\sigma^4} \\
		\end{bmatrix}\\
		Cov(\hat\alpha)^{-1} &= I_n(\alpha) = n\pi_1(1-\pi_1)\\
		\frac{\partial h}{\partial \theta} &= (\frac{1}{\pi_1(1-\pi_1)}, 0 , 0)\\
	\end{align*} 
	Then we have
	\begin{align*}   
		Cov(\hat\alpha)^{-1} \Sigma^{h} &= I_n(\alpha) \frac{\partial h}{\partial \theta} I_n(\theta)^{-1} \frac{\partial h}{\partial \theta}^T\\
		&=  n\pi_1(1-\pi_1)  (\frac{1}{\pi_1(1-\pi_1)}, 0 , 0) \begin{bmatrix}
			\pi_1(1-\pi_1)/n  & 0 & 0 \\
			0 & \sigma^2/n   &  0 \\
			0 &  0 & 2\sigma^4/n \\
		\end{bmatrix} (\frac{1}{\pi_1(1-\pi_1)}, 0 , 0)^T\\   
		&= 1
	\end{align*}     
	So we have $Cov(\hat\alpha)^{-1} Cov(h(\hat\theta^F))$ converges to a matrix which does not depend on $\theta$.
	\item[(f)]Now suppose that $\pi_1$ is known. Will the results of (b) - (e) be changed? Please
	explain. If so, then derive the corresponding results and compare with those obtained
	above.\\
	If $\pi_1$ is known,
	\begin{itemize}
		\item [(i)] For (b), does not change as the parameters are $\alpha = (\alpha_0, \alpha_1)^T$ which does not involve $\pi_1$.
		\begin{align*}
			I_n(\alpha) &= -E[ \frac{\partial ln^2 p(Y_i|\alpha)}{\partial \alpha^2}]\\
			&= \begin{bmatrix}
				n \pi_1 (1-\pi_1) &   \sum_{i=1}^n \pi_1 (1-\pi_1)x_i\\
				\sum_{i=1}^n \pi_1 (1-\pi_1)x_i &  \sum_{i=1}^n \pi_1 (1-\pi_1)x_ix_i^T\\
			\end{bmatrix}\\
			Cov(\alpha) &= I_n(\alpha)^{-1} = \frac{1}{[\sum_{i=1}^n n x_i^2 - (\sum_{i=1}^n x_i)^2]\pi_1 (1-\pi_1)}  \begin{bmatrix}
				\sum_{i=1}^n n x_i^2 &   -\sum_{i=1}^n x_i\\
				-\sum_{i=1}^n x_i\sum_{i=1}^n \pi_1 (1-\pi_1)x_i & n\\
			\end{bmatrix}
		\end{align*} 
		\item[(ii)] For (c), it involves $\pi_1$, so the result will change. We have covariance matrix for $\theta = (\mu_1, \mu_0, \sigma^2)^T$, 
		\begin{align*}    
			I(\theta) &= E[- \frac{1}{n} \frac{\partial ln^2 p(X,Y|\theta)}{\partial \theta^2}], \qquad
			= \begin{bmatrix}
				\frac{\pi_1}{\sigma^2} & 0 & 0\\
				0 & \frac{1-\pi_1}{\sigma^2} & 0\\
				0 & 0 & \frac{1}{2\sigma^4} \\
			\end{bmatrix}\\
			\sqrt{n} (\hat{\theta} - \theta) & \xrightarrow[]{d} N \left(0, \Sigma \right), \qquad
			\Sigma = I(\theta)^{-1} = \begin{bmatrix}
				\frac{\sigma^2}{ \pi_1} & 0 & 0\\
				0 & \frac{\sigma^2}{ 1-\pi_1} & 0\\
				0 & 0 & 2\sigma^4 \\
			\end{bmatrix}
		\end{align*} 
		\item[(iii)] For (d), the $h(\theta)$ does not involve $\pi_1$, but the Jacobian matrix and $I(\theta)$ will change when $\pi_1$ is known. We have covariance matrix for $h(\theta) = c(\mu, \sigma^2)$.
		\begin{align*}    
			h(\theta^F ) &= (\alpha_0, \alpha_1)= \left(log(\frac{\pi_1}{1-\pi_1}) +\frac{\mu_0^2 - \mu_1^2}{2\sigma^2},  \frac{(\mu_1-\mu_0)}{\sigma^2} \right)^T\\
			\sqrt{n} (h(\hat\theta^F ) - h(\theta^F )) & \xrightarrow[]{d} N \left(0, \Sigma_ h \right)\\
			h(\theta^F )' &= \begin{bmatrix}
				-\frac{\mu_1}{\sigma^2} & \frac{\mu_0}{\sigma^2} &-\frac{(\mu_0^2 - \mu_1^2)}{2\sigma^4}\\
				\frac{1}{\sigma^2} & -\frac{1}{\sigma^2} & -\frac{(\mu_1-\mu_0)}{\sigma^4}\\
			\end{bmatrix}\\
			\Sigma(\theta) &= \begin{bmatrix}
				\frac{\sigma^2}{ \pi_1} & 0 & 0\\
				0 & \frac{\sigma^2}{ 1-\pi_1} & 0\\
				0 & 0 & 2\sigma^4 \\
			\end{bmatrix}\\
			\Sigma^{h} &= h(\theta^F )'\Sigma(\theta) (\theta^F )'^T\\
			&= \begin{bmatrix}
				\frac{\mu_0}{(1-\pi_1)\sigma^2} + \frac{\mu_1}{\pi_1\sigma^2} + \frac{(\mu_0^2 - \mu_1^2)^2}{2\sigma^4} & -\frac{1}{\sigma^2}(\frac{\mu_0}{1-\pi_1} + \frac{\mu_1}{\pi_1}) + \frac{(\mu_1-\mu_0)(\mu_0^2- \mu_1^2)}{\sigma^4} \\
				-\frac{1}{\sigma^2}(\frac{\mu_0}{1-\pi_1} + \frac{\mu_1}{\pi_1}) + \frac{(\mu_1-\mu_0)(\mu_0^2- \mu_1^2)}{\sigma^4}  &  \frac{2(\mu_1-\mu_0)^2}{\sigma^4}\\
			\end{bmatrix}
		\end{align*}
		\item[(iv)] For (e), the only parameter that need to estimate is $\alpha_0 = log(\pi_1/(1-\pi_1))$, which is now known. The question is meaningless. 
	\end{itemize}
\end{itemize}

\end{document}