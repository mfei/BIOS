

\section{Problem 1- Carlin and Louis}
Diagnostic test, represent $P(D=d|T=t)$ in terms of test sensitivity, $P(T=1|D=1)$. Specificity, $P(T= 0|D= 0)$. and disease prevalence, $P(D=1)$, and relate to Bayes' theorem.
 
\begin{align*}
	P(D=d|T=t) &= \frac{P(D=d, T=t)}{P(T=t)} \\
	&= \frac{P(D=d, T=t)}{\sum_{D=d} P(T=t, D=d) P(D=d)} \\
	&= \frac{P(T=t|D=d) P(D=t)}{P(T=t, D=0)P(D=0) + P(T=t, D=1)P(D=1)}\\
	&= \frac{P(T=t|D=d) P(D=d)}{P(T=t|D=0)P(D=0)^2 + P(T=t|D=1)P(D=1)^2} \\
\end{align*}

So we have

\begin{align*}
P(D= 1|T= 1) &= \frac{P(T= 1|D= 1) P(D=1)}{P(T=1|D=0)(1- P(D=1))^2 + P(T=1|D=1)P(D=1)^2} \\
&=  \frac{P(T= 1|D= 1) P(D=1)}{(1- P(T=0|D=0))(1- P(D=1))^2 + P(T=1|D=1)P(D=1)^2} \\
P(D= 0|T= 1) &= \frac{P(T= 1|D= 0) (1-P(D=1))}{P(T=1|D=0)(1- P(D=1))^2 + P(T=1|D=1)P(D=1)^2} \\
&=  \frac{(1- P(T=0|D=0)) (1-P(D=1))}{(1- P(T=0|D=0))(1- P(D=1))^2 + P(T=1|D=1)P(D=1)^2} \\
P(D= 0|T= 0) &= \frac{P(T= 0|D= 0) (1-P(D=1))}{P(T=0|D=0)(1- P(D=1))^2 + P(T=0|D=1)P(D=1)^2} \\
&=  \frac{P(T=0|D=0) (1-P(D=1))}{ P(T=0|D=0)(1- P(D=1))^2 + (1- P(T=1|D=1)) P(D=1)^2} \\
P(D= 1|T= 0) &= \frac{P(T= 0|D= 1) P(D=1)}{P(T=0|D=0)(1- P(D=1))^2 + P(T=0|D=1)P(D=1)^2} \\
&=  \frac{ (1- P(T=1|D=1)) P(D=1)}{ P(T=0|D=0)(1- P(D=1))^2 + (1- P(T=1|D=1)) P(D=1)^2} \\
\end{align*}

\section{Problem 2}
Suppose $X_1, .., X_n$ is a random sample from X, where X has density 

\begin{align}
	p(x|r, \theta) &= \begin{cases} 
		{r+x-1 \choose x} \theta^{r} (1-\theta)^{x} & x=0,1,2...\\
	     0 & \text{otherwise}
	\end{cases}
\end{align}

where $0< \theta < 1, r$ is known, and $\theta$ is unknown.

\begin{itemize}
	\item [(a)] Derive Jeffreys's prior for $\theta$. 
	
\begin{align*}
	p(\theta) & \propto \sqrt{det I(\theta)} \\
	log p(x|r, \theta) &= log {r+x-1 \choose x} + rlog \theta + x log(1-\theta) \\
	\diffp{log p(x|r, \theta)}{\theta} &= \frac{r}{\theta} - \frac{x}{1-\theta} \\
	\diffp{log p(x|r, \theta)}{\theta \theta} &= -\frac{r}{\theta^2} - \frac{x}{(1-\theta)^2} \\
	I(\theta) &= -E[\diffp{log p(x|r, \theta)}{\theta \theta} ] = \frac{r}{\theta^2} + \frac{E[X]}{(1-\theta)^2}
\end{align*}	
	
	The $p(x|r, \theta)$ is negative binomial distribution, which we have 
\begin{align*}
	E(X) &= \frac{r(1-\theta)}{\theta} \\
	I(\theta) &= \frac{r}{\theta^2 (1-\theta)}
\end{align*}	
Thus, Jeffery's prior is 

\begin{align*}
	\pi(\theta) & \propto  I(\theta)^{1/2} = [\frac{r}{\theta^2 (1-\theta)}]^{1/2}
\end{align*}	
	
	\item[(b)] Suppose the prior for $\theta$ is $\theta \sim beta(\alpha_0, \lambda_0)$, where $\alpha_0, \lambda_0$ are specified hyperparameters. Derive $E(X)$ and $Var(X)$.
	
	\begin{align*}
		\pi(\theta)	&= \frac{1}{B(\alpha_0, \lambda_0)} \theta^{\alpha_0 -1} (1-\theta)^{\lambda_0 -1} 
	\end{align*}	
	
	While from part (a), we know that $x|\theta \sim NB(r, \theta)$, then 
	
	\begin{align*}
		E(X|\theta) &= \frac{r(1-\theta)}{\theta} \\
		Var(X|\theta) &=  \frac{r(1-\theta)}{\theta^2} 
	\end{align*}
So
\begin{align*}
	E(X) &= E[E(X|\theta)] = E[\frac{r(1-\theta)}{\theta} ]\\
	&= \int_{0}^{1} \frac{r(1-\theta)}{\theta} \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \theta^{\alpha_0-1} (1-\theta)^{\lambda_0 -1} d\theta \\
	&=  r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)}  \int_{0}^{1}  \theta^{\alpha_0-2} (1-\theta)^{\lambda_0} d\theta \\
	&= r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0 -1) \Gamma(\lambda_0+1) }{\Gamma(\alpha_0 + \lambda_0)} \int_{0}^{1} \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0-1) \Gamma(\lambda_0 +1)}  \theta^{\alpha_0-2} (1-\theta)^{\lambda_0} d\theta \\
	&= r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0 -1) \Gamma(\lambda_0+1) }{\Gamma(\alpha_0 + \lambda_0)} \\
	&= \frac{r \lambda_0}{\alpha_0 -1} , \qquad \alpha_0 > 1
\end{align*}
For variance,
\begin{align*}
	Var(X) &= E[Var(X|\theta)] + Var[E[X|\theta]]  \\
	&= E[\frac{r(1-\theta)}{\theta^2} ] + Var[\frac{r(1-\theta)}{\theta} ]\\
	E[\frac{r(1-\theta)}{\theta^2} ] 
	&= \int_{0}^{1} \frac{r(1-\theta)}{\theta^2} \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \theta^{\alpha_0-1} (1-\theta)^{\lambda_0 -1} d\theta \\
	&=  r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)}  \int_{0}^{1}  \theta^{\alpha_0-3} (1-\theta)^{\lambda_0} d\theta \\
	&= r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0 -2) \Gamma(\lambda_0+1) }{\Gamma(\alpha_0 + \lambda_0 -1)} \int_{0}^{1} \frac{\Gamma(\alpha_0 + \lambda_0-1)}{\Gamma(\alpha_0-2) \Gamma(\lambda_0 +1)}  \theta^{\alpha_0-3} (1-\theta)^{\lambda_0} d\theta \\
	&= r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0 -2) \Gamma(\lambda_0+1) }{\Gamma(\alpha_0 + \lambda_0-1)} \\
	&= \frac{r \lambda_0 (\alpha_0 + \lambda_0 -1)}{(\alpha_0 -1) (\alpha_0 -2)} , \qquad \alpha_0 > 2\\
	E[[\frac{r(1-\theta)}{\theta}] ^2]
	&= \int_{0}^{1} [\frac{r(1-\theta)}{\theta} ]^2\frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \theta^{\alpha_0-1} (1-\theta)^{\lambda_0 -1} d\theta \\
	&= r^2 \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)}  \int_{0}^{1}  \theta^{\alpha_0-3} (1-\theta)^{\lambda_0 +1} d\theta \\
	&= r^2 \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0-2) \Gamma(\lambda_0-2) }{\Gamma(\alpha_0+ \lambda_0)}  \int_{0}^{1}  \frac{\Gamma(\alpha_0+ \lambda_0)}{\Gamma(\alpha_0-2) \Gamma(\lambda_0-2) } \theta^{\alpha_0-3} (1-\theta)^{\lambda_0 +1} d\theta \\
	&= \frac{r^2 \lambda_0 (\lambda_0+1)}{(\alpha_0-1 )(\alpha_0-2)} , \qquad \alpha_0 > 2
\end{align*}
So we have 
\begin{align*}
	Var(\frac{r(1-\theta)}{\theta} ) &= E(\frac{r(1-\theta)}{\theta} )^2 - [E(\frac{r(1-\theta)}{\theta} )  ]^2 \\
	&= \frac{r^2 \lambda_0 (\lambda_0+1)}{(\alpha_0-1 )(\alpha_0-2)}  - [\frac{r \lambda_0}{\alpha_0 -1}]^2 \\
	&= \frac{r^2 \lambda_0 (\lambda_0 + \alpha_0-1)}{(\alpha_0-1)^2 (\alpha_0-2)} ,\qquad \alpha_0 > 2
\end{align*}
Thus,
\begin{align*}
	Var(X) &= E[\frac{r(1-\theta)}{\theta^2} ] + Var(\frac{r(1-\theta)}{\theta} ) \\
	&=\frac{r \lambda_0 (\alpha_0 + \lambda_0 -1)}{(\alpha_0 -1) (\alpha_0 -2)}   + \frac{r^2 \lambda_0 (\lambda_0 + \alpha_0-1)}{(\alpha_0-1)^2 (\alpha_0-2)} \\
	&= \frac{r \lambda_0 (\alpha_0 + \lambda_0 -1) (\alpha_0 + r -1)}{(\alpha_0-1)^2 (\alpha_0-2)}, \qquad \alpha_0 > 2
\end{align*}

	\item[(c)] Suppose the prior for $\theta$ is $\theta \sim beta(\alpha_0, \lambda_0)$, where $\alpha_0, \lambda_0$ are specified hyperparameters. Derive the posterior distribution of $\theta$.
	
	\begin{align*}
	\pi(\theta | X) & \propto \frac{\Gamma(\alpha_0 + \lambda_0) }{\Gamma(\alpha_0) \Gamma(\lambda_0)} \theta^{\alpha_0-1} (1-\theta)^{\lambda_0-1} \prod_{i=1}^n {r+x_i-1 \choose x_i} \theta^{r} (1-\theta)^{x_i} , \qquad \theta \in (0,1) 
\end{align*}

because beta distribution is a conjugate of the negative binomial distribution, we have the posterior distribution of $\theta|X$ also a negative binomial

	\begin{align*}
	\pi(\theta | X) & = {nr+\alpha_0+ \sum_i x_i + \lambda_0-3 \choose \sum_i x_i + \lambda_0-1} \theta^{nr + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0-1}
\end{align*}

	\item[(d)] Suppose the prior for $\theta$ is $\theta \sim beta(\alpha_0, \lambda_0)$, where $\alpha_0, \lambda_0$ are specified hyperparameters. Derive the posterior predictive distribution of $z=(z_1, z_2)$, where conditional on $\theta$, $z_1, z_2$ are i.i.d with distribution of (1).
	
	The predictive distribution 
	
	\begin{align*}
		p(z | X) & = \int p(z| \theta) p(\theta | X) d \theta
	\end{align*}

As $z=(z_1, z_2)$, where conditional on $\theta$, $z_1, z_2$ are i.i.d with distribution of (1), so

\begin{align*}
	p(z | \theta) & = p(z_1| \theta) p(z_2|\theta) = {r+z_1-1 \choose z_1} {r+z_2-1 \choose z_2} \theta^{2r} (1-\theta)^{z_1+z_2} \\
	p(z | X) & = \int_{0}^{1} {r+z_1-1 \choose z_1} {r+z_2-1 \choose z_2} \theta^{2r} (1-\theta)^{z_1+z_2}  {nr+\alpha_0+ \sum_i x_i + \lambda_0-3 \choose \sum_i x_i + \lambda_0-1} \\
	& \theta^{nr + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0-1} d \theta\\
	&= {r+z_1-1 \choose z_1} {r+z_2-1 \choose z_2} {nr+\alpha_0+ \sum_i x_i + \lambda_0-3 \choose \sum_i x_i + \lambda_0-1} \\
	& \int_{0}^{1}  \theta^{2r} (1-\theta)^{z_1+z_2} \theta^{nr + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0-1} d \theta
\end{align*}

We can construct the negative binomial distribution for
\begin{align*}
	& \int_{0}^{1}  \theta^{2r} (1-\theta)^{z_1+z_2} \theta^{nr + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0-1} d \theta \\
	&= \frac{1}{A} \int_{0}^{1} {(n+2) r + \alpha_0 + \sum_i x_i + \lambda_0 + z_1+z_2 -3 \choose \sum_i x_i + \lambda_0-1} \theta^{(n+2) r + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0 + z_1+z_2 -1} d \theta 
\end{align*}
let $A = {(n+2) r + \alpha_0 + \sum_i x_i + \lambda_0 + z_1+z_2 -3 \choose \sum_i x_i + \lambda_0-1}$, so

\begin{align*}
	p(z | \theta) & =  {r+z_1-1 \choose z_1} {r+z_2-1 \choose z_2} {nr+\alpha_0+ \sum_i x_i + \lambda_0-3 \choose \sum_i x_i + \lambda_0-1} \frac{1}{{(n+2) r + \alpha_0 + \sum_i x_i + \lambda_0 + z_1+z_2 -3 \choose \sum_i x_i + \lambda_0-1}}
\end{align*}

\end{itemize}

\section{Problem 3}
We have distribution 
\begin{align}
	\pi(\theta) &= \begin{cases} 
		\frac{\beta^{\alpha} }{\Gamma(\alpha)} \theta^{-(\alpha + 1)} exp(-\frac{\beta}{\theta}), & \qquad \theta > 0 \\
		0 & \qquad \theta \leq 0
		\end{cases} 
	\label{eq:1}
\end{align}

\begin{itemize}
	\item [(a)] Verify that $\pi(\theta)$ is a probability density function.
	
	We need to show that $\int \pi(\theta) d\theta = 1$.
	\begin{align*}
	\int_{0}^{\infty}	\pi(\theta) d \theta &= 
		\int_{0}^{\infty}	\frac{\beta^{\alpha} }{\Gamma(\alpha)} \theta^{-(\alpha + 1)} exp(-\frac{\beta}{\theta}) d \theta \\
		\text{let} x &= \frac{1}{\theta}, J = |\diffp{\theta}{x}| = \frac{1}{x^2} \\
		&= \int_{0}^{\infty} \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha + 1} exp(-\beta x) \frac{1}{x^2} dx \\
		&= \int_{0}^{\infty} \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha -1} exp(-\beta x)  dx \\
		&= 1
	\end{align*}
	
	\item[(b)] Consider family of prior distribution, show it is a conjugate prior for normal family with a known value of mean $\mu$, and unknown value of variance $\theta$. 
	
	 We need to show that the posterior distribution of $p(\theta|x)$ is also a gamma distribution, which follows the distribution $(\ref{eq:1})$.
	 
	\begin{align*}
		p(x_i|\mu, \theta) &= \frac{1}{\sqrt{2 \pi \theta}} exp(- \frac{(x_i-\mu)^2}{2 \theta} )\\
		p(\theta|X) &= \frac{\prod_{i=1}^n f(x_i| \mu, \theta) f(\theta)}{f(X)} \\
		&= \frac{ (2\pi \theta)^{-n/2} exp(-\frac{\sum_{i=1} (x_i-\mu)^2}{2\theta}) \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-(\alpha + 1)} exp(-\frac{\beta}{\theta}) }{f(X)} 
	\end{align*} 
To calculate $f(X)= \int f(x|\theta) f(\theta) d \theta$,
\begin{align*}
	f(X)&=  \int_{0}^{\infty} (2\pi \theta)^{-n/2} exp(-\frac{\sum_{i=1} (x_i-\mu)^2}{2\theta}) \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-(\alpha + 1)} exp(-\frac{\beta}{\theta}) d \theta \\	
	&= \int_{0}^{\infty} (2\pi )^{-n/2} \frac{\beta^{\alpha}}{\Gamma(\alpha)}  exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2\theta}) \theta^{-(\alpha + 1 + \frac{n}{2})}  d \theta \\	
	& \text{let} y= \frac{1}{\theta}, \qquad J=|\diffp{\theta}{y}| = \frac{1}{y^2} \\
	&= \int_{0}^{\infty} (2\pi )^{-n/2} \frac{\beta^{\alpha}}{\Gamma(\alpha)}  exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2} y) y^{(\alpha + 1 + \frac{n}{2})} y^{-2} d y \\	
	&= \int_{0}^{\infty} (2\pi )^{-n/2} \frac{\beta^{\alpha}}{\Gamma(\alpha)}  exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2} y) y^{(\alpha + \frac{n}{2})-1}  d y 
\end{align*} 
Further,
\begin{align*}
	f(X) &= (2\pi )^{-n/2}  \frac{\beta^{\alpha}}{\Gamma(\alpha)} \frac{\Gamma(\alpha + \frac{n}{2})}{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}\\
	& \int_{0}^{\infty} \frac{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}{\Gamma(\alpha + \frac{n}{2})} exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2} y) y^{(\alpha + \frac{n}{2})-1}  d y \\	
	&=  (2\pi )^{-n/2}  \frac{\beta^{\alpha}}{\Gamma(\alpha)} \frac{\Gamma(\alpha + \frac{n}{2})}{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}
\end{align*}
Then the posterior distribution
\begin{align*}
	p(x_i|\mu, \theta) &=  \frac{ (2\pi )^{-n/2} \frac{\beta^{\alpha}}{\Gamma(\alpha)}  exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2\theta}) \theta^{-(\alpha + 1 + \frac{n}{2})}   }{(2\pi )^{-n/2}  \frac{\beta^{\alpha}}{\Gamma(\alpha)} \frac{\Gamma(\alpha + \frac{n}{2})}{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}} \\
	&= \frac{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}{\Gamma(\alpha + \frac{n}{2})} exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2\theta}) \theta^{-(\alpha + 1 + \frac{n}{2})}
\end{align*} 

The above distribution follows the distribution $(\ref{eq:1})$, so gamma distribution is a conjugate prior for a normal family with unknown variance.

\end{itemize}

\section{Problem 4}
Suppose that the lifetime X of a medical device follows a shifted exponential distribution

\begin{align*}
	p(x|\theta, \mu) &= \theta exp(-\theta (x-\mu)) I(x> \mu)
\end{align*}

where $\theta > 0, -\infty < \mu < \infty$. Suppose that $X_1, X_2, ... X_n$ is a random sample from X.

\begin{itemize}
	\item[(a)]  Derive the posterior mean and variance.

	As we know that the gamma distribution is a conjugate prior for exponential distribution. So the posterior distribution is also a gamma distribution.
	
	\begin{align*}
		\theta & \sim gamma(a_0, b_0) = \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0-1} exp(-b_0 x) \\
		p(x_i|\theta, \mu) &= \theta exp(-\theta (x_i-\mu)) I(x_i> \mu) \\
		p(\theta|X) & \propto p(X|\theta) p(\theta) \\
		&= \prod_{i=1}^n \theta exp(-\theta (x_i-\mu)) I(x_i> \mu) \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0-1} exp(-b_0 \theta) \\
		&= \theta^{a_0 + n-1} exp[ -\theta \sum_{i=1}^n (x_i-\mu) - b_0 \theta]
	\end{align*}

Thus, the posterior distribution of $\theta$

\begin{align*}
	p(\theta|X) & \sim Gamma(a_0 + n, \sum_{i=1}^n (x_i-\mu) + b_0) , \qquad X_{(1)} > \mu
\end{align*}
	
Then the 
\begin{align*}
	E(\theta|X) & = \frac{a_0 + n}{\sum_{i=1}^n (x_i-\mu) + b_0} \\
	Var(\theta|X) &= \frac{a_0 + n}{(\sum_{i=1}^n (x_i-\mu) + b_0)^2} 
\end{align*}


\item[(b)] Suppose that $(\mu, \theta)$ are both unknown, $\mu \sim N(\mu_0, \sigma_0^2), \theta \sim gamma(a_0, b_0), (\mu, \theta)$, are assumed independent a priori.

\begin{itemize}
	\item [(i)] Derive the joint posterior distribution of $(\mu, \theta)$  and express it in the simplest possible form. Note that the normalizing constant of the joint posterior does not have an analytic closed form, but can be expressed as an expectation.
	
	Joint posterior distribution of $(\mu, \theta)$, in which $(\mu, \theta)$ are independent priori.
\begin{align*}
	p(\mu, \theta|X) & = \frac{p(X|\mu, \theta) p(\theta) p(\mu)}{p(X)} \\
	p(X|\mu, \theta) p(\theta) p(\mu) &= \prod_{i=1}^n \theta exp(-\theta (x_i-\mu)) I(x_i> \mu) \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0-1} exp(-b_0 \theta) \frac{1}{\sqrt{2\pi} \sigma_0} exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2})\\
	&= \frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0+n-1} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2})\\
\end{align*}

The normalizing constant 
\begin{align*}
	p(X) &= \int \int p(\theta, \mu, x) d\theta d\mu \\
	&= \int_{0}^{\infty} \int_{-\infty}^{\infty}  \frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0+n-1} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}) d\mu d\theta\\
	&= \frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)}  \int_{-\infty}^{\infty} \Big[\int_{0}^{\infty}  \theta^{a_0+n-1} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)] d\theta\Big]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}) d\mu \\
	&=  \frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)} \frac{\Gamma(a_0+n)}{[\sum (x_i-\mu)+b_0]^{a_0+n}} \int_{-\infty}^{\infty}    exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}) d\mu \\
	&= \frac{b_0^{a_0} \Gamma(a_0+n)}{\Gamma(a_0)} \int_{-\infty}^{\infty}  \frac{1}{[\sum (x_i-\mu)+b_0]^{a_0+n}}  \frac{1}{\sqrt{2\pi} \sigma_0}  exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}) d\mu \\
	&=  \frac{b_0^{a_0} \Gamma(a_0+n)}{\Gamma(a_0)} E_{\mu} \Big[ \frac{1}{[\sum (x_i-\mu)+b_0]^{a_0+n}} \Big]
\end{align*}
Thus the posterior distribution of $(\mu, \theta|X)$

\begin{align*}
	p(\mu, \theta|X) & = \frac{\frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0+n-1} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2})}{\frac{b_0^{a_0} \Gamma(a_0+n)}{\Gamma(a_0)} E_{\mu} \Big[ \frac{1}{[\sum (x_i-\mu)+b_0]^{a_0+n}} \Big]} \\
	&= \frac{1}{\sqrt{2\pi} \sigma_0} \frac{\theta^{a_0+n-1} }{\Gamma(a_0+n) E_{\mu} \Big[ \frac{1}{[\sum (x_i-\mu)+b_0]^{a_0+n}} \Big]} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2})
\end{align*}

\item[(ii)] Obtain the limiting joint posterior distribution $\sigma_0 \rightarrow \infty$.

As $\sigma_0 \rightarrow \infty$, the posterior distribution will be a gamma distribution,
\begin{align*}
	p(\mu, \theta|X)& \propto \theta^{a_0+n-1} \exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)] I(X_{(1)}>\mu) 
\end{align*}
For the normalizing constant, as $\sigma_0 \rightarrow \infty$, we can write
\begin{align*}
	C&= \Gamma(n + \alpha_0) \int_{-\infty}^{\infty} (\sum_{i=1}^n (x_i-\mu)  + b_0)^{-(n+\alpha_0)} I(X_{(1)}>\mu) d\mu\\
	& = \Gamma(n + \alpha_0) \int_{-\infty}^{X_{(1)}} (\sum_{i=1}^n (x_i-\mu)  + b_0)^{-(n+\alpha_0)} d \mu \\
	& = \Gamma(n + \alpha_0) \int_{-\infty}^{X_{(1)}} n^{-(n+\alpha_0)} (\sum_{i=1}^n \frac{x_i}{n}- \mu  + \frac{b_0}{n})^{-(n+\alpha_0)} d \mu \\
	&= \frac{\Gamma(n + \alpha_0)}{n^{(n+\alpha_0) (n + \alpha_0 -1)}} \Big[ \frac{b_0}{n} + \frac{\sum x_i}{n} - X_{(1)} \Big]^{-(n+ \alpha_0 -1)}  \\
	&=  \frac{\Gamma(n + \alpha_0 -1)}{n}  \Big[b_0 + {\sum x_i}- n X_{(1)} \Big]^{-(n+ \alpha_0 -1)} 
\end{align*}

Thus, 
\begin{align*}
	p(\mu, \theta|X)& = \frac{n \theta^{a_0+n-1} \exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)] I(X_{(1)}>\mu) }{\Gamma(n + \alpha_0 -1) \Big[b_0 + {\sum x_i}- n X_{(1)} \Big]^{-(n+ \alpha_0 -1)}} 
\end{align*}

\end{itemize}

\end{itemize}

\section{Problem 5}
Let $U(a, b)$ denote the uniform distribution on the interval $(a, b)$. 

\begin{itemize}
	\item [(a)] $U(\theta,\theta+1)$, find posterior distribution of $\theta$
	
	\begin{align*}
		\theta < X_1 ... X_n < \theta + 1 \\
		p(\theta) &= \frac{1}{ b_0 - a_0} I(a_0 \leq \theta \leq b_0)\\
		p(\theta |X) &= \frac{p(x|\theta) p(\theta)}{p(X)} \\
		p(x|\theta) p(\theta) &= \prod_{i=1}^n I(x_i \in (\theta, \theta + 1)) \frac{1}{ b_0 - a_0} I(a_0 \leq \theta \leq b_0)\\
		&= \frac{1}{ b_0 - a_0} I(X_{i} \in (\theta, \theta + 1)) I(a_0 \leq \theta \leq b_0)\\
		&=  \frac{1}{ b_0 - a_0}  I(X_{(1)} \geq \theta) I ( X_{(n)}  \leq (\theta + 1)) I(a_0 \leq \theta \leq b_0)\\
		&= \frac{1}{ b_0 - a_0}  I \Big( \max(a_0, X_{(n)}-1) \leq \theta \leq \min(X_{(1)}, b_0) \Big)
	\end{align*}
	where $X_{(1)}, X_{(n)}$ are the minimum and maximum of the $X_1, ... X_n$. 
	While the normalizing constant term,
	\begin{align*}
		p(X) &= \frac{1}{ b_0 - a_0} \int_{\max(a_0, X_{(n)}-1) } ^{\min(X_{(1)}, b_0)}  d\theta\\
		&=  \frac{\min(X_{(1)}, b_0) -\max(a_0, X_{(n)}-1) }{ (b_0 - a_0)} 
	\end{align*}	
Thus, the posterior distribution 

	\begin{align*}
	p(\theta|X) &= \frac{\frac{1}{ b_0 - a_0}  I \Big( max(a_0, X_{(n)}-1) \leq \theta \leq min(X_{(1)}, b_0) \Big)}{\frac{min(X_{(1)}, b_0) -max(a_0, X_{(n)}-1) }{ (b_0 - a_0)}} \\
	&= \frac{ I \Big( max(a_0, X_{(n)}-1) \leq \theta \leq min(X_{(1)}, b_0) \Big)}{min(X_{(1)}, b_0) -max(a_0, X_{(n)}-1)}
\end{align*}	

	The posterior median of $\theta$
\begin{align*}
	\begin{cases*}
		p(\theta \geq m|X_1, ..X_n)  \geq \frac{1}{2}, \qquad \rightarrow  \frac{m - max(a_0, X_{(n)}-1)}{min(X_{(1)}, b_0) -max(a_0, X_{(n)}-1)} \geq \frac{1}{2} \\
		p(\theta \leq m|X_1, ..X_n)  \geq \frac{1}{2} \qquad  \rightarrow \frac{min(X_{(1)}, b_0)- m }{min(X_{(1)}, b_0) -max(a_0, X_{(n)}-1)} \geq \frac{1}{2}
	\end{cases*}
\end{align*}	
	So the posterior median of $\theta$ is $\frac{min(X_{(1)}, b_0) + max(a_0, X_{(n)}-1)}{2} $.
	
\item[(b)] Suppose $X_1...X_n$ are i.i.d. $U(\theta_{1}, \theta_{2})$, where $(\theta_{1}, \theta_{2})$ are both unknown. Suppose that the joint prior for $(\theta_{1}, \theta_{2})$, where $\theta_1 \sim U(a_0, b_0), \theta_2 \sim U(a_1, b_1)$ for which $\theta_2 > \theta_1$. Derive joint posterior distribution of $(\theta_1, \theta_2)$ and derive the posterior mean and variance of $\theta_2 - \theta_1$.

As $\theta_1 \sim U(a_0, b_0), \theta_2 \sim U(a_1, b_1)$ for which $\theta_2 > \theta_1$. The $a_0, a_1, b_0, b_1$ are specified hyperparamters, while $a_1 > b_0$. 
So we have $\theta_2, \theta_1$ range as below 

\begin{align*}
	\pi(\theta_1, \theta_2) &= \pi(\theta_2|\theta_1) \pi(\theta_1) \\
	\pi(\theta_2| \theta_1) &= \frac{1}{b_1 - a_1} I \Big(  a_1 \leq \theta_2 \leq b_1 \Big) I \Big(   \theta_2 > \theta_1 \Big) \\
	\pi(\theta_1) &= \frac{1}{b_0 - a_0} I \Big( a_0 \leq \theta_1 \leq b_0 \Big) \\
	\pi(\theta_1, \theta_2) &= \frac{1}{b_1 - a_1} \frac{1}{b_0 - a_0} I \Big( a_0 \leq \theta_1 \leq b_0 \Big) I \Big(  a_1 \leq \theta_2 \leq b_1 \Big) I \Big(   \theta_2 > \theta_1 \Big)
\end{align*}	
let $ \theta_{1}^{U} = \min(b_0, X_{(1)}), \theta_{2}^{L} = \max(a_1, X_{(n)})$, then $\theta_1 \in (a_0, \theta_{1}^{U} , \theta_2 \in (\theta_{2}^{L}, b_1)$. Also $\theta_1 \leq \theta_2 $
The joint posterior distribution
\begin{align*}
	p(X|\theta_1, \theta_2) &= \prod_{i=1}^n \frac{1}{\theta_2-\theta_1} I(\theta_1 < x_i < \theta_2)\\
	p(\theta_2, \theta_1|X) &= \frac{p(X|\theta_1, \theta_2) p(\theta_1) p(\theta_2)}{p(X)} = \frac{p(X, \theta_1, \theta_2) }{p(X)} \\
	p(X, \theta_1, \theta_2) &= [\frac{1}{\theta_2-\theta_1}]^n \frac{1}{b_0-a_0} \frac{1}{b_1- a_1} I(X_{(1)} > \theta_1) I(X_{(n)} < \theta_2) I \Big(  \theta_1 \leq \theta_2 \leq b_1 \Big) I \Big(  a_0 \leq \theta_1 \leq b_0 \Big)  \\
	&= [\frac{1}{\theta_2-\theta_1}]^n \frac{1}{b_0-a_0} \frac{1}{b_1- a_1}  I \Big(\theta_1 \in (a_0, \theta_{1}^{U} ) \Big) I \Big(\theta_2 \in (\theta_{2}^{L}, b_1)\Big) 
\end{align*}	

The normalizing constant term C,
\begin{align*}
	p(X) &=\frac{1}{b_0-a_0} \frac{1}{b_1- a_1} \int_{a_0}^{\theta_{1}^{U}} \int_{\theta_{2}^{L}}^{b_1} [\frac{1}{\theta_2-\theta_1}]^n  d\theta_1 d\theta_2  \\
	& = \frac{1}{n-1} \frac{1}{2-n} \Big[ (b_1 - \theta_1^{U})^{-n+2} - (\theta_{2}^{L} - \theta_1^{U})^{-n+2} - (b_1 - a_0)^{-n+2} + (\theta_{2}^{L} - a_0)^{-n+2} \Big],\\
	& \theta_{1}^{U} = \min(b_0, X_{(1)}), \theta_{2}^{L} = \max(a_1, X_{(n)})
\end{align*}	

The joint posterior distribution of $\theta_1, \theta_2$

\begin{align*}
	p(\theta_2, \theta_1|X) &= \frac{  (n-1) (2-n)}{ (\theta_2-\theta_1)^n \Big[ (b_1 - \theta_1^{U})^{-n+2} - (\theta_{2}^{L} - \theta_1^{U})^{-n+2} - (b_1 - a_0)^{-n+2} + (\theta_{2}^{L} - a_0)^{-n+2} \Big]}\\
	& I \Big(\theta_1 \in (a_0, \theta_{1}^{U} ) \Big) I \Big(\theta_2 \in (\theta_{2}^{L}, b_1)\Big) 
\end{align*}	

The posterior mean and variance 
\begin{align*}
	C &=  \frac{  (n-1) (2-n)}{ \Big[ (b_1 - \theta_1^{U})^{-n+2} - (\theta_{2}^{L} - \theta_1^{U})^{-n+2} - (b_1 - a_0)^{-n+2} + (\theta_{2}^{L} - a_0)^{-n+2} \Big]}\\
	E(\theta_2- \theta_1|X) &=C \int_{a_0}^{\theta_{1}^{U}} \int_{\theta_{2}^{L}}^{b_1}\frac{ \theta_2-\theta_1}{ (\theta_2-\theta_1)^n } d\theta_1 d\theta_2 ,\\
	&= \frac{C}{(2-n)(3-n)} \Big[(\theta_2^L - \theta_1^U)^{3-n} - (\theta_2^L - a_0)^{3-n} - (b_1-\theta_1^U)^{3-n} + (b_1-a_0)^{3-n}\Big]\\
	E \Big((\theta_2- \theta_1)\|X \Big) &=C \int_{a_0}^{\theta_{1}^{U}} \int_{\theta_{2}^{L}}^{b_1}\frac{ (\theta_2-\theta_1)^2}{ (\theta_2-\theta_1)^n } d\theta_1 d\theta_2 ,\\
	&= \frac{C}{(4-n)(3-n)} \Big[(\theta_2^L - \theta_1^U)^{4-n} - (\theta_2^L - a_0)^{4-n} - (b_1-\theta_1^U)^{4-n} + (b_1-a_0)^{4-n}\Big]\\
	Var(\theta_2- \theta_1|X) & = E \Big((\theta_2- \theta_1)\|X \Big)- E(\theta_2- \theta_1|X)^2 \\
	&= \frac{C}{(4-n)(3-n)} \Big[(\theta_2^L - \theta_1^U)^{4-n} - (\theta_2^L - a_0)^{4-n} - (b_1-\theta_1^U)^{4-n} + (b_1-a_0)^{4-n}\Big] \\
	&- \frac{C^2}{(2-n)^2(3-n)^2} \Big[(\theta_2^L - \theta_1^U)^{3-n} - (\theta_2^L - a_0)^{3-n} - (b_1-\theta_1^U)^{3-n} + (b_1-a_0)^{3-n}\Big]^2
\end{align*}	

\end{itemize}



\section{Problem 6}
Find the density $p(x)$ from the Exercise on page 51 of the notes. The joint posterior density of $p(\mu, \tau|x)$ can also be obtained in this case by recognizing

\begin{align*}
	p(\mu, \tau|x)&= p(\mu| \tau, x) p(\tau|x), \qquad \text{normal} \times \text{gamma} 
\end{align*}	

Find $p(\mu | \tau, x), p(\tau | x), p(x)$. 

We could recognize the posterior distribution is a product of normal and gamma distribution. 

\begin{align*}
	p(\mu |\tau) & \sim N(\mu_0, \tau^{-1} \sigma_0^2) = \frac{1}{\sqrt{2\pi \tau^{-1} \sigma_0^2} } \exp(-\frac{(\mu-\mu_0)^2}{2 \tau^{-1} \sigma_0^2})\\
	p(\mu, \tau | x) & \propto \tau^{\frac{n + \delta_0 +1}{2} -1} \exp \Big[ -\frac{\tau}{2} \Big( \gamma_0 + \frac{ (\mu-\mu_0)^2}{\sigma_0^2} + \sum (x_i-\mu)^2 \Big) \Big]\\
	& \propto \tau^{\frac{n + \delta_0 +1}{2} -1} \exp(-\frac{\tau}{2} (\gamma_0 + \frac{\mu_0^2}{\sigma_0^2} + \sum_{i=1}^n x_i^2)) \exp(-\frac{\tau}{2} (\mu^2(n+\frac{1}{\sigma_0^2}) -2\mu(\sum_{i=1}^n x_i + \frac{\mu_0}{\sigma_0^2}))) 
\end{align*}	

In probability theory and statistics, the normal-gamma distribution (or Gaussian-gamma distribution) is a bivariate four-parameter family of continuous probability distributions. It is the conjugate prior of a normal distribution with unknown mean and precision.

\begin{align*}
	p(\mu ,\tau |x) & = \frac{\frac{ \gamma_0 + \frac{ (\mu-\mu_0)^2}{\sigma_0^2} + \sum (x_i-\mu)^2  }{2} ^{\frac{n + \delta_0 +1}{2} }}{\Gamma(\frac{n + \delta_0 +1}{2} )} \tau^{\frac{n + \delta_0 +1}{2} -1} \exp \Big[ -\frac{\tau}{2} \Big( \gamma_0 + \frac{ (\mu-\mu_0)^2}{\sigma_0^2} + \sum (x_i-\mu)^2 \Big) \Big]
\end{align*}

We also have 
\begin{align*}
	p(\mu| \tau,x)& \propto \tau^{\frac{n + \delta_0 +1}{2} -1} exp(-\frac{\tau}{2} (\gamma_0 + \frac{\mu_0^2}{\sigma_0^2} + \sum_{i=1}^n x_i^2)) \\
	p(\mu| \tau,x)& = \frac{[\frac{\gamma_0 + \frac{\mu_0^2}{\sigma_0^2} + \sum_{i=1}^n x_i^2}{2}]^{\frac{n + \delta_0 +1}{2}}}{\Gamma(\frac{n + \delta_0 +1}{2})} \tau^{\frac{n + \delta_0 +1}{2} -1} exp(-\frac{\tau}{2} (\gamma_0 + \frac{\mu_0^2}{\sigma_0^2} + \sum_{i=1}^n x_i^2)) \\
	p(\tau | x)& \propto  exp(-\frac{\tau}{2} (\mu^2(n+\frac{1}{\sigma_0^2}) -2\mu(\sum_{i=1}^n x_i + \frac{\mu_0}{\sigma_0^2}))) 
\end{align*}	

So we could derive $p(x)$ without integral

\begin{align*}
	p(\mu, \tau| x) p(x)&= p(\mu, \tau, x) = p(x| \mu, \tau) p(\mu|\tau) p(\tau) \\
	p(\mu, \tau, x) &= (2\pi)^{-\frac{n}{2}} \tau^{\frac{n}{2}} \exp(-\frac{\tau}{2} \sum(x_i - \mu)^2) (2\pi)^{-\frac{1}{2}} \tau^{\frac{1}{2}} \exp(-\frac{\tau}{2 \sigma_0^2} (\mu - \mu_0)^2) \frac{(\frac{\gamma_0}{2})^{\frac{\delta_0}{2}}}{\Gamma(\frac{\delta_0}{2})} \tau^{\frac{\delta_0}{2} -1} \exp(-\frac{\tau \gamma_0}{2})\\
	&= (2\pi)^{-\frac{n+1}{2}} \frac{(\frac{\gamma_0}{2})^{\frac{\delta_0}{2}}}{\Gamma(\frac{\delta_0}{2})} \tau^{\frac{n+\delta_0-1}{2}} \exp \Big( -\frac{\tau}{2} \sum(x_i -\mu)^2 -\frac{\tau}{2\sigma_0^2 }(\mu- \mu_0)^2 - \frac{\tau}{2}\gamma_0 \Big)\\
	p(x) &= \frac{p(\mu, \tau, x)}{p(\mu, \tau | x) } \\
	&= \frac{ (2\pi)^{-\frac{n+1}{2}} \frac{(\frac{\gamma_0}{2})^{\frac{\delta_0}{2}}}{\Gamma(\frac{\delta_0}{2})} \tau^{\frac{n+\delta_0-1}{2}} \exp \Big( -\frac{\tau}{2} \sum(x_i -\mu)^2 -\frac{\tau}{2\sigma_0^2 }(\mu- \mu_0)^2 - \frac{\tau}{2}\gamma_0 \Big)}{\frac{\frac{ \gamma_0 + \frac{ (\mu-\mu_0)^2}{\sigma_0^2} + \sum (x_i-\mu)^2  }{2} ^{\frac{n + \delta_0 +1}{2} }}{\Gamma(\frac{n + \delta_0 +1}{2} )} \tau^{\frac{n + \delta_0 +1}{2} -1} \exp \Big[ -\frac{\tau}{2} \Big( \gamma_0 + \frac{ (\mu-\mu_0)^2}{\sigma_0^2} + \sum (x_i-\mu)^2 \Big) \Big] } \\
	&= \frac{ (2\pi)^{-\frac{n+1}{2}} \frac{(\frac{\gamma_0}{2})^{\frac{\delta_0}{2}}}{\Gamma(\frac{\delta_0}{2})} \Gamma(\frac{n + \delta_0 +1}{2} )}{\frac{ \gamma_0 + \frac{ (\mu-\mu_0)^2}{\sigma_0^2} + \sum (x_i-\mu)^2  }{2} ^{\frac{n + \delta_0 +1}{2}}} 
\end{align*}	

\section{Problem 7}
Consider the model
\begin{align*}
	y_i &= \beta x_i + \epsilon_i
\end{align*}	
where the $\epsilon_i, i=1,..n$ are i.i.d. $N(0, \sigma^2)$ random variables and $(\beta, \sigma^2)$ are both unknown. Let $\tau = 1/\sigma^2$. Consider the joint improper prior
\begin{align*}
	\pi(\beta, \tau) & \propto \tau^{-1}
\end{align*}	

\begin{itemize}
	\item [(a)] Derive the joint posterior density of $(\beta, \tau)$.
	
	\begin{align*}
		\epsilon_i & \sim N(0, \sigma^2) \\
		y_i & \sim N(\beta x_i, \sigma^2) \\
		p(y_i|\beta, \sigma^2) &= \frac{1}{\sqrt{2\pi} \sigma} exp[-\frac{(y_i-\beta x_i)^2}{2\sigma^2}] \\
		\tau &= 1/\sigma^2, \qquad \pi(\beta, \tau) = \tau^{-1}
	\end{align*}	
	
	The joint posterior distribution of $(\beta, \tau)$ 
\begin{align*}
	p(\beta, \tau|X) &= \frac{p(x|\beta, \tau) p(\beta, \tau)}{p(x)} \\
	p(x|\beta, \tau) p(\beta, \tau) &= (\frac{\tau^{1/2}}{\sqrt{2\pi}})^n exp(-\frac{\tau \sum_{i=1}^n (y_i-\beta x_i)^2}{2}) \tau^{-1}
\end{align*}	

	The normalizing constant
		\begin{align*}
		p(X) &= \int_{0}^{\infty} \int_{-\infty}^{\infty} (\frac{\tau^{1/2}}{\sqrt{2\pi}})^n exp(-\frac{\tau \sum_{i=1}^n (y_i-\beta x_i)^2}{2}) \tau^{-1} d\beta d\tau \\
		&=  \int_{0}^{\infty} \int_{-\infty}^{\infty} \frac{\tau^{n/2-1}}{(2\pi)^{n/2}} exp[-\frac{\tau \sum_{i=1}^n x_i^2}{2} (\beta^2 -2 \beta \frac{\sum x_i y_i}{\sum x_i^2} ) - \frac{\tau}{2} \sum y_i^2] d\beta d\tau \\
		&= \int_{0}^{\infty} \frac{\tau^{n/2-1-1/2}}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2}} exp[-\frac{\tau}{2} \sum y_i^2 + \frac{\tau (\sum x_iy_i)^2}{2 \sum x_i^2}] \\
		& \int_{-\infty}^{\infty} \frac{(\tau \sum x_i^2)^{1/2}}{\sqrt{2\pi}} exp[-\frac{\tau \sum x_i^2 (\beta - \frac{\sum x_i y_i}{\sum x_i^2})^2}{2}] d\beta d\tau \\
		&= \int_{0}^{\infty} \frac{\tau^{n/2-1-1/2}}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2}} exp[-\frac{\tau}{2} \sum y_i^2 + \frac{\tau (\sum x_iy_i)^2}{2 \sum x_i^2}] d\tau\\
		&= \frac{1}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2}} \int_{0}^{\infty} \tau^{n/2-1-1/2} exp[-\frac{\tau}{2} (\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})] d\tau \\
		&=  \frac{\Gamma((n-1)/2)}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2} [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{(n-1)/2}} \\
		& \int_{0}^{\infty} \frac{[\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{(n-1)/2}}{\Gamma((n-1)/2)} \tau^{n/2-1-1/2} exp[-\frac{\tau}{2} (\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})] d\tau \\
		&= \frac{\Gamma \Big( \frac{n-1}{2} \Big)}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2} \Big [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2} \Big ]^{(n-1)/2}}
	\end{align*}	

So the joint posterior distribution
\begin{align*}
	p(\beta, \tau|X) &= \frac{({\tau^{n/2-1}}) \exp(-\frac{\tau \sum_{i=1}^n (y_i-\beta x_i)^2}{2}) [\sum x_i^2]^{1/2} \Big [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2} \Big ]^{(n-1)/2}}{ 2\pi \Gamma \Big( \frac{n-1}{2} \Big)} \\
	& = C  {\tau^{\frac{n}{2}-1}} \exp \Big(-\frac{\tau \sum_{i=1}^n (y_i-\beta x_i)^2}{2} \Big), \\
	 C &= (2\pi)^{-\frac{n}{2}} \Big( \sum x_i^2\Big)^{\frac{1}{2}} \Big [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2} \Big ]^{\frac{n-1}{2}}
\end{align*}	

\item[(b)] Derive the marginal posterior density of $\beta$ and mean and variance.

The marginal distribution of $\beta$ 

\begin{align*}
	p(\beta|X) &= \int_{0}^{\infty} p(\beta, \tau|X) d \tau \\
	& \propto C \int_{0}^{\infty}  {\tau^{n/2-1}} exp \Big(-\frac{\tau}{2} \sum_{i=1}^n (y_i-\beta x_i)^2 \Big) d\tau \\
	& \propto \Biggl\{(\sum x_i^2) \Big[ \beta - (\sum x_i^2)^{-1} (\sum x_i y_i)\Big]^2 + \sum y_i^2 - (\sum x_i^2)^{-1} (\sum x_i y_i)^2 \Biggr\}^{-\frac{n}{2}}
\end{align*}	

let 
\begin{align*}
	a &= (\sum x_i^2)^{-1} (\sum x_i y_i) \\
	b&= \sum y_i^2 - (\sum x_i^2)^{-1} (\sum x_i y_i)^2 \\
	d &= (\sum x_i^2)
\end{align*}	

then 
\begin{align*}
	p(\beta|X) & \propto \Biggl\{d \Big[ \beta - a\Big]^2 + b \Biggr\}^{-\frac{n}{2}} \\
	& \propto \Big[ 1 + \frac{d(\beta - a)^2}{b}  \Big]^{-\frac{n}{2}}
\end{align*}	

So $\beta|X \sim$ t-distribution with location parameter $a$, dispersion parameter $\frac{b}{(n-1)d}$, and $(n-1)$ degrees of freedom

\begin{align*}
	p(\beta|X) & = \frac{\Gamma(\frac{n}{2}) \pi^{-1/2} (b/d)^{-1/2}}{\Gamma(\frac{n-1}{2})} \Big[ 1 + \frac{d(\beta - a)^2}{b}  \Big]^{-\frac{n}{2}}
\end{align*}	


posterior mean and variance of $\beta$
\begin{align*}
	E(\beta|X) &= a = (\sum x_i^2)^{-1} (\sum x_i y_i)\\
	Var(\beta|X)&= \frac{v}{v-2} \Sigma = \frac{n-1}{n-3} \frac{\sum y_i^2 - (\sum x_i^2)^{-1} (\sum x_i y_i)^2 }{\sum x_i^2}
\end{align*}	

\item[(c)] Derive the marginal posterior density and calculate the posterior mean and variance.

The marginal distribution of $\tau$ 

\begin{align*}
	p(\tau |X) &= \int_{0}^{\infty} p(\beta, \tau |X) d \beta \\
	& \propto 2 C \int_{0}^{\infty}  {\tau^{n/2-1}} \exp \Big(-\frac{\tau}{2} \sum_{i=1}^n (y_i-\beta x_i)^2 \Big) d\beta \\
	& \propto 2C \tau^{\frac{n}{2}-1} \exp(-\frac{\tau}{2} \sum y_i^2) \int_0^{\infty} \exp \Big( -\frac{\tau}{2} \beta^2 \sum x_i^2 + \tau \beta \sum x_i y_i \Big) d\beta \\
	& \propto 2C \tau^{\frac{n}{2}-1} \exp(-\frac{\tau}{2} \sum y_i^2) \exp \Big( \frac{\tau}{2} \frac{(\sum x_i y_i)^2}{\sum x_i^2} \Big) \int_0^{\infty} \exp \Big( -\frac{\tau}{2} \Big[ \beta - \frac{\sum x_i y_i}{\sum x_i^2} \Big]^2 \Big) d\beta \\
	& \propto  C \tau^{\frac{n-1}{2}-1} \exp \Big(-\frac{\tau}{2} \Big[ \sum y_i^2 - \frac{(\sum x_i y_i)^2}{\sum x_i^2}\Big] \Big) 
\end{align*}

So $\tau|X \sim gamma \Big(\frac{n-1}{2}, \frac{1}{2} \Big[ \sum y_i^2 - \frac{(\sum x_i y_i)^2}{\sum x_i^2}\Big] \Big)$.

The posterior mean and variance

\begin{align*}
	E(\tau |X) &= \frac{n-1 }{ \sum y_i^2 - \frac{(\sum x_i y_i)^2}{\sum x_i^2}} \\
	Var(\tau |X) &= \frac{2(n-1) }{ \Big[ \sum y_i^2 - \frac{(\sum x_i y_i)^2}{\sum x_i^2}\Big]^2}
\end{align*}

\end{itemize}

