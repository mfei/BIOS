
\section{Problem 1}
Derive all of the full conditional distibution for $\beta, \sigma^2, \gamma$ for the variable subset selection method of George and McCulloch.

Suppose we have the following linear model

\begin{align*}
	Y & = X\beta + \epsilon, \qquad \epsilon \sim N_n(0, \sigma^2 I)
 \end{align*}
Consider a prior for each $\beta_i, \beta= (\beta_1,..\beta_p)'$ to be a mixture of two normal densities, and thus

\begin{align*}
	\beta_i| \gamma_i & \sim (1- \gamma_i) N(0, \tau_i^2) + \gamma_i N(0, c_i^2 \tau_i^2)
 \end{align*}

where $\gamma_i$ is a binary random variable with $p(\gamma_i=1) = 1- p(\gamma_i=0) = p$

The mixture prior for $\beta_i | \gamma_i$ can be written in vector form as 
\begin{align*}
	\beta| \gamma &  N_p \big( 0, D_{\gamma} R D_{\gamma} \big) 
 \end{align*}
where $\gamma = (\gamma_1, .. \gamma_p)$, R is the prior correlation matrix and 
\begin{align*}
	D_{\gamma} &= \diag \big( a_1 \tau_1, .. , a_p \tau_p \big) 
 \end{align*}
where $ a_i = 1$ if $\gamma_i = 0$ and $ a_i = c_i $ if $\gamma_i = 1$.

The prior for $\sigma^2$, 
\begin{align*}
 \sigma^2 | \gamma & \sim IG \Big( \frac{v_{\gamma}}{2}, \frac{v_{\gamma} \lambda_{\gamma}}{2} \Big)
 \end{align*}
The prior for $\gamma$ can be taken as 
\begin{align*}
 f(\gamma) &= \prod_{i=1}^p p_i^{\gamma_i} (1- p_i)^{1- \gamma_i}
 \end{align*}

The posterior distribution of $\beta, \sigma^2$ and $\gamma$:
\begin{align*}
 f(\beta, \sigma^2, \gamma | Y) &=f(Y| \beta, \sigma^2, \gamma) f(\beta) f(\sigma^2) f(\gamma)\\
&\propto (\sigma^2)^{-n/2} \exp \big(-\frac{(Y- X \beta)'(Y- X \beta)}{2 \sigma^2} \big)  N_p \big( 0, D_{\gamma} R D_{\gamma} \big)\\
& IG \Big( \frac{v_{\gamma}}{2}, \frac{v_{\gamma} \lambda_{\gamma}}{2} \Big) \prod_{i=1}^p p_i^{\gamma_i} (1- p_i)^{1- \gamma_i}
 \end{align*}

The full conditional distribution of $\beta$
\begin{align*}
 f(\beta |\sigma^2, \gamma, Y) &\propto  \exp \big(-\frac{(Y- X \beta)'(Y- X \beta)}{2 \sigma^2} \big) 
 \end{align*}

The full conditional distribution of $\sigma^2$ 
\begin{align*}
 f(\sigma^2 |\beta, \gamma, Y) &\propto (\sigma^2)^{-n/2} \exp \big(-\frac{(Y- X \beta)'(Y- X \beta)}{2 \sigma^2} \big)  
 \end{align*}

The full conditional distribution of $\gamma$ 
\begin{align*}
 f(\gamma |\beta, \sigma^2, Y) &\propto   N_p \big( 0, D_{\gamma} R D_{\gamma} \big) IG \Big( \frac{v_{\gamma}}{2}, \frac{v_{\gamma} \lambda_{\gamma}}{2} \Big) \prod_{i=1}^p p_i^{\gamma_i} (1- p_i)^{1- \gamma_i}
 \end{align*}

\section{Problem 2}
Consider a general regression model, and let $p(y_i | \theta, x_i)$ denote the sampling density of $y_i$ for case $i$ and $x_i$ is the $p \times 1$ vector of covariates for subject i. Assume that the observations $y_i$ are independent for $i=1,..n$. Let $p(\theta| y, X)$ denote the posterior density of $\theta$ from this model, where $\theta$ is $p \times 1$, X is $n \times p$, and $y$ is $n \times 1$. Show that 
\begin{align*}
 CPO_i &= \Big \{ E_{\theta|y, X} \big( \frac{1}{p(y_i | \theta, x_i)} \big) \Big \}^{-1}
 \end{align*}

Conditional Predictive Ordinate (CPO) statistic for outlier detection, is the predictive density of the ith observation given $(y_{(-i)}, X_{(-i)})$, evaluated at $y_i$. That is
  \begin{align*}
  CPO_i & = p \big(z_i | y_{(-i)}, X_{(-i)} \big) \big |_{z_i = y_i} \\
&=  \int  \frac{p(y_i | \theta, X_i) p(\theta | y_{(-i)}, X_{(-i)})}{p(y, X)} d\theta 
 \end{align*}

By Baye's theorem,

  \begin{align*}
\int \frac{1}{p(y| \theta)} p(\theta| y)  d\theta &= \int \frac{1}{p(y| \theta)} \frac{p(y|theta) p(\theta)}{p(y)} d\theta \\
&= \frac{1}{p(y)} \int p(\theta) d\theta = \frac{1}{p(y)}
 \frac{1}{p(y)} & = \int \frac{p(\theta| y)}{p(y| \theta)} d\theta\\
&= E_{\theta} \big \{ \frac{1}{p(y | \theta)} | y \big \}
 \end{align*}
 
 Then $CPO_i$, the predictive distribution of $y_i | X_{(i)}, Y_{(i)}$,
 
\begin{align*}
 \frac{1}{CPO_i} & =  E_{\theta|y} \big \{ \frac{1}{p(y_i | \theta)} \big \}\\
&= E_{\theta|y, X} \big[\frac{1}{p(y_i | \theta)} \big] \\
CPO_i &= \Big \{ E_{\theta|y, X} \big( \frac{1}{p(y_i | \theta, x_i)} \big) \Big \}^{-1}
\end{align*}
 
\section{Problem 3}
We consider data on $n=136$ patients from a liver cancer clinical trial. We are interested in how the number of cancerous liver nodes $(y)$ when entering the trial is predicted by six other baseline characteristics: body mass index $(x_1)$, age in years $(x_2)$, time since diagnosis of the disease in weeks $(x_3)$, two biochemical markers $(0-abnomal, 1- normal)$: Alpha fetoprotein $(x_4)$, and Anti Hepatitis B antigen $(x_5)$, and associated jaundice $(x_6, 1- yes, 0-no)$. In all the computations, standardize all covariates. We consider a Poisson regression model (with a canonical link) for the data, with an intercept and covariates $(z_1, z_2,... z_6)$, and thus our regression coefficient vector is $\beta = (\beta_0, \beta_1,.. \beta_6)$. 

\begin{itemize}
\item[(a)] Write out the likelihood function of $\beta$ for this model. 
The poisson regression model
\begin{align*}
f(y|\lambda) &= \frac{\lambda^y e^{-\lambda}}{y!} \\
log f(y) &=y log \lambda  - \lambda - log y!
\end{align*}

The canonical link is $ log \lambda=  X\beta$

\begin{align*}
b(\theta) &= \lambda = e^{X \beta} \\
log f(y_i) &= y_i X_i \beta -  e^{X_i \beta}  - log y! \\
l(\beta) &= \sum_{i=1}^n y_i x_i \beta - \sum_{i=1}^n e^{X_i \beta}
\end{align*}

\item[(b)] Compute the maximum likelihood estimate of $\beta$ and the standard errors of the estimates. 

The maximum likelihood estimate of $\beta$, there is no closed form solution for $\hat{\beta}$, and we can use R.
\begin{align*}
\partial_{\beta} l(\beta) &= \sum_{i=1}^n y_i x_i  - \sum_{i=1}^n e^{x_i \beta} x_i  = 0 \\
\partial_{\beta}^2 l(\beta) &= - \sum_{i=1}^n e^{x_i \beta} x_i x_i^T \\
I_n(\beta) &= - E \big[ \partial_{\beta}^2 l(\beta) \big] = E \big[\sum_{i=1}^n e^{x_i \beta} x_i x_i^T \big] \\
Var(\hat{\beta}) &= \frac{1}{I_n(\hat{\beta})}
\end{align*}

\item[(c)] Consider the conjugate prior for $\beta$ with $a_0 = 1$ and $y_0 = (1,...1)$. Use stan to obtain samples from the posterior distribution of $\beta$. 

A conjugate prior for $\beta$ is 
\begin{align*}
\theta & \sim D(a_0, y_0) \\
\pi(\theta) & \sim \exp \big( a_0 (\theta y_0 - b(\theta)) \big) 
\end{align*}

Then the likelihood function and prior function $\pi(\beta)$
\begin{align*}
p(y| \beta) & \propto \exp \big( \sum_{i=1}^n y_i x_i \beta - \sum_{i=1}^n e^{X_i \beta} \big) \\
\pi(\beta) &= \exp \big(a_0 \sum_{i=1}^n y_i x_i \beta - a_0 \sum_{i=1}^n e^{X_i \beta} \big) 
\end{align*}


\begin{itemize}
\item[(i)] Compute the posterior mean and covariance matrix of $\beta$.

\item[(ii)] the $2.5\%, 50\%, 97.5\%$ posterior percentiles of $(\beta_0, ..\beta_6)$.

\item[(iii)] Use stan to plot the marginal posterior distributions of $\beta$.

\item[(iv)] Use stan to assess convergence diagnositics.

\end{itemize}

\item[(d)] Repeat part (c) using a uniform improper prior for $\beta$. Run both Stan and the Bayes procedure in SAS GENMOD to summarize the results. 


\end{itemize}

