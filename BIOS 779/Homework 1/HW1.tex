% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[thinc]{esdiff}
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{BIOS 779 HW1}
\author{Mingwei Fei}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{Problem 1- Carlin and Louis}
Diagnostic test, represent $P(D=d|T=t)$ in terms of test sensitivity, $P(T=1|D=1)$. Specificity, $P(T= 0|D= 0)$. and disease prevalence, $P(D=1)$, and relate to Bayes' theorem.
 
\begin{align*}
	P(D=d|T=t) &= \frac{P(D=d, T=t)}{P(T=t)} \\
	&= \frac{P(D=d, T=t)}{\sum_{D=d} P(T=t, D=d) P(D=d)} \\
	&= \frac{P(T=t|D=d) P(D=t)}{P(T=t, D=0)P(D=0) + P(T=t, D=1)P(D=1)}\\
	&= \frac{P(T=t|D=d) P(D=d)}{P(T=t|D=0)P(D=0)^2 + P(T=t|D=1)P(D=1)^2} \\
\end{align*}

So we have

\begin{align*}
P(D= 1|T= 1) &= \frac{P(T= 1|D= 1) P(D=1)}{P(T=1|D=0)(1- P(D=1))^2 + P(T=1|D=1)P(D=1)^2} \\
&=  \frac{P(T= 1|D= 1) P(D=1)}{(1- P(T=0|D=0))(1- P(D=1))^2 + P(T=1|D=1)P(D=1)^2} \\
P(D= 0|T= 1) &= \frac{P(T= 1|D= 0) (1-P(D=1))}{P(T=1|D=0)(1- P(D=1))^2 + P(T=1|D=1)P(D=1)^2} \\
&=  \frac{(1- P(T=0|D=0)) (1-P(D=1))}{(1- P(T=0|D=0))(1- P(D=1))^2 + P(T=1|D=1)P(D=1)^2} \\
P(D= 0|T= 0) &= \frac{P(T= 0|D= 0) (1-P(D=1))}{P(T=0|D=0)(1- P(D=1))^2 + P(T=0|D=1)P(D=1)^2} \\
&=  \frac{P(T=0|D=0) (1-P(D=1))}{ P(T=0|D=0)(1- P(D=1))^2 + (1- P(T=1|D=1)) P(D=1)^2} \\
P(D= 1|T= 0) &= \frac{P(T= 0|D= 1) P(D=1)}{P(T=0|D=0)(1- P(D=1))^2 + P(T=0|D=1)P(D=1)^2} \\
&=  \frac{ (1- P(T=1|D=1)) P(D=1)}{ P(T=0|D=0)(1- P(D=1))^2 + (1- P(T=1|D=1)) P(D=1)^2} \\
\end{align*}

\section{Problem 2}
Suppose $X_1, .., X_n$ is a random sample from X, where X has density 

\begin{align}
	p(x|r, \theta) &= \begin{cases} 
		{r+x-1 \choose x} \theta^{r} (1-\theta)^{x} & x=0,1,2...\\
	     0 & \text{otherwise}
	\end{cases}
\end{align}

where $0< \theta < 1, r$ is known, and $\theta$ is unknown.

\begin{itemize}
	\item [(a)] Derive Jeffreys's prior for $\theta$. 
	
\begin{align*}
	p(\theta) & \propto \sqrt{det I(\theta)} \\
	log p(x|r, \theta) &= log {r+x-1 \choose x} + rlog \theta + x log(1-\theta) \\
	\diffp{log p(x|r, \theta)}{\theta} &= \frac{r}{\theta} - \frac{x}{1-\theta} \\
	\diffp{log p(x|r, \theta)}{\theta \theta} &= -\frac{r}{\theta^2} - \frac{x}{(1-\theta)^2} \\
	I(\theta) &= -E[\diffp{log p(x|r, \theta)}{\theta \theta} ] = \frac{r}{\theta^2} + \frac{E[X]}{(1-\theta)^2}
\end{align*}	
	
	The $p(x|r, \theta)$ is negative binomial distribution, which we have 
\begin{align*}
	E(X) &= \frac{r(1-\theta)}{\theta} \\
	I(\theta) &= \frac{r}{\theta^2 (1-\theta)}
\end{align*}	
Thus, Jeffery's prior is 

\begin{align*}
	\pi(\theta) & \propto  I(\theta)^{1/2} = [\frac{r}{\theta^2 (1-\theta)}]^{1/2}
\end{align*}	
	
	\item[(b)] Suppose the prior for $\theta$ is $\theta \sim beta(\alpha_0, \lambda_0)$, where $\alpha_0, \lambda_0$ are specified hyperparameters. Derive $E(X)$ and $Var(X)$.
	
	\begin{align*}
		\pi(\theta)	&= \frac{1}{B(\alpha_0, \lambda_0)} \theta^{\alpha_0 -1} (1-\theta)^{\lambda_0 -1} 
	\end{align*}	
	
	While from part (a), we know that $x|\theta \sim NB(r, \theta)$, then 
	
	\begin{align*}
		E(X|\theta) &= \frac{r(1-\theta)}{\theta} \\
		Var(X|\theta) &=  \frac{r(1-\theta)}{\theta^2} 
	\end{align*}
So
\begin{align*}
	E(X) &= E[E(X|\theta)] = E[\frac{r(1-\theta)}{\theta} ]\\
	&= \int_{0}^{1} \frac{r(1-\theta)}{\theta} \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \theta^{\alpha_0-1} (1-\theta)^{\lambda_0 -1} d\theta \\
	&=  r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)}  \int_{0}^{1}  \theta^{\alpha_0-2} (1-\theta)^{\lambda_0} d\theta \\
	&= r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0 -1) \Gamma(\lambda_0+1) }{\Gamma(\alpha_0 + \lambda_0)} \int_{0}^{1} \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0-1) \Gamma(\lambda_0 +1)}  \theta^{\alpha_0-2} (1-\theta)^{\lambda_0} d\theta \\
	&= r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0 -1) \Gamma(\lambda_0+1) }{\Gamma(\alpha_0 + \lambda_0)} \\
	&= \frac{r \lambda_0}{\alpha_0 -1} , \qquad \alpha_0 > 1
\end{align*}
For variance,
\begin{align*}
	Var(X) &= E[Var(X|\theta)] + Var[E[X|\theta]]  \\
	&= E[\frac{r(1-\theta)}{\theta^2} ] + Var[\frac{r(1-\theta)}{\theta} ]\\
	E[\frac{r(1-\theta)}{\theta^2} ] 
	&= \int_{0}^{1} \frac{r(1-\theta)}{\theta^2} \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \theta^{\alpha_0-1} (1-\theta)^{\lambda_0 -1} d\theta \\
	&=  r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)}  \int_{0}^{1}  \theta^{\alpha_0-3} (1-\theta)^{\lambda_0} d\theta \\
	&= r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0 -2) \Gamma(\lambda_0+1) }{\Gamma(\alpha_0 + \lambda_0 -1)} \int_{0}^{1} \frac{\Gamma(\alpha_0 + \lambda_0-1)}{\Gamma(\alpha_0-2) \Gamma(\lambda_0 +1)}  \theta^{\alpha_0-3} (1-\theta)^{\lambda_0} d\theta \\
	&= r \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0 -2) \Gamma(\lambda_0+1) }{\Gamma(\alpha_0 + \lambda_0-1)} \\
	&= \frac{r \lambda_0 (\alpha_0 + \lambda_0 -1)}{(\alpha_0 -1) (\alpha_0 -2)} , \qquad \alpha_0 > 2\\
	E[[\frac{r(1-\theta)}{\theta}] ^2]
	&= \int_{0}^{1} [\frac{r(1-\theta)}{\theta} ]^2\frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \theta^{\alpha_0-1} (1-\theta)^{\lambda_0 -1} d\theta \\
	&= r^2 \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)}  \int_{0}^{1}  \theta^{\alpha_0-3} (1-\theta)^{\lambda_0 +1} d\theta \\
	&= r^2 \frac{\Gamma(\alpha_0 + \lambda_0)}{\Gamma(\alpha_0) \Gamma(\lambda_0)} \frac{\Gamma(\alpha_0-2) \Gamma(\lambda_0-2) }{\Gamma(\alpha_0+ \lambda_0)}  \int_{0}^{1}  \frac{\Gamma(\alpha_0+ \lambda_0)}{\Gamma(\alpha_0-2) \Gamma(\lambda_0-2) } \theta^{\alpha_0-3} (1-\theta)^{\lambda_0 +1} d\theta \\
	&= \frac{r^2 \lambda_0 (\lambda_0+1)}{(\alpha_0-1 )(\alpha_0-2)} , \qquad \alpha_0 > 2
\end{align*}
So we have 
\begin{align*}
	Var(\frac{r(1-\theta)}{\theta} ) &= E(\frac{r(1-\theta)}{\theta} )^2 - [E(\frac{r(1-\theta)}{\theta} )  ]^2 \\
	&= \frac{r^2 \lambda_0 (\lambda_0+1)}{(\alpha_0-1 )(\alpha_0-2)}  - [\frac{r \lambda_0}{\alpha_0 -1}]^2 \\
	&= \frac{r^2 \lambda_0 (\lambda_0 + \alpha_0-1)}{(\alpha_0-1)^2 (\alpha_0-2)} ,\qquad \alpha_0 > 2
\end{align*}
Thus,
\begin{align*}
	Var(X) &= E[\frac{r(1-\theta)}{\theta^2} ] + Var(\frac{r(1-\theta)}{\theta} ) \\
	&=\frac{r \lambda_0 (\alpha_0 + \lambda_0 -1)}{(\alpha_0 -1) (\alpha_0 -2)}   + \frac{r^2 \lambda_0 (\lambda_0 + \alpha_0-1)}{(\alpha_0-1)^2 (\alpha_0-2)} \\
	&= \frac{r \lambda_0 (\alpha_0 + \lambda_0 -1) (\alpha_0 + r -1)}{(\alpha_0-1)^2 (\alpha_0-2)}, \qquad \alpha_0 > 2
\end{align*}

	\item[(c)] Suppose the prior for $\theta$ is $\theta \sim beta(\alpha_0, \lambda_0)$, where $\alpha_0, \lambda_0$ are specified hyperparameters. Derive the posterior distribution of $\theta$.
	
	\begin{align*}
	\pi(\theta | X) & \propto \frac{\Gamma(\alpha_0 + \lambda_0) }{\Gamma(\alpha_0) \Gamma(\lambda_0)} \theta^{\alpha_0-1} (1-\theta)^{\lambda_0-1} \prod_{i=1}^n {r+x_i-1 \choose x_i} \theta^{r} (1-\theta)^{x_i} , \qquad \theta \in (0,1) 
\end{align*}

because beta distribution is a conjugate of the negative binomial distribution, we have the posterior distribution of $\theta|X$ also a negative binomial

	\begin{align*}
	\pi(\theta | X) & = {nr+\alpha_0+ \sum_i x_i + \lambda_0-3 \choose \sum_i x_i + \lambda_0-1} \theta^{nr + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0-1}
\end{align*}

	\item[(d)] Suppose the prior for $\theta$ is $\theta \sim beta(\alpha_0, \lambda_0)$, where $\alpha_0, \lambda_0$ are specified hyperparameters. Derive the posterior predictive distribution of $z=(z_1, z_2)$, where conditional on $\theta$, $z_1, z_2$ are i.i.d with distribution of (1).
	
	The predictive distribution 
	
	\begin{align*}
		p(z | X) & = \int p(z| \theta) p(\theta | X) d \theta
	\end{align*}

As $z=(z_1, z_2)$, where conditional on $\theta$, $z_1, z_2$ are i.i.d with distribution of (1), so

\begin{align*}
	p(z | \theta) & = p(z_1| \theta) p(z_2|\theta) = {r+z_1-1 \choose z_1} {r+z_2-1 \choose z_2} \theta^{2r} (1-\theta)^{z_1+z_2} \\
	p(z | X) & = \int_{0}^{1} {r+z_1-1 \choose z_1} {r+z_2-1 \choose z_2} \theta^{2r} (1-\theta)^{z_1+z_2}  {nr+\alpha_0+ \sum_i x_i + \lambda_0-3 \choose \sum_i x_i + \lambda_0-1} \\
	& \theta^{nr + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0-1} d \theta\\
	&= {r+z_1-1 \choose z_1} {r+z_2-1 \choose z_2} {nr+\alpha_0+ \sum_i x_i + \lambda_0-3 \choose \sum_i x_i + \lambda_0-1} \\
	& \int_{0}^{1}  \theta^{2r} (1-\theta)^{z_1+z_2} \theta^{nr + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0-1} d \theta
\end{align*}

We can construct the negative binomial distribution for
\begin{align*}
	& \int_{0}^{1}  \theta^{2r} (1-\theta)^{z_1+z_2} \theta^{nr + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0-1} d \theta \\
	&= \frac{1}{A} \int_{0}^{1} {(n+2) r + \alpha_0 + \sum_i x_i + \lambda_0 + z_1+z_2 -3 \choose \sum_i x_i + \lambda_0-1} \theta^{(n+2) r + \alpha_0-1} (1-\theta)^{\sum_i x_i + \lambda_0 + z_1+z_2 -1} d \theta 
\end{align*}
let $A = {(n+2) r + \alpha_0 + \sum_i x_i + \lambda_0 + z_1+z_2 -3 \choose \sum_i x_i + \lambda_0-1}$, so

\begin{align*}
	p(z | \theta) & =  {r+z_1-1 \choose z_1} {r+z_2-1 \choose z_2} {nr+\alpha_0+ \sum_i x_i + \lambda_0-3 \choose \sum_i x_i + \lambda_0-1} \frac{1}{{(n+2) r + \alpha_0 + \sum_i x_i + \lambda_0 + z_1+z_2 -3 \choose \sum_i x_i + \lambda_0-1}}
\end{align*}

\end{itemize}

\section{Problem 3}
We have distribution 
\begin{align}
	\pi(\theta) &= \begin{cases} 
		\frac{\beta^{\alpha} }{\Gamma(\alpha)} \theta^{-(\alpha + 1)} exp(-\frac{\beta}{\theta}), & \qquad \theta > 0 \\
		0 & \qquad \theta \leq 0
		\end{cases} 
	\label{eq:1}
\end{align}

\begin{itemize}
	\item [(a)] Verify that $\pi(\theta)$ is a probability density function.
	
	We need to show that $\int \pi(\theta) d\theta = 1$.
	\begin{align*}
	\int_{0}^{\infty}	\pi(\theta) d \theta &= 
		\int_{0}^{\infty}	\frac{\beta^{\alpha} }{\Gamma(\alpha)} \theta^{-(\alpha + 1)} exp(-\frac{\beta}{\theta}) d \theta \\
		\text{let} x &= \frac{1}{\theta}, J = |\diffp{\theta}{x}| = \frac{1}{x^2} \\
		&= \int_{0}^{\infty} \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha + 1} exp(-\beta x) \frac{1}{x^2} dx \\
		&= \int_{0}^{\infty} \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha -1} exp(-\beta x)  dx \\
		&= 1
	\end{align*}
	
	\item[(b)] Consider family of prior distribution, show it is a conjugate prior for normal family with a known value of mean $\mu$, and unknown value of variance $\theta$. 
	
	 We need to show that the posterior distribution of $p(\theta|x)$ is also a gamma distribution, which follows the distribution $(\ref{eq:1})$.
	 
	\begin{align*}
		p(x_i|\mu, \theta) &= \frac{1}{\sqrt{2 \pi \theta}} exp(- \frac{(x_i-\mu)^2}{2 \theta} )\\
		p(\theta|X) &= \frac{\prod_{i=1}^n f(x_i| \mu, \theta) f(\theta)}{f(X)} \\
		&= \frac{ (2\pi \theta)^{-n/2} exp(-\frac{\sum_{i=1} (x_i-\mu)^2}{2\theta}) \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-(\alpha + 1)} exp(-\frac{\beta}{\theta}) }{f(X)} 
	\end{align*} 
To calculate $f(X)= \int f(x|\theta) f(\theta) d \theta$,
\begin{align*}
	f(X)&=  \int_{0}^{\infty} (2\pi \theta)^{-n/2} exp(-\frac{\sum_{i=1} (x_i-\mu)^2}{2\theta}) \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{-(\alpha + 1)} exp(-\frac{\beta}{\theta}) d \theta \\	
	&= \int_{0}^{\infty} (2\pi )^{-n/2} \frac{\beta^{\alpha}}{\Gamma(\alpha)}  exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2\theta}) \theta^{-(\alpha + 1 + \frac{n}{2})}  d \theta \\	
	& \text{let} y= \frac{1}{\theta}, \qquad J=|\diffp{\theta}{y}| = \frac{1}{y^2} \\
	&= \int_{0}^{\infty} (2\pi )^{-n/2} \frac{\beta^{\alpha}}{\Gamma(\alpha)}  exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2} y) y^{(\alpha + 1 + \frac{n}{2})} y^{-2} d y \\	
	&= \int_{0}^{\infty} (2\pi )^{-n/2} \frac{\beta^{\alpha}}{\Gamma(\alpha)}  exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2} y) y^{(\alpha + \frac{n}{2})-1}  d y 
\end{align*} 
Further,
\begin{align*}
	f(X) &= (2\pi )^{-n/2}  \frac{\beta^{\alpha}}{\Gamma(\alpha)} \frac{\Gamma(\alpha + \frac{n}{2})}{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}\\
	& \int_{0}^{\infty} \frac{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}{\Gamma(\alpha + \frac{n}{2})} exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2} y) y^{(\alpha + \frac{n}{2})-1}  d y \\	
	&=  (2\pi )^{-n/2}  \frac{\beta^{\alpha}}{\Gamma(\alpha)} \frac{\Gamma(\alpha + \frac{n}{2})}{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}
\end{align*}
Then the posterior distribution
\begin{align*}
	p(x_i|\mu, \theta) &=  \frac{ (2\pi )^{-n/2} \frac{\beta^{\alpha}}{\Gamma(\alpha)}  exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2\theta}) \theta^{-(\alpha + 1 + \frac{n}{2})}   }{(2\pi )^{-n/2}  \frac{\beta^{\alpha}}{\Gamma(\alpha)} \frac{\Gamma(\alpha + \frac{n}{2})}{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}} \\
	&= \frac{[\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2}]^{\alpha + \frac{n}{2}}}{\Gamma(\alpha + \frac{n}{2})} exp(-\frac{\sum_{i=1} (x_i-\mu)^2 + 2 \beta}{2\theta}) \theta^{-(\alpha + 1 + \frac{n}{2})}
\end{align*} 

The above distribution follows the distribution $(\ref{eq:1})$, so gamma distribution is a conjugate prior for a normal family with unknown variance.

\end{itemize}

\section{Problem 4}
Suppose that the lifetime X of a medical device follows a shifted exponential distribution

\begin{align*}
	p(x|\theta, \mu) &= \theta exp(-\theta (x-\mu)) I(x> \mu)
\end{align*}

where $\theta > 0, -\infty < \mu < \infty$. Suppose that $X_1, X_2, ... X_n$ is a random sample from X.

\begin{itemize}
	\item[(a)]  Derive the posterior mean and variance.

	As we know that the gamma distribution is a conjugate prior for exponential distribution. So the posterior distribution is also a gamma distribution.
	
	\begin{align*}
		\theta & \sim gamma(a_0, b_0) = \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0-1} exp(-b_0 x) \\
		p(x_i|\theta, \mu) &= \theta exp(-\theta (x_i-\mu)) I(x_i> \mu) \\
		p(\theta|X) & \propto p(X|\theta) p(\theta) \\
		&= \prod_{i=1}^n \theta exp(-\theta (x_i-\mu)) I(x_i> \mu) \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0-1} exp(-b_0 \theta) \\
		&= \theta^{a_0 + n-1} exp[ -\theta \sum_{i=1}^n (x_i-\mu) - b_0 \theta]
	\end{align*}

Thus, the posterior distribution of $\theta$

\begin{align*}
	p(\theta|X) & \sim Gamma(a_0 + n, \sum_{i=1}^n (x_i-\mu) + b_0) , \qquad X_{(1)} > \mu
\end{align*}
	
Then the 
\begin{align*}
	E(\theta|X) & = \frac{a_0 + n}{\sum_{i=1}^n (x_i-\mu) + b_0} \\
	Var(\theta|X) &= \frac{a_0 + n}{(\sum_{i=1}^n (x_i-\mu) + b_0)^2} 
\end{align*}


\item[(b)] Suppose that $(\mu, \theta)$ are both unknown, $\mu \sim N(\mu_0, \sigma_0^2), \theta \sim gamma(a_0, b_0), (\mu, \theta)$, are assumed independent a priori.

\begin{itemize}
	\item [(i)] Derive the joint posterior distribution of $(\mu, \theta)$  and express it in the simplest possible form. Note that the normalizing constant of the joint posterior does not have an analytic closed form, but can be expressed as an expectation.
	
	Joint posterior distribution of $(\mu, \theta)$, in which $(\mu, \theta)$ are independent priori.
\begin{align*}
	p(\mu, \theta|X) & = \frac{p(X|\mu, \theta) p(\theta) p(\mu)}{p(X)} \\
	p(X|\mu, \theta) p(\theta) p(\mu) &= \prod_{i=1}^n \theta exp(-\theta (x_i-\mu)) I(x_i> \mu) \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0-1} exp(-b_0 \theta) \frac{1}{\sqrt{2\pi} \sigma_0} exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2})\\
	&= \frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0+n-1} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2})\\
\end{align*}

The normalizing constant 
\begin{align*}
	p(X) &= \int \int p(\theta, \mu, x) d\theta d\mu \\
	&= \int_{0}^{\infty} \int_{-\infty}^{\infty}  \frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0+n-1} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}) d\mu d\theta\\
	&= \frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)}  \int_{-\infty}^{\infty} \Big[\int_{0}^{\infty}  \theta^{a_0+n-1} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)] d\theta\Big]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}) d\mu \\
	&=  \frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)} \frac{\Gamma(a_0+n)}{[\sum (x_i-\mu)+b_0]^{a_0+n}} \int_{-\infty}^{\infty}    exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}) d\mu \\
	&= \frac{b_0^{a_0} \Gamma(a_0+n)}{\Gamma(a_0)} \int_{-\infty}^{\infty}  \frac{1}{[\sum (x_i-\mu)+b_0]^{a_0+n}}  \frac{1}{\sqrt{2\pi} \sigma_0}  exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}) d\mu \\
	&=  \frac{b_0^{a_0} \Gamma(a_0+n)}{\Gamma(a_0)} E_{\mu} \Big[ \frac{1}{[\sum (x_i-\mu)+b_0]^{a_0+n}} \Big]
\end{align*}
Thus the posterior distribution of $(\mu, \theta|X)$

\begin{align*}
	p(\mu, \theta|X) & = \frac{\frac{1}{\sqrt{2\pi} \sigma_0} \frac{b_0^{a_0}}{\Gamma(a_0)} \theta^{a_0+n-1} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2})}{\frac{b_0^{a_0} \Gamma(a_0+n)}{\Gamma(a_0)} E_{\mu} \Big[ \frac{1}{[\sum (x_i-\mu)+b_0]^{a_0+n}} \Big]} \\
	&= \frac{1}{\sqrt{2\pi} \sigma_0} \frac{\theta^{a_0+n-1} }{\Gamma(a_0+n) E_{\mu} \Big[ \frac{1}{[\sum (x_i-\mu)+b_0]^{a_0+n}} \Big]} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)]   exp(-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2})
\end{align*}

\item[(ii)] Obtain the limiting joint posterior distribution $\sigma_0 \rightarrow \infty$.

As $\sigma_0 \rightarrow \infty$, the posterior distribution will be a gamma distribution,
\begin{align*}
	p(\mu, \theta|X)& \rightarrow  \frac{\theta^{a_0+n-1} }{\Gamma(a_0+n) E_{\mu} \Big[ \frac{1}{[\sum (x_i-\mu)+b_0]^{a_0+n}} \Big]} exp[-\theta (\sum_{i=1}^n (x_i-\mu)  + b_0)]  \\
	& \sim Gamma(a_0 +n, \sum (x_i-\mu)+b_0)
\end{align*}

\end{itemize}

\end{itemize}

\section{Problem 5}
Let $U(a, b)$ denote the uniform distribution on the interval $(a, b)$. 

\begin{itemize}
	\item [(a)] $U(\theta,\theta+1)$, find posterior distribution of $\theta$
	
	\begin{align*}
		\theta < X_1 ... X_n < \theta + 1 \\
		p(\theta) &= \frac{1}{ b_0 - a_0} \\
		p(\theta |X) &= \frac{p(x|\theta) p(\theta)}{p(X)} \\
		p(x|\theta) p(\theta) &= \prod_{i=1}^n I(x_i \in (\theta, \theta + 1)) \frac{1}{ b_0 - a_0} \\
		&= \frac{1}{ b_0 - a_0} I(X_{i} \in (\theta, \theta + 1)) \\
		&=  \frac{1}{ b_0 - a_0}  I(X_{(1)} > \theta) I ( X_{(n)} < (\theta + 1))
	\end{align*}
	where $X_{(1)}, X_{(n)}$ are the minimum and maximum of the $X_1, ... X_n$. 
	While the normalizing constant term,
	\begin{align*}
		p(X) &= \frac{1}{ b_0 - a_0} \int_{X_{(n)}-1} ^{X_{(1)}}  d\theta\\
		&=  \frac{1+ X_{(1)} -X_{(n)}}{ (b_0 - a_0)} 
	\end{align*}	
Thus, the posterior distribution 

	\begin{align*}
	p(\theta|X) &= \frac{\frac{1}{ b_0 - a_0}  I(X_{(1)} > \theta) I ( X_{(n)} < (\theta + 1))}{\frac{1+ X_{(1)} -X_{(n)}}{ (b_0 - a_0)}} \\
	&= \frac{I(X_{(1)} > \theta) I ( X_{(n)} < (\theta + 1))}{1+ X_{(1)} -X_{(n)}}
\end{align*}	

	The posterior mean of $\theta$
\begin{align*}
	E(\theta|X) &=  \int_{X_{(n)}-1} ^{X_{(1)}} \theta \frac{1}{1+ X_{(1)} -X_{(n)}} d\theta\\
	&=  \frac{X_{(1)} ^2 - (X_{(n)}-1)^2}{ 2({1+ X_{(1)} -X_{(n)}}) } 
\end{align*}	
	
\item[(b)] Suppose $X_1...X_n$ are i.i.d. $U(\theta_{1}, \theta_{2})$, where $(\theta_{1}, \theta_{2})$ are both unknown. Suppose that the joint prior for $(\theta_{1}, \theta_{2})$, where $\theta_1 \sim U(a_0, b_0), \theta_2 \sim U(a_1, b_1)$ for which $\theta_2 > \theta_1$. Derive joint posterior distribution of $(\theta_1, \theta_2)$ and derive the posterior mean and variance of $\theta_2 - \theta_1$.

As $\theta_1 \sim U(a_0, b_0), \theta_2 \sim U(a_1, b_1)$ for which $\theta_2 > \theta_1$. The $a_0, a_1, b_0, b_1$ are specified hyperparamters, while $a_1 > b_0$. So we have $\theta_2, \theta_1$ independent priori. 

\begin{align*}
	\pi(\theta_1, \theta_2) &= \pi(\theta_2|\theta_1) \pi(\theta_1) \\
	\pi(\theta_2, \theta_1) &= \pi(\theta_1) \pi(\theta_2), \qquad \theta_1 \sim U(a_0, b_0), \theta_2 \sim U(a_1, b_1)
\end{align*}	

The joint posterior distribution
\begin{align*}
	p(X|\theta_1, \theta_2) &= \prod_{i=1}^n \frac{1}{\theta_2-\theta_1} I(\theta_1 < x_i < \theta_2)\\
	p(\theta_2, \theta_1|X) &= \frac{p(X|\theta_1, \theta_2) p(\theta_1) p(\theta_2)}{p(X)}\\
	p(X|\theta_1, \theta_2) p(\theta_1) p(\theta_2) &= [\frac{1}{\theta_2-\theta_1}]^n \frac{1}{b_0-a_0} \frac{1}{b_1- a_1} I(X_{(1)} > \theta_1) I(X_{(n)} < \theta_2) , \qquad \theta_1 \in (a_0, b_0), \theta_2 \in (a_1, b_1) 
\end{align*}	

The normalizing constant term,
\begin{align*}
	p(X) &= \int_{a_0}^{X_{(1)}} \int_{X_{(n)}}^{b_1} [\frac{1}{\theta_2-\theta_1}]^n \frac{1}{b_0-a_0} \frac{1}{b_1- a_1} d\theta_1 d\theta_2  ,\qquad X_{(1)} < b_0, X_{(n)} > a_1
\end{align*}	
let $ y= \theta_2- \theta_1, \theta_2 = y+\theta_1$. The Jacobian transformation matrix
\begin{align*}
	\vert \diffp{{(\theta_2, \theta_1)}}{{(y, \theta_1)}} \vert &= \Bigg|
	\begin{pmatrix*}
		1 & 1 \\
		0 & 1
	\end{pmatrix*}
	\Bigg| = 1
\end{align*}	
So ,
\begin{align*}
	p(X) &= \frac{1}{b_0-a_0} \frac{1}{b_1- a_1} \int_{a_0}^{X_{(1)}}  \Big[\int_{X_{(n)}- X_{(1)}}^{b_1 - a_0} y^{-n}  dy \Big] d\theta_1 
\end{align*}	


\end{itemize}



\section{Problem 6}
Find the density $p(x)$ from the Exercise on page 51 of the notes. The joint posterior density of $p(\mu, \tau|x)$ can also be obtained in this case by recognizing

\begin{align*}
	p(\mu, \tau|x)&= p(\mu| \tau, x) p(\tau|x), \qquad \text{normal} \times \text{gamma} 
\end{align*}	

Find $p(\mu| \tau, x), p(\tau|x), p(x)$. 

We could recognize the posterior distribution is a product of normal and gamma distribution. 

\begin{align*}
	p(\mu|\tau) & \sim N(\mu_0, \tau^{-1} \sigma_0^2) = \frac{1}{\sqrt{2\pi \tau^{-1} \sigma_0^2} } exp(-\frac{(\mu-\mu_0)^2}{2 \tau^{-1} \sigma_0^2})\\
	p(\mu, \tau|x)&= \tau^{\frac{n + \delta_0 +1}{2} -1} exp(-\frac{\tau}{2} (\gamma_0 + \frac{\mu_0^2}{\sigma_0^2} + \sum_{i=1}^n x_i^2)) exp(-\frac{\tau}{2} (\mu^2(n+\frac{1}{\sigma_0^2}) -2\mu(\sum_{i=1}^n x_i + \frac{\mu_0}{\sigma_0^2}))) 
\end{align*}	

So we have 
\begin{align*}
	p(\mu| \tau,x)& \propto \tau^{\frac{n + \delta_0 +1}{2} -1} exp(-\frac{\tau}{2} (\gamma_0 + \frac{\mu_0^2}{\sigma_0^2} + \sum_{i=1}^n x_i^2)) \\
	p(\mu| \tau,x)& = \frac{[\frac{\gamma_0 + \frac{\mu_0^2}{\sigma_0^2} + \sum_{i=1}^n x_i^2}{2}]^{\frac{n + \delta_0 +1}{2}}}{\Gamma(\frac{n + \delta_0 +1}{2})} \tau^{\frac{n + \delta_0 +1}{2} -1} exp(-\frac{\tau}{2} (\gamma_0 + \frac{\mu_0^2}{\sigma_0^2} + \sum_{i=1}^n x_i^2)) \\
	p(\tau | x)& \propto  exp(-\frac{\tau}{2} (\mu^2(n+\frac{1}{\sigma_0^2}) -2\mu(\sum_{i=1}^n x_i + \frac{\mu_0}{\sigma_0^2}))) \\
	p(\tau | x)&= 
\end{align*}	
So 
\begin{align*}
	p(X) &= 
\end{align*}	

\section{Problem 7}
Consider the model
\begin{align*}
	y_i &= \beta x_i + \epsilon_i
\end{align*}	
where the $\epsilon_i, i=1,..n$ are i.i.d. $N(0, \sigma^2)$ random variables and $(\beta, \sigma^2)$ are both unknown. Let $\tau = 1/\sigma^2$. Consider the joint improper prior
\begin{align*}
	\pi(\beta, \tau) & \propto \tau^{-1}
\end{align*}	

\begin{itemize}
	\item [(a)] Derive the joint posterior density of $(\beta, \tau)$.
	
	\begin{align*}
		\epsilon_i & \sim N(0, \sigma^2) \\
		y_i & \sim N(\beta x_i, \sigma^2) \\
		p(y_i|\beta, \sigma^2) &= \frac{1}{\sqrt{2\pi} \sigma} exp[-\frac{(y_i-\beta x_i)^2}{2\sigma^2}] \\
		\tau &= 1/\sigma^2, \qquad \pi(\beta, \tau) = \tau^{-1}
	\end{align*}	
	
	The joint posterior distribution of $(\beta, \tau)$ 
\begin{align*}
	p(\beta, \tau|X) &= \frac{p(x|\beta, \tau) p(\beta, \tau)}{p(x)} \\
	p(x|\beta, \tau) p(\beta, \tau) &= (\frac{\tau^{1/2}}{\sqrt{2\pi}})^n exp(-\frac{\tau \sum_{i=1}^n (y_i-\beta x_i)^2}{2}) \tau^{-1}
\end{align*}	

	The normalizing constant
		\begin{align*}
		p(X) &= \int_{0}^{\infty} \int_{-\infty}^{\infty} (\frac{\tau^{1/2}}{\sqrt{2\pi}})^n exp(-\frac{\tau \sum_{i=1}^n (y_i-\beta x_i)^2}{2}) \tau^{-1} d\beta d\tau \\
		&=  \int_{0}^{\infty} \int_{-\infty}^{\infty} \frac{\tau^{n/2-1}}{(2\pi)^{n/2}} exp[-\frac{\tau \sum_{i=1}^n x_i^2}{2} (\beta^2 -2 \beta \frac{\sum x_i y_i}{\sum x_i^2} ) - \frac{\tau}{2} \sum y_i^2] d\beta d\tau \\
		&= \int_{0}^{\infty} \frac{\tau^{n/2-1-1/2}}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2}} exp[-\frac{\tau}{2} \sum y_i^2 + \frac{\tau (\sum x_iy_i)^2}{2 \sum x_i^2}] \\
		& \int_{-\infty}^{\infty} \frac{(\tau \sum x_i^2)^{1/2}}{\sqrt{2\pi}} exp[-\frac{\tau \sum x_i^2 (\beta - \frac{\sum x_i y_i}{\sum x_i^2})^2}{2}] d\beta d\tau \\
		&= \int_{0}^{\infty} \frac{\tau^{n/2-1-1/2}}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2}} exp[-\frac{\tau}{2} \sum y_i^2 + \frac{\tau (\sum x_iy_i)^2}{2 \sum x_i^2}] d\tau\\
		&= \frac{1}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2}} \int_{0}^{\infty} \tau^{n/2-1-1/2} exp[-\frac{\tau}{2} (\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})] d\tau \\
		&=  \frac{\Gamma((n-1)/2)}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2} [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{(n-1)/2}} \\
		& \int_{0}^{\infty} \frac{[\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{(n-1)/2}}{\Gamma((n-1)/2)} \tau^{n/2-1-1/2} exp[-\frac{\tau}{2} (\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})] d\tau \\
		&= \frac{\Gamma((n-1)/2)}{(2\pi)^{n/2-1} [\sum x_i^2]^{1/2} [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{(n-1)/2}}
	\end{align*}	

So the joint posterior distribution
\begin{align*}
	p(\beta, \tau|X) &= \frac{({\tau^{n/2-1}}) exp(-\frac{\tau \sum_{i=1}^n (y_i-\beta x_i)^2}{2}) [\sum x_i^2]^{1/2} [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{(n-1)/2}}{ 2\pi \Gamma((n-1)/2)} 
\end{align*}	

\item[(b)] Derive the marginal posterior density of $\beta$ and mean and variance.

The marginal distribution of $\beta$ 

\begin{align*}
	p(\beta|X) &= \int_{0}^{\infty} p(\beta, \tau|X) d \tau \\
	&=\int_{0}^{\infty}  \frac{({\tau^{n/2-1}}) exp(-\frac{\tau \sum_{i=1}^n (y_i-\beta x_i)^2}{2}) [\sum x_i^2]^{1/2} [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{(n-1)/2}}{ 2\pi \Gamma((n-1)/2)} d\tau \\
	&= \frac{[\sum x_i^2]^{1/2} [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{\frac{n-1}{2}} \Gamma(\frac{n}{2})}{2\pi \Gamma(\frac{n-1}{2}) [\frac{\sum(y_i-\beta x_i)^2}{2}]^{\frac{n}{2}}} \\
	& \int_{0}^{\infty} \frac{[\frac{\sum(y_i-\beta x_i)^2}{2}]^{\frac{n}{2}}}{ \Gamma(\frac{n}{2})} \tau^{\frac{n}{2}-1} exp(-\frac{\tau \sum_{i=1}^n (y_i-\beta x_i)^2}{2})  d\tau \\
	&=\frac{[\sum x_i^2]^{1/2} [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{\frac{n-1}{2}} \Gamma(\frac{n}{2})}{2\pi \Gamma(\frac{n-1}{2}) [\frac{\sum(y_i-\beta x_i)^2}{2}]^{\frac{n}{2}}} 
\end{align*}	

posterior mean
\begin{align*}
	E(\beta|X) &= \int_{-\infty}^{\infty} \beta \frac{[\sum x_i^2]^{1/2} [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{\frac{n-1}{2}} \Gamma(\frac{n}{2})}{2\pi \Gamma(\frac{n-1}{2}) [\frac{\sum(y_i-\beta x_i)^2}{2}]^{\frac{n}{2}}} d\beta \\
	&= \frac{[\sum x_i^2]^{1/2} [\frac{(\sum y_i^2 - \frac{(\sum x_iy_i)^2}{\sum x_i^2})}{2}]^{\frac{n-1}{2}} \Gamma(\frac{n}{2})}{2\pi \Gamma(\frac{n-1}{2}) } \int_{-\infty}^{\infty} \beta [\frac{\sum(y_i-\beta x_i)^2}{2}]^{\frac{n}{2}} d\beta
\end{align*}	

\item[(c)] Derive the marginal posterior density and calculate the posterior mean andvariance.

The marginal distribution of $\tau$ 



\end{itemize}

\end{document}
