
 \section{Normal Distribution}

\subsection{MGF for Univariate Normal}

\begin{align*}
	M(t) &= exp(\mu t + \sigma^2 t^2/2), \qquad \text{MGF for } N(\mu, \sigma^2)\\
	M_{\sqrt{w_i}X_i}(t) &= E[exp(\sqrt{w_i} t X_i)] = exp(\mu \sqrt{w_i} t + \sigma^2 [\sqrt{w_i} t]^2/2), \qquad \mu=0 \\
	&= exp(\sigma^2 w_i t^2/2)
\end{align*}
Then the linear combination $y_n$
\begin{align*}
	M_{Y_n}(t) &= E[exp \left( (\sqrt{w_1} X_1 + \sqrt{w_2} X_2 + .. + \sqrt{w_n} X_n) t \right)] \\
	&= E[exp(\sqrt{w_1} X_1 t)] E[exp(\sqrt{w_2} X_2 t)] E[exp(\sqrt{w_3} X_3 t)].. E[exp(\sqrt{w_n} X_n t)] \\
	&= exp(\sigma^2 w_1 t^2/2) exp(\sigma^2 w_2 t^2/2) exp(\sigma^2 w_3 t^2/2).. exp(\sigma^2 w_n t^2/2) \\
	&= exp(\sigma^2 [w_1+ w_2 + .. w_n] t^2/2) = exp(\sigma^2 t^2/2)
\end{align*}
So $Y_n \sim N(0, \sigma^2)$. 

\subsection{Bivariate Normal Distribution}

The Bivariate Normal Distribution is always connected with partitioned covariance matrix. Assume vector (X, Y) is Gaussian. 

\begin{theorem}

Two random variables X and Y are said to be bivariate normal, or jointly normal, if $aX+bY$ has a normal distribution for all $a,b \in R$.
\end{theorem}

If $X \sim N( \mu_X, \sigma^2_X)$ and $Y \sim N( \mu_Y, \sigma^2_Y)$ are jointly normal, then $ X + Y  \sim N( \mu_X + \mu_Y, \sigma^2_X + \sigma^2_Y + 2 \rho(X,Y) \sigma_X \sigma_Y) $. 


We consider $X + Y$ is a also normal distribution, then the covariance 

\begin{align*}
	Cov(X+Y) &= Cov(X) + Cov(Y) + 2Cov(XY) = \sigma^2_X + \sigma^2_Y + 2 \rho(X,Y) \sigma_X \sigma_Y
\end{align*}

How to provide a simple way to generate jointly normal random variables? The basic idea is that we can start from several independent random variables and by considering their linear combinations, we can obtain bivariate normal random variables. 

Let $Z_1$ and $Z_2$ be two independent N(0,1) random variables. Define

\begin{align*}
	X &= Z_1, \qquad Y= \rho Z_1 + \sqrt{1-\rho^2} Z_2
\end{align*}

where $\rho$ is a real number in (-1, 1). Show that X and Y are bivariate normal.

First, note that since $Z_1$ and $Z_2$ are normal and independent, they are jointly normal, with the joint PDF

\begin{align*}
	f_{Z_1, Z_2} (z_1, z_2) &= f_{Z_1}(z_1) f_{Z_2}(z_2) \\
	&= \frac{1}{2 \pi} exp \left(-\frac{1}{2} [z_1^2 + z_2^2] \right)
\end{align*}

We need to show $aX+bY$ is normal for all $a,b \in R$. We have

\begin{align*}
	aX + bY &= a Z_1 + b(\rho Z_1 + \sqrt{1- \rho^2} Z_2) \\
	&= (a + b\rho) Z_1 + b \sqrt{1- \rho^2} Z_2
\end{align*}

which is a linear combination of $Z_1$ and $Z_2$, and thus it is normal.


We can use the method of transformation to find the joint PDF of X and Y. The inverse transformation is given by

\begin{align*}
	Z_1 &= X = h_1(X, Y) \\
	Z_2 &= -\frac{\rho}{\sqrt{1-\rho^2}} X + \frac{1}{\sqrt{1-\rho^2}} Y = h_2(X, Y)
\end{align*}

We have

\begin{align*}
	f_{XY}(z_1, z_2) &= f_{Z_1, Z_2} (h_1(X, Y), h_2(X, Y)) |J|\\
	&= f_{Z_1, Z_2} (x, -\frac{\rho}{\sqrt{1-\rho^2}} x + \frac{1}{\sqrt{1-\rho^2}} y) |J|
\end{align*}

where 

\begin{align*}
	J &= det \begin{bmatrix}
		\diffp{h_1}{x} & \diffp{h_1}{y} \\
		\diffp{h_2}{x} & \diffp{h_2}{y}
	\end{bmatrix} = det \begin{bmatrix}
	1 & 0 \\
	-\frac{\rho}{\sqrt{1-\rho^2}} & \frac{1}{\sqrt{1-\rho^2}}
\end{bmatrix} = \frac{1}{\sqrt{1-\rho^2}}
\end{align*}

Thus, we conclude that,

\begin{align*}
	f_{XY}(z_1, z_2) &= \frac{1}{2 \pi \sqrt{1-\rho^2}} exp \left( -\frac{1}{2 (1- \rho^2)} [x^2 -2 \rho xy + y^2] \right)
\end{align*}


To find the $\rho$

\begin{align*}
	Var(X) &= Var(Z_1) =1 \\
	Var(Y) &= \rho^2 Var(Z_1) + (1- \rho^2) Var(Z_2) = 1 \\
	\rho(X,Y) &= Cov(X, Y) = Cov(Z_1, \rho Z_1 + \sqrt{1-\rho^2} Z_2) \\
	&= \rho Cov(Z_1, Z_2) + \sqrt{1-\rho^2} Cov(Z_1, Z_2) \\
	&= \rho
\end{align*}

Now, if you want two jointly normal random variables X and Y such that $X \sim N(\mu_X, \sigma^2_X), Y \sim N(\mu_Y, \sigma^2_Y)$, and $\rho(X,Y)= \rho$, you can start with two independent N(0,1) random variables, $Z_1$ and $Z_2$, and define

\begin{align*}
	X &= \sigma_X Z_1 + \mu_X \\
	Y &= \sigma_Y \left(\rho Z_1 + \sqrt{1-\rho^2} Z_2 \right)  + \mu_Y
\end{align*}

construction using Z1 and Z2 can be used to solve problems regarding bivariate normal distributions. Third, this method gives us a way to generate samples from the bivariate normal distribution using a computer program. 

\subsection{Conditional Distribution}

\begin{theorem}
Suppose X and Y are jointly normal random variables with parameters $\mu_X, \sigma^2_X, \mu_Y, \sigma^2_Y$, and $ \rho $. Then, given $X=x, Y$ is normally distributed with

\begin{align*}
	E[Y|X=x] &= \mu_Y + \rho \sigma_Y \frac{x-\mu_X}{\sigma_X} \\
	Var(Y|X=x) &= (1- \rho^2) \sigma^2_Y
\end{align*}

\end{theorem}

One way to solve this problem is by using the joint PDF formula. In particular, since $X \sim N(\mu_X, \sigma^2_X), Y \sim N(\mu_Y, \sigma^2_Y)$, we can use

\begin{align*}
	f_{Y|X=x} (y|x) &= \frac{f_{XY} (x, y)}{f_X(x)}
\end{align*}
or we use

\begin{align*}
	X &= \sigma_X Z_1 + \mu_X \\
	Y &= \sigma_Y \left(\rho Z_1 + \sqrt{1-\rho^2} Z_2 \right)  + \mu_Y
\end{align*}

Thus, given $X=x$, 
\begin{align*}
	Z_1 &= \frac{x - \mu_X}{\sigma_X} \\
	Y &= \sigma_Y \rho \frac{x- \mu_X}{\sigma_X} + \sigma_Y \sqrt{1- \rho^2} Z_2 + \mu_Y 
\end{align*}

Since $Z_1$ and $Z_2$ are independent, knowing $Z_1$ does not provide any information on $Z_2$. We have shown that given $X=x, Y$ is a linear function of $Z_2$, thus it is normal. In particular

\begin{align*}
	E[Y|X=x] &= \sigma_Y \rho \frac{x- \mu_X}{\sigma_X} + \sigma_Y \sqrt{1- \rho^2} E[Z_2] + \mu_Y  \\
	 &= \mu_Y + \rho \sigma_Y \frac{x-\mu_X}{\sigma_X} \\
	Var[Y|X=x] &= \sigma_Y^2 (1- \rho^2) Var(Z_2) = (1- \rho^2) \sigma_Y^2
\end{align*}



\section{Multivariate Normal Distribution}

\subsection{Moment Generating Function}

Suppose $Y \sim MVN(\mu, \Sigma)$, $\Sigma$ is positive definite matrix. 

Then we can decompose $\Sigma = B B^T$ for some nonsingular matrix B since $\Sigma$ is positive definite. 

 \begin{align*}
    X &= B^{-1} (Y- \mu), \qquad Y = \mu + B X 
\end{align*}

So we have $X_1, â€¦ X_n$ are independent standard normal, $X = (X_1,.. X_n)^T \sim MVN(0, I_n)$. 

 \begin{align*}
    M_Y(t) &=  E\Big[ e^{t^T Y}\Big ]=  E\Big[ e^{t^T (\mu + B X)}\Big ] = e^{t^T \mu} E \Big[ e^{l^T X} \Big], \qquad l^T = t^T B \\
    &= e^{t^T \mu} E \Big[ e^{\sum_{i=1}^n l_i Y_i} \Big], \qquad l= (l_1,.. l_n)\\
    &= e^{t^T \mu} \prod_{i=1}^n E \Big[ e^{ l_i Y_i} \Big] \\
    &= e^{t^T \mu} \prod_{i=1}^n e^{l_i^2/2}\\
    &= \exp \Big( \mu^T t + \frac{1}{2} l^T l \Big) \\
    &= \exp \Big( \mu^T t + \frac{1}{2} t^T \Sigma t \Big)
\end{align*}

 \subsection{Marginal and conditional distributions of a multivariate normal vector}

A $K \times 1$ random vector X is multivariate normal if its joint probability density function is 
\begin{align*}
	f_X(x) &= (2\pi)^{-K/2} |det(V)|^{-1/2} exp(-\frac{1}{2} (x-\mu)^T V^{-1}(x-\mu)) 
\end{align*}

where $\mu$ is a $K \times 1$ mean vector, $V$ is a $K \times K$ covariance matrix.

Partition of the vector:

 We partition X into two sub-vectors $X_a$ and $X_b$ such that
\begin{align*}
	X &= \begin{pmatrix}
		X_a \\
		X_b
	\end{pmatrix}
\end{align*}
The sub-vectors $X_a$ and $X_b$ have dimensions $K_a \times 1$ and $K_b \times 1$ respectively. Moreover, $K_a + K_b = K$.

Partition of the parameters

We partition the mean vector and covariance matrix as follows:

\begin{align*}
	\mu &= \begin{pmatrix}
		\mu_a \\
		\mu_b
	\end{pmatrix}
\end{align*}
and 

\begin{align*}
	V &= \begin{pmatrix}
		V_a & V_{ab}^T \\
		V_{ab} & V_b
	\end{pmatrix}
\end{align*}

Normality of the sub-vectors

The marginal distributions of the two sub-vectors are also multivariate normal.

\begin{itemize}

\item[(i)] 
\begin{proof}

The random vector $X_a$ can be written as a linear transformation of X:

\begin{align*}
	X_a &= A X
\end{align*}
Where $A$ is a $K_a \times K$ matrix whose entries are either zero or one. Thus, $X_a $ has a multivariate normal distribution because it is a linear transformation of the multivariate normal random vector X and multivariate normality is preserved by linear transformations. Same as $X_b = B X$ where $B$ is a $K_b \times K$ matrix whose entries are either zero or one. 

Independence of the sub-vectors

$X_a$ and $X_b$ are independent if and only if $V_{ab} = 0$.

$X_a$ and $X_b$ are independent if and only if their joint moment generating function is equal to the product of their individual moment generating functions. Since $X_a$ is multivariate normal, its joint moment generating function is 

\begin{align*}
	M_{X_a}(t_a) &= exp(t^T_a \mu_a + \frac{1}{2} t_a^T V_a t_a) \\
	M_{X_b}(t_b) &= exp(t^T_b \mu_b + \frac{1}{2} t_b^T V_b t_b) 
\end{align*}

The joint moment generating function of $X_a$ and $X_b$, which is just the joint moment generating function of X, is

\begin{align*}
	M_{X_a, X_b}(t_a, t_b) &= M_{X}(t) \\
	&= exp(t^T \mu + \frac{1}{2} t^T V t) \\
	&= exp \left( [t_a^T t_b^T] \begin{bmatrix}
		\mu_a \\
		\mu_b
	\end{bmatrix} + \frac{}{} [t_a^T t_b^T] \begin{bmatrix}
	V_a & V_{ab}^T \\
	V_{ab} & V_b
\end{bmatrix} [t_a t_b] \right) \\
&= exp \left( t_a^T \mu_a + t_b^T \mu_b + \frac{1}{2} t_a^T V_a t_a + \frac{1}{2} t_b^T V_b t_b + \frac{1}{2} t_b^T V_{ab} t_a + \frac{1}{2} t_a^T V_{ab}^T t_b \right) \\
&= exp \left( t_a^T \mu_a + t_b^T \mu_b + \frac{1}{2} t_a^T V_a t_a + \frac{1}{2} t_b^T V_b t_b +  t_b^T V_{ab} t_a  \right) \\
&= exp \left( t_a^T \mu_a + \frac{1}{2} t_a^T V_a t_a \right) exp \left( t_b^T \mu_b + \frac{1}{2} t_b^T V_b t_b \right) exp( t_b^T V_{ab} t_a)
\end{align*}

from which it is obvious that $M_{X_a, X_b}(t_a, t_b) = M_{X_a}(t_a) M_{X_b}(t_b)$ if and only if $V_{ab} = 0$.

\end{proof}

\item[(ii)] Conditional distributions of MVN are MVN. 
Suppose $X \sim N_n(\mu, \Sigma)$. Using the partition above, we have

\begin{align*}
	X_1 | X_2 &= x_2 \sim N_r(\mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2 - \mu_2), \Sigma_{11.2})\\
	\Sigma_{11.2} &= \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} = (1 - \rho^2) \sigma_1^2
\end{align*}
\end{itemize}

